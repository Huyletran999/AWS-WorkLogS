[{"uri":"https://huyletran999.github.io/AWS-WorkLogS/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: “AI-Driven Development Life Cycle: Reimagining Software Engineering” Event Objectives Explore the transformative shift in software development driven by generative AI. Introduce the AI-Driven Development Life Cycle (AI-DLC) and its core concepts. Kiro and Amazon Q Developer demonstration Speakers Toan Huynh – Specialist SA, PACE My Nguyen - Sr. Prototyping Architect, Amazon Web Services - ASEAN Key Highlights Focused on the concept of AI-DLC, a framework where AI orchestrates the development process, including planning, task decomposition, and architectural suggestions, while developers retain ultimate responsibility for validation, decision-making, and oversight - AI-DLC Core Concept: The approach is Human-Centric, with AI acting as a Collaborator to enhance developer capabilities, leading to Accelerated Delivery (cycles measured in hours/days instead of weeks/months) .\n- AI-DLC Workflow: It\u0026rsquo;s an iterative loop involving AI Tasks (Create plan, Implement Plan, Seek clarification) and Human Tasks (Provide clarification, Implement Plan), where the AI repeatedly asks clarifying questions and only implements solutions after human validation .\n- AI-DLC Stages: The lifecycle is broken down into Inception, Construction, and Operation. Each stage builds richer context for the next:\nInception: Includes building context, elaborating intent with User Stories, and planning with Units of Work.\nConstruction: Involves Domain Modeling, code generation and testing, adding architectural components, and deploying with IaC \u0026amp; tests.\nOperation: Focuses on deploying in production and managing incidents.\n- Challenges AI-DLC Aims to Solve:\nScaling AI development: AI coding tools can fail with complex projects.\nLimited control: Existing tools make it difficult to collaborate with and manage AI agents.\nCode quality: Maintaining quality control when moving from proof-of-concept to production becomes difficult.\nDeep Dive: Kiro - The AI IDE for Prototype to Production Kiro, an AI-first Integrated Development Environment (IDE) that supports the AI-DLC, focusing on Spec-driven development - Spec-driven Development: Kiro turns a high-level prompt (e.g., \u0026ldquo;I want to create a chat application like Slack\u0026rdquo;) into clear requirements (requirements.md), system design (design.md), and discrete tasks (tasks.md), fundamentally shifting development from \u0026ldquo;vibe coding\u0026rdquo; to a structured, traceable process. Developers collaborate with Kiro on these specs, which serve as the source of truth.\n- Agentic Workflows: Kiro\u0026rsquo;s AI agents implement the spec while keeping the human developer in control, with the key features being:\n+ Implementation Plan: Kiro generates a detailed Implementation Plan with start tasks, sub-tasks (e.g., \u0026ldquo;Implement user registration and login endpoints,\u0026rdquo; \u0026ldquo;Implement JWT middleware\u0026rdquo;), and links them back to specific requirements for validation .\n+ Agent Hooks: These delegate tasks to AI agents that trigger on events such as \u0026ldquo;file save.\u0026rdquo; They autonomously execute in the background based on pre-defined prompts, helping to scale work by generating documentation, unit tests, or optimizing code performance.\nKey Takeaways - AI Ensures Production Readiness: Kiro creating detailed design documents (like data flow diagrams and API contracts), and generating unit tests before the code is written, ensures that AI-generated code is production-ready and maintainable, not just a quick prototype.\n- Human Control via Artifacts: Developers maintain control not by writing the bulk of the code, but by validating and refining the artifacts—the requirements, the design, and the task plan—before the AI agents execute the implementation.\nApplying to Work - Integrate Amazon Q Developer/Similar Tools: Integrating AI coding assistants into my academic projects to automate boilerplate code and common tasks to boost productivity.\n- Focus on High-Value Tasks: By letting AI automate undifferentiated heavy lifting, I can focus my time on mastering higher-value, creative tasks like Domain Modeling and Architectural Design, which are crucial human-centric activities in the Construction phase.\nEvent Experience Attending the AI-Driven Development Life Cycle: Reimagining Software Engineering event provided a fascinating glimpse into the future of software development. It was clear that Generative AI isn\u0026rsquo;t just a coding assistant; it\u0026rsquo;s poised to become a core orchestrator of the entire development process. The session was well-structured, moving from the overarching concept of AI-DLC to specific demonstrations of Amazon Q Developer and Kiro. The demo of Kiro was particularly impactful, showing how a single text prompt can be transformed into a full, executable, and traceable development plan inside the IDE.\nLessons learned The three main challenges with current AI development (scaling, limited control, and code quality) made the structured, human-validated approach of AI-DLC seem highly necessary and well-thought-out. Some event photos "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"SQL to NoSQL: Hiện đại hóa lớp truy cập dữ liệu với Amazon DynamoDB AWS Database Blog Bởi Ramana Mannava, Mahesh Kumar Vemula, và Akber Rizwan Shaik 03 JUL 2025 Chuyên mục: Advanced (300), Amazon DynamoDB, Migration, Technical How-to\nTrong Phần 1 của loạt bài, chúng tôi đã khám phá cách di chuyển hiệu quả từ SQL sang Amazon DynamoDB. Sau khi thiết lập các chiến lược mô hình hóa dữ liệu được thảo luận trong Phần 2, giờ đây chúng tôi xem xét những cân nhắc chính để phân tích và thiết kế bộ lọc, phân trang, các trường hợp biên (edge cases) và các phép tổng hợp, xây dựng trên các mô hình dữ liệu đã thiết kế nhằm tạo ra một lớp truy cập dữ liệu hiệu quả.\nThành phần này kết nối ứng dụng của bạn với các tính năng và khả năng của DynamoDB. Việc chuyển đổi từ các mẫu truy cập dựa trên SQL sang cách tiếp cận điều khiển bằng API của DynamoDB mở ra cơ hội tối ưu hóa cách ứng dụng tương tác với lớp dữ liệu của nó.\nPhần cuối cùng của loạt bài này tập trung vào việc triển khai một lớp trừu tượng hiệu quả và xử lý các mẫu truy cập dữ liệu khác nhau trong DynamoDB.\nThiết kế lại mô hình thực thể Mô hình thực thể, đại diện cho cấu trúc dữ liệu trong ứng dụng của bạn, sẽ cần được thiết kế lại để phù hợp với mô hình dữ liệu của DynamoDB. Điều này có thể bao gồm việc phi chuẩn hóa (de-normalizing) các mô hình và tái cấu trúc các mối quan hệ giữa các thực thể.\nBên cạnh đó, hãy cân nhắc công sức liên quan tới các cấu hình sau:\nChú thích thuộc tính DynamoDB: Gắn chú thích cho các thuộc tính thực thể với các thuộc tính đặc thù của DynamoDB, bao gồm partition key, sort key, thông tin local secondary index (LSI) và thông tin global secondary index (GSI). Ví dụ, khi sử dụng mô hình persistence đối tượng .NET, bạn cần ánh xạ các lớp và thuộc tính với các bảng và thuộc tính DynamoDB. Cấu hình tiền tố khóa: Trong thiết kế một bảng đơn (single table design), bạn có thể phải cấu hình tiền tố cho partition key và sort key trong các mô hình thực thể. Phân tích cách những tiền tố này sẽ được sử dụng để truy vấn trong lớp truy cập dữ liệu của bạn. Đoạn code sau là ví dụ minh họa cho việc cấu hình tiền tố khóa trong các mô hình thực thể:\npublic class Post { private const string PREFIX = \u0026#34;POST#\u0026#34;; public string Id { get; private set; } public string Content { get; private set; } public string AuthorId { get; private set; } public Post(string id, string content, string authorId) { Id = id; Content = content; AuthorId = authorId; } // Property that automatically adds prefix public string PartitionKey =\u0026gt; $\u0026#34;{PREFIX}{Id}\u0026#34;; } // Usage example var post = new Post(\u0026#34;123\u0026#34;, \u0026#34;Hello World\u0026#34;, \u0026#34;USER#456\u0026#34;); var queryKey = post.PartitionKey; // Gets \u0026#34;POST#123\u0026#34; Thiết kế lại quy tắc ánh xạ: Do thay đổi trong mô hình thực thể, các quy tắc ánh xạ hiện có giữa view models của ứng dụng và các mô hình thực thể có thể cần được thiết kế lại.\nThiết kế lớp trừu tượng API DynamoDB Lớp trừu tượng API DynamoDB đóng gói các thao tác DynamoDB bên dưới đồng thời cung cấp cho ứng dụng của bạn một giao diện sạch sẽ. Hãy cùng khám phá các thành phần bạn có thể cần triển khai trong lớp này.\nXử lý lỗi và cơ chế thử lại Các kịch bản lưu lượng cao thường dẫn tới các lỗi tạm thời cần xử lý. Ví dụ, khi nội dung lan truyền mạnh hoặc một bài đăng của người nổi tiếng thu hút sự chú ý đột ngột, bạn có thể gặp ngoại lệ vượt quá throughput. Bạn có thể cần triển khai:\nCơ chế thử lại tự động với exponential backoff cho các lỗi tạm thời. Chuyển dịch ngoại lệ DynamoDB thành các lỗi có ý nghĩa cho ứng dụng. Ghi nhật ký lỗi nhất quán để phục vụ khắc phục sự cố. Xử lý thất bại kiểm tra điều kiện (conditional check failures) trong các cập nhật đồng thời. Quản lý thao tác hàng loạt (Batch operation management) Ứng dụng thường cần xử lý nhiều mục một cách hiệu quả để cung cấp trải nghiệm người dùng tốt. Xem xét các kịch bản như tải một news feed cá nhân hóa kết hợp bài đăng từ nhiều người theo dõi. Bạn có thể cần triển khai:\nTự động chia nhỏ các yêu cầu trong giới hạn của DynamoDB. Xử lý song song để tối ưu hiệu năng. Cơ chế khôi phục cho các thất bại một phần trong batch. Theo dõi tiến độ cho các thao tác chạy dài. Tải dữ liệu liên quan của thực thể (Loading related entity data) Khi di chuyển từ cơ sở dữ liệu quan hệ sang DynamoDB, có một nhận thức phổ biến rằng dữ liệu quan hệ thường được phi chuẩn hóa và việc truy cập dữ liệu liên quan trở nên đơn giản. Tuy nhiên, điều này không phải lúc nào cũng đúng. Mặc dù trong một số trường hợp các quan hệ có thể được mô hình hóa bằng chiến lược single-item modeling, tùy theo chi phí và cân nhắc hiệu năng, các quan hệ có thể được mô hình hóa bằng các chiến lược khác như vertical partitioning hoặc composite sort keys.\nKhi thích nghi với DynamoDB, bạn có thể phải phát triển các phương thức trợ giúp trong lớp trừu tượng của mình để tải dữ liệu quan hệ của một thực thể (navigation properties) một cách hiệu quả. Các phương thức này cần xem xét kiến trúc ứng dụng, mẫu truy cập và chiến lược mô hình hóa dữ liệu.\nVí dụ, trong ứng dụng mạng xã hội của chúng tôi, việc tải các comment cho một bài đăng có thể yêu cầu các phương pháp khác nhau tùy thuộc vào chiến lược mô hình hóa được chọn – từ việc lấy thuộc tính đơn giản trong single-item models đến các thao tác truy vấn trong vertical partitioning.\nĐối với các thực thể liên quan sử dụng chiến lược single-item, có thể không cần logic tải cụ thể vì tất cả dữ liệu được truy xuất trong một thao tác API duy nhất. Tuy nhiên, đối với các chiến lược mô hình khác như vertical partitioning, các phương thức trong lớp trừu tượng của bạn cần xử lý truy vấn hiệu quả dựa trên điều kiện lọc và phân trang. Ví dụ, khi comments được lưu như các item riêng biệt chia sẻ partition key của bài đăng, phương thức phải truy vấn và phân trang các item liên quan một cách hiệu quả. Xây dựng dựa trên khả năng vận hành batch, bạn có thể mở rộng các phương thức này để xử lý tải dữ liệu liên quan cho nhiều mục. Ví dụ, khi tải comments cho nhiều bài đăng, sử dụng BatchGetItem để thực hiện các việc sau: Sử dụng cơ chế batching đã thiết lập để nhóm các yêu cầu. Áp dụng retry và chiến lược xử lý lỗi. Cung cấp giao diện nhất quán cho cả thao tác đơn lẻ và thao tác hàng loạt. Khi sử dụng GSI, bạn có thể cần truy xuất các thuộc tính bổ sung không được bao gồm trong projection của GSI. Thiết kế các chiến lược để tải hiệu quả dữ liệu cần thiết trong khi tối thiểu hóa các cuộc gọi API và tối ưu hóa hiệu năng và chi phí. Phương thức trong lớp trừu tượng của bạn có thể cần cung cấp:\nGiao diện nhất quán để tải dữ liệu liên quan. Tối ưu hóa các cuộc gọi API và chi phí. Đơn giản hóa bảo trì thông qua triển khai tập trung. Đoạn code sau là ví dụ minh họa cho việc tải các navigation properties:\n// Entity with navigation property public class Post { public string Id { get; set; } public string Content { get; set; } public IEnumerable\u0026lt;Comment\u0026gt; Comments { get; set; } } // Interface for loading related data public interface INavigationPropertyManager { Task\u0026lt;IEnumerable\u0026lt;T\u0026gt;\u0026gt; LoadRelatedItemsAsync\u0026lt;T\u0026gt;(string parentId); Task\u0026lt;IDictionary\u0026lt;string, IEnumerable\u0026lt;T\u0026gt;\u0026gt;\u0026gt; LoadRelatedItemsInBatchAsync\u0026lt;T\u0026gt;(IEnumerable\u0026lt;string\u0026gt; parentIds); } // Service using the loader public class PostService { private readonly INavigationPropertyManager _navigationPropertyManager; public PostService(INavigationPropertyManager navigationPropertyManager) { _navigationPropertyManager = navigationPropertyManager; } public async Task\u0026lt;IEnumerable\u0026lt;Comment\u0026gt;\u0026gt; GetPostCommentsAsync(string postId) { return await _navigationPropertyManager.LoadRelatedItemsAsync\u0026lt;Comment\u0026gt;(postId); } } Khi thiết kế các phương thức này, hãy phân tích các mẫu tải hiện tại của ứng dụng và đánh giá liệu việc duy trì các mẫu tương tự trong DynamoDB có thể mang lại lợi ích cho hiệu năng và trải nghiệm người dùng hay không.\nÁnh xạ phản hồi (Response mapping) Khi ứng dụng phát triển, cấu trúc dữ liệu và yêu cầu thay đổi theo thời gian. Ví dụ, khi thêm các tính năng mới như phản ứng (reactions) cho bài đăng ngoài lượt thích đơn giản, hoặc giới thiệu nội dung đa phương tiện phong phú trong hồ sơ người dùng, khả năng tương thích ngược (backward compatibility) trở nên quan trọng. Bạn có thể cần triển khai logic ánh xạ để thực hiện các chức năng sau:\nChuyển các item DynamoDB thành các domain object. Xử lý tương thích ngược khi mô hình dữ liệu tiến hóa. Quản lý giá trị mặc định cho các thuộc tính thiếu. Hỗ trợ các phiên bản khác nhau của cùng một thực thể. Xây dựng biểu thức filter (Filter expression building) Các nhu cầu truy xuất dữ liệu phức tạp thường xuất hiện trong các ứng dụng hiện đại. Ví dụ, khi người dùng muốn tìm bài đăng trong khung thời gian cụ thể có mức tương tác cao, hoặc khi lọc comment dựa trên các mẫu tương tác người dùng. Lớp trừu tượng của bạn có thể cần:\nChuyển đổi tiêu chí tìm kiếm phức tạp thành các biểu thức filter DynamoDB. Xử lý nhiều điều kiện filter một cách động. Quản lý expression attribute names và values. Hỗ trợ lọc thuộc tính lồng nhau. Triển khai phân trang (Pagination implementation) Việc điều hướng dữ liệu hiệu quả quan trọng đối với trải nghiệm người dùng. Xem xét các kịch bản như người dùng cuộn qua news feed vô hạn, hoặc người kiểm duyệt duyệt các comment trên các bài viral. Bạn có thể cần triển khai:\nPhân trang dựa trên token sử dụng LastEvaluatedKey. Cấu hình kích thước trang (page size) có thể tùy chỉnh. Xử lý hiệu quả các tập kết quả lớn. Hành vi phân trang nhất quán trên các truy vấn khác nhau. Đoạn code sau là ví dụ minh họa cho việc phân trang:\n// Enhanced interface adding pagination support public interface INavigationPropertyManager { Task\u0026lt;IEnumerable\u0026lt;T\u0026gt;\u0026gt; LoadRelatedItemsAsync\u0026lt;T\u0026gt;(string parentId); Task\u0026lt;IDictionary\u0026lt;string, IEnumerable\u0026lt;T\u0026gt;\u0026gt;\u0026gt; LoadRelatedItemsInBatchAsync\u0026lt;T\u0026gt;(IEnumerable\u0026lt;string\u0026gt; parentIds); // method for paginated loading Task\u0026lt;PagedResult\u0026lt;T\u0026gt;\u0026gt; LoadRelatedItemsPagedAsync\u0026lt;T\u0026gt;(string parentId, PaginationOptions options); } public class PaginationOptions { public int PageSize { get; set; } = 20; public string ExclusiveStartKey { get; set; } } public class PagedResult\u0026lt;T\u0026gt; { public IEnumerable\u0026lt;T\u0026gt; Items { get; set; } public string LastEvaluatedKey { get; set; } } // With pagination support public class PostService { private readonly INavigationPropertyManager _navigationPropertyManager; public PostService(INavigationPropertyManager navigationPropertyManager) { _navigationPropertyManager = navigationPropertyManager; } public async Task\u0026lt;PagedResult\u0026lt;Comment\u0026gt;\u0026gt; GetPostCommentsPagedAsync(string postId, int pageSize = 20, string nextToken = null) { var options = new PaginationOptions { PageSize = pageSize, ExclusiveStartKey = nextToken }; return await _navigationPropertyManager.LoadRelatedItemsPagedAsync\u0026lt;Comment\u0026gt;(postId, options); } } Mã hóa dữ liệu (Data encryption) Bảo vệ dữ liệu nhạy cảm của người dùng là điều tối quan trọng trong các ứng dụng hiện đại. Bạn có thể cần triển khai:\nMã hóa phía client sử dụng AWS Database Encryption SDK bên cạnh mã hóa phía server do DynamoDB cung cấp. Mã hóa chọn lọc các thuộc tính. Xử lý lỗi cho các thao tác mã hóa. Observability (Quan sát/Giám sát) Giám sát sức khỏe ứng dụng và hiệu năng là điều cần thiết. Hãy xem xét giám sát các chỉ số Amazon CloudWatch sau:\nTheo dõi độ trễ request: Giám sát các chỉ số DynamoDB như SuccessfulRequestLatency, và tạo các metric tùy chỉnh cho TransactionConflict và ConditionalCheckFailedRequests. Tiêu thụ capacity: Theo dõi ConsumedReadCapacityUnits và ConsumedWriteCapacityUnits. Tỷ lệ lỗi và các mẫu lỗi: Giám sát ConditionalCheckFailedRequests, SystemErrors, UserErrors. Hiệu năng truy vấn: Theo dõi ThrottledRequests, ScannedCount hoặc Count, thời gian lọc phía client, và độ trễ cuộc gọi dịch vụ bên ngoài. Quản lý giao dịch (Transaction management) Duy trì tính nhất quán dữ liệu là điều quan trọng trong nhiều kịch bản. Bạn có thể cần:\nXử lý các thao tác giao dịch. Quản lý timeout và xung đột. Logic bù trừ (compensation) cho các giao dịch thất bại. Xử lý bộ lọc (Handling filters) Sau khi bạn thiết kế lớp trừu tượng API DynamoDB với các thao tác cơ bản và khả năng tải dữ liệu, hãy phân tích cách thích nghi các mẫu truy vấn hiện có để phù hợp với cách truy vấn của DynamoDB.\nTrong khi các cơ sở dữ liệu quan hệ sử dụng trình tối ưu hóa truy vấn cho các điều kiện WHERE, DynamoDB trao quyền cho nhà phát triển kiểm soát chính xác luồng thực thi truy vấn. DynamoDB xử lý truy vấn theo hai bước:\nĐầu tiên, nó truy xuất các item khớp với key condition expression dựa trên partition và sort keys. Sau đó, trước khi trả kết quả, nó áp dụng filter expressions lên các thuộc tính không phải khóa. Mặc dù filter expressions không giảm tiêu thụ RCU vì toàn bộ tập kết quả vẫn được đọc trước khi lọc, chúng giảm chi phí truyền tải dữ liệu và cải thiện hiệu năng ứng dụng bằng cách lọc dữ liệu ở cấp độ dịch vụ DynamoDB.\nXử lý các yêu cầu lọc phức tạp (Handling complex filter requirements) Khả năng biểu thức linh hoạt của DynamoDB xử lý nhiều kịch bản lọc trực tiếp, và bạn có thể triển khai lọc phía client cho bất kỳ yêu cầu bổ sung nào. Một vài ví dụ bao gồm:\nCác hàm hoặc phương thức không được hỗ trợ: Khi làm việc với các bộ lọc tham chiếu đến các hàm hệ thống (như SUBSTRING, CONCAT, DATEADD, DATEDIFF), hãy truy xuất dữ liệu từ DynamoDB và áp dụng các bộ lọc chuyên biệt này ở lớp ứng dụng. Tải dữ liệu thực thể liên quan: Đối với các truy vấn lọc dựa trên thuộc tính của các thực thể liên quan, ứng dụng có thể cần tải dữ liệu từ nhiều bảng DynamoDB hoặc các bộ sưu tập item và áp dụng lọc tại lớp ứng dụng. Tích hợp với nguồn dữ liệu bên ngoài: Trong kiến trúc microservice, lọc có thể yêu cầu dữ liệu từ các dịch vụ hoặc cơ sở dữ liệu khác. Hãy xem xét trường hợp sử dụng truy xuất comment bài đăng theo tác giả đang hoạt động (active) và điểm sentiment, yêu cầu dữ liệu từ dịch vụ người dùng ngoài và cơ sở dữ liệu phân tích:\nOriginal SQL Query demonstrating filters across different data sources:\nSELECT c.*, u.name, u.profile_pic, u.status, m.sentiment_score FROM comments c JOIN users u ON c.user_id = u.id JOIN comment_analytics m ON c.id = m.comment_id WHERE c.post_id = \u0026#39;123\u0026#39; AND c.created_at \u0026gt; DATEADD(year, -1, GETUTCDATE()) AND u.status = \u0026#39;ACTIVE\u0026#39; AND m.sentiment_score \u0026gt; 0.8 Mã C# minh họa:\npublic class PostCommentService { // ... Initialize readonly fields ... public async Task\u0026lt;IEnumerable\u0026lt;Comment\u0026gt;\u0026gt; GetPostCommentsAsync( string postId, DateTime startDate, double minSentimentScore) { // Step 1: Query DynamoDB for comments var comments = await _dynamoDbContext.QueryAsync\u0026lt;Comment\u0026gt;(postId, QueryOperator.GreaterThanOrEqual, new [] { startDate.ToString(\u0026#34;yyyy-MM-dd\u0026#34;) }).GetRemainingAsync(); // Step 2: Get user details and filter by active status var userIds = comments.Select(c =\u0026gt; c.UserId).Distinct(); var userDetails = await _userService.GetUserDetailsAsync(userIds); comments = comments.Where(c =\u0026gt; userDetails[c.UserId].Status == \u0026#34;ACTIVE\u0026#34;); // Step 3: Apply sentiment score filter from analytics var commentIds = comments.Select(c =\u0026gt; c.CommentId); var sentimentScores = await _analyticsDb.GetSentimentScoresAsync(commentIds); return comments.Where(c =\u0026gt; sentimentScores[c.CommentId] \u0026gt; minSentimentScore); } } Khi phân tích các truy vấn hiện có, hãy xác định các kịch bản yêu cầu lọc phía client và đánh giá tác động hiệu năng của chúng. Xử lý phân trang (Handling pagination) Đánh giá chiến lược phân trang hiện tại của ứng dụng và điều chỉnh cho phù hợp với khả năng của DynamoDB. Trong khi các ứng dụng cơ sở dữ liệu quan hệ thường hiển thị tổng số trang cho người dùng, DynamoDB được tối ưu cho phân trang theo khóa và tiến tới trước (forward-only), sử dụng LastEvaluatedKey.\nVì việc triển khai các tính năng như tổng số bản ghi cần duyệt toàn bộ bảng (full table scans), hãy cân nhắc các giải pháp thay thế như điều hướng theo con trỏ (cursor-based) hoặc các mẫu “load more\u0026quot;. Đối với các ứng dụng cần ngữ cảnh kích thước tập kết quả, hãy cân nhắc triển khai các bộ đếm (counters) thay vì tính tổng thời gian thực. Khi thiết kế phân trang trong lớp truy cập dữ liệu cho DynamoDB, hiểu hành vi phân trang cốt lõi của nó. DynamoDB có thể không trả tất cả các item khớp trong một cuộc gọi API do hai ràng buộc chính: tham số \u0026ldquo;Limit\u0026rdquo; và kích thước đọc tối đa 1 MB.\nPhân tích tác động của lọc: Đánh giá các bộ lọc truy vấn và tính phân phối (cardinality) của dữ liệu. Tối ưu hóa tham số Limit: Hướng tới giá trị limit phù hợp gần với kích thước trang mong muốn trong khi tính đến tác động của bộ lọc. Giám sát hiệu năng: Theo dõi số cuộc gọi API trên mỗi yêu cầu trang và thời gian phản hồi trung bình. Xử lý các trường hợp biên (Handling edge cases) Khi di chuyển lớp truy cập dữ liệu sang DynamoDB, hãy xác định và giải quyết các trường hợp biên liên quan tới các thao tác dữ liệu ở quy mô lớn:\nCác thao tác khối lượng lớn có thể dự đoán được: Xem xét kịch bản một người dùng có hàng triệu người theo dõi đăng nội dung. Các mẫu thiết kế như write sharding hoặc batch processing có thể giúp quản lý những kịch bản này. Các sự kiện tăng đột biến không mong đợi: Ví dụ, khi một bài đăng bất ngờ nhận được lượng tương tác lớn. Những kịch bản này đòi hỏi các chiến lược như scaling động, caching và mô hình bất đồng bộ. Xử lý các phép tổng hợp và dữ liệu phi chuẩn hóa (Handling aggregations and de-normalized data) Quản lý các phép tổng hợp (Managing aggregations) Các cơ sở dữ liệu thường dùng JOIN và GROUP BY cho các phép tổng hợp thời gian thực. Các mẫu truy cập của DynamoDB hỗ trợ các cách tiếp cận khác như duy trì các thực thể aggregation để lưu các giá trị được tiền tính toán.\nXử lý dữ liệu phi chuẩn hóa (Handling de-normalized data) DynamoDB thường yêu cầu phi chuẩn hóa dữ liệu. Ví dụ, chúng tôi lưu trạng thái người dùng trực tiếp trên các thực thể bài đăng để cho phép lọc hiệu quả. Cách tiếp cận này đánh đổi tăng số thao tác ghi để cải thiện hiệu năng đọc.\nQuản lý cập nhật (Managing updates) Cập nhật đồng bộ (Synchronous updates): Cho các tính năng quan trọng cần tính nhất quán ngay lập tức (ví dụ: dùng giao dịch). Cập nhật bất đồng bộ (Asynchronous updates): Sử dụng Amazon DynamoDB Streams và AWS Lambda cho các cập nhật không cần tính nhất quán ngay lập tức. Xử lý phân tích (Analytical processing) Đối với các truy vấn phân tích phức tạp, hãy cân nhắc các dịch vụ bổ trợ: Amazon Redshift, Zero-ETL integration, Amazon Athena, hoặc Amazon SageMaker Lakehouse.\nKết luận Trong bài viết này, chúng tôi đã khám phá các chiến lược để hiện đại hóa lớp truy cập dữ liệu của ứng dụng cho DynamoDB. Việc chuyển đổi từ các mẫu dựa trên SQL sang cách tiếp cận điều khiển bằng API của DynamoDB đem lại cơ hội tối ưu hóa cách ứng dụng tương tác với dữ liệu của nó. Sự hợp tác chặt chẽ giữa các nhóm cơ sở dữ liệu và ứng dụng giúp tạo ra các giải pháp cân bằng giữa hiệu năng, tối ưu chi phí và khả năng mở rộng.\nVề các tác giả Ramana Kiran Mannava: Chuyên gia Tư vấn Cấp cao tại AWS Professional Services, chuyên sâu về .NET và AWS. Akber Rizwan Shaik: Chuyên gia Tư vấn Cấp cao tại AWS Professional Services. Mahesh Kumar Vemula: Chuyên gia Tư vấn Cấp cao tại AWS Professional Services, đam mê Serverless. "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Familiarized with the working environment and AWS fundamentals by setting up the account, completing core VPC and EC2 labs, and establishing the Hugo documentation site.\nWeek 2: Deepened networking and storage expertise by configuring AWS Transit Gateway, hosting static websites on S3, and implementing data backup and recovery strategies.\nWeek 3: Focused on strengthening security posture through IAM, KMS, and AWS Organizations, while advancing operational capabilities with Amazon FSx and Lambda automation.\nWeek 4: Explored AWS database services and successfully migrated a custom MSSQL database to MySQL using the Schema Conversion Tool, alongside formulating the team workshop proposal.\nWeek 5: Advanced the workshop proposal by refining the architecture and Lambda algorithms, while mastering data streaming and analytics pipelines using Amazon Kinesis, Glue, and Athena.\nWeek 6: Finalized the workshop proposal and architecture by integrating Amazon GuardDuty and EventBridge, and deployed the updated documentation to GitHub Pages.\nWeek 7: Prototyped the automated incident response workflow by integrating GuardDuty with EventBridge for notification and remediation, while enhancing the architecture .\nWeek 8: Dedicated the week to mastering AWS core concepts for the midterm exam while refining the project architecture with AWS Step Functions and EC2 isolation logic.\nWeek 9: Refined the architecture by adopting AWS Step Functions for orchestration and implementing a serverless ETL pipeline to process CloudTrail logs for the custom dashboard.\nWeek 10: Validated the automated response logic by deploying the AWS Step Functions workflow, debugging Lambda parsers against real GuardDuty findings, and enforcing least-privilege security controls.\nWeek 11: Optimized architectural reliability by integrating Amazon SQS and prepared for the Infrastructure as Code transition using AWS CDK, while strengthening edge security knowledge through AWS WAF and CloudFront.\nWeek 12: Transitioned to Infrastructure as Code by initializing the AWS CDK environment and enhanced the incident response workflow with automated forensics collection.\n"},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect with FCJ members and mentors. Find out what working in an office is like. Install Linux, learn how to properly use Linux. Learn the basics of AWS. Complete first and second module. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon - Read internship rules\n- Create AWS account - Learnt what AWS is - Module 1 Lab 1 Done (Learnt how to create AWS account and manage user groups) - Module 1 Lab 7 Done (Learnt how to create budgets of using the service) - Lab 7-3 (Usage Budget) cannot be done, error in usage type dropdown, showing nothing - Module 1 Lab 9 Done (Learnt about AWS Support Services, Its type, benefits and how to request supports) 08/09/2025 08/09/2025 Create new AWS Account MFA for AWS Accounts Create Admin Group and Admin User Account Authentication Support Explore and Configure AWS Management Console Creating Support Cases and Case Management in AWS Tue - Get started on Module 2 theory:\n+ Learnt about VPC (Amazon Virtual Private Cloud)\n+ Learnt about Subnets and Routetable, Security Groups\n+ Learnt about ENI and EIP\n+ Learnt about VPC Peering and Transit Gateway + Learnt about Elastic Load Balancing\n+ Learnt about EC2\n- Setup site for workshop report - Installed Hugo - Successfully write worklog using markdown and Hugo 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ Wed -Sun - Complete Module 2\u0026rsquo;s labs\n- Lab 3: + Learnt about the resources necessary to create and run EC2 instances + Successfully configured and run EC2 instances + Successfully connect and ping to EC2 instances + Created NAT Gateway to allow private EC2 connections - Lab 10: + Learnt how to create and use keypair for security + Learnt how to configure security group to manage connections + Successfully connect and use RDP via EC2 + Set up hybrid DNS with Route 53 Resolver (In progress, Cloud Formation template didn\u0026rsquo;t create security group to proceed with the lab) - Lab 19: + Successfully created VPC Peering Connection + Learnt how to configure Network ACLs + Enabled Cross-Peer DNS to resolve private host names - Downloaded and used MobaXTerm to connect to EC2 instances - Downloaded and used Putty to configures keypairs 10/09/2025 12/09/2025 Lab 3 Lab 10 Lab 19 Week 1 Achievements: Created and secured an AWS account, including setting up budgets and exploring support services. Completed theory and practical labs for VPC, Subnets, Security Groups, and Routetables. Successfully deployed and connected to EC2 instances, configured a NAT Gateway, and established connections using VPC Peering. Documentation Setup: Successfully installed Hugo and configured the site for writing the worklog using markdown. Tool Proficiency: Learned to use MobaXTerm and PuTTY for connecting to and managing EC2 instances. Learned cost management through budget alerts and gained experience troubleshooting CloudFormation templates during lab exercises. Completed Module 1 and Module 2. "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.11-appendices/5.11.1-cloudtrail-etl/","title":"CloudTrail ETL Code","tags":[],"description":"","content":" import json import boto3 import gzip import re import os from datetime import datetime, timezone s3 = boto3.client(\u0026#34;s3\u0026#34;) firehose= boto3.client(\u0026#34;firehose\u0026#34;) # -------------------------------------------------- # CONFIG # -------------------------------------------------- SOURCE_PREFIX = \u0026#34;exportedlogs/vpc-dns-logs/\u0026#34; FIREHOSE_STREAM_NAME = os.environ.get(\u0026#34;FIREHOSE_STREAM_NAME\u0026#34;) VPC_RE = re.compile(r\u0026#34;/(vpc-[0-9A-Za-z\\-]+)\u0026#34;) ISO_TS_RE = re.compile(r\u0026#34;^\\d{4}-\\d{2}-\\d{2}T\u0026#34;) def read_gz(bucket, key): obj = s3.get_object(Bucket=bucket, Key=key) with gzip.GzipFile(fileobj=obj[\u0026#34;Body\u0026#34;]) as f: return f.read().decode(\u0026#34;utf-8\u0026#34;, errors=\u0026#34;replace\u0026#34;) def flatten_once(d): out = {} for k, v in (d or {}).items(): if isinstance(v, dict): for k2, v2 in v.items(): out[f\u0026#34;{k}_{k2}\u0026#34;] = v2 else: out[k] = v return out def safe_int(x): try: return int(x) except: return None def parse_dns_line(line): raw = line.strip() if not raw: return None json_part = raw prefix_ts = None if ISO_TS_RE.match(raw): try: prefix_ts, rest = raw.split(\u0026#34; \u0026#34;, 1) json_part = rest except: pass if not json_part.startswith(\u0026#34;{\u0026#34;): idx = json_part.find(\u0026#34;{\u0026#34;) if idx != -1: json_part = json_part[idx:] try: obj = json.loads(json_part) except: return None flat = flatten_once(obj) if prefix_ts: flat[\u0026#34;_prefix_ts\u0026#34;] = prefix_ts return flat def lambda_handler(event, context): print(f\u0026#34;Received S3 Event. Records: {len(event.get(\u0026#39;Records\u0026#39;, []))}\u0026#34;) firehose_records = [] for record in event.get(\u0026#34;Records\u0026#34;, []): if \u0026#34;s3\u0026#34; not in record: continue bucket = record[\u0026#34;s3\u0026#34;][\u0026#34;bucket\u0026#34;][\u0026#34;name\u0026#34;] key = record[\u0026#34;s3\u0026#34;][\u0026#34;object\u0026#34;][\u0026#34;key\u0026#34;] if not key.startswith(SOURCE_PREFIX) or not key.endswith(\u0026#34;.gz\u0026#34;): print(f\u0026#34;Skipping file: {key}\u0026#34;) continue print(f\u0026#34;Processing S3 file: {key}\u0026#34;) # Extract VPC ID from file path vpc_id_match = VPC_RE.search(key) vpc_id = vpc_id_match.group(1) if vpc_id_match else \u0026#34;unknown\u0026#34; # Read and process file content content = read_gz(bucket, key) if not content: continue for line in content.splitlines(): r = parse_dns_line(line) if not r: continue # Create flattened JSON record out = { \u0026#34;version\u0026#34;: r.get(\u0026#34;version\u0026#34;), \u0026#34;account_id\u0026#34;: r.get(\u0026#34;account_id\u0026#34;), \u0026#34;region\u0026#34;: r.get(\u0026#34;region\u0026#34;), \u0026#34;vpc_id\u0026#34;: r.get(\u0026#34;vpc_id\u0026#34;, vpc_id), \u0026#34;query_timestamp\u0026#34;: r.get(\u0026#34;query_timestamp\u0026#34;), \u0026#34;query_name\u0026#34;: r.get(\u0026#34;query_name\u0026#34;), \u0026#34;query_type\u0026#34;: r.get(\u0026#34;query_type\u0026#34;), \u0026#34;query_class\u0026#34;: r.get(\u0026#34;query_class\u0026#34;), \u0026#34;rcode\u0026#34;: r.get(\u0026#34;rcode\u0026#34;), \u0026#34;answers\u0026#34;: json.dumps(r.get(\u0026#34;answers\u0026#34;), ensure_ascii=False), \u0026#34;srcaddr\u0026#34;: r.get(\u0026#34;srcaddr\u0026#34;), \u0026#34;srcport\u0026#34;: safe_int(r.get(\u0026#34;srcport\u0026#34;)), \u0026#34;transport\u0026#34;: r.get(\u0026#34;transport\u0026#34;), \u0026#34;srcids_instance\u0026#34;: r.get(\u0026#34;srcids_instance\u0026#34;), \u0026#34;timestamp\u0026#34;: (r.get(\u0026#34;query_timestamp\u0026#34;) or r.get(\u0026#34;timestamp\u0026#34;) or r.get(\u0026#34;_prefix_ts\u0026#34;)) } # Add newline for JSONL format json_row = json.dumps(out, ensure_ascii=False) + \u0026#34;\\n\u0026#34; firehose_records.append({\u0026#39;Data\u0026#39;: json_row}) # Send to Firehose in batches of 500 if firehose_records: total_records = len(firehose_records) print(f\u0026#34;Sending {total_records} records to Firehose...\u0026#34;) batch_size = 500 for i in range(0, total_records, batch_size): batch = firehose_records[i:i + batch_size] try: response = firehose.put_record_batch( DeliveryStreamName=FIREHOSE_STREAM_NAME, Records=batch ) if response[\u0026#39;FailedPutCount\u0026#39;] \u0026gt; 0: print(f\u0026#34;Warning: {response[\u0026#39;FailedPutCount\u0026#39;]} records failed\u0026#34;) except Exception as e: print(f\u0026#34;Firehose error: {e}\u0026#34;) return {\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;, \u0026#34;total_records\u0026#34;: len(firehose_records)} "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.5-processing-setup/5.5.1-create-kinesis-data-firehose/","title":"Create Kinesis Data Firehose","tags":[],"description":"","content":"Create Kinesis Data Firehose Delivery Streams Create cloudtrail-firehose-stream Open Kinesis Console → Delivery streams → Create delivery stream\nConfigure:\nSource: Direct PUT Destination: Amazon S3 Stream name: cloudtrail-firehose-stream S3 bucket: processed-cloudtrail-logs-ACCOUNT_ID-REGION Prefix: processed-cloudtrail/date=!{timestamp:yyyy-MM-dd}/ Error prefix: processed-cloudtrail/errors/date=!{timestamp:yyyy-MM-dd}/error-type=!{firehose:error-output-type}/ Buffer size: 10 MB Buffer interval: 300 seconds Compression: GZIP IAM role: CloudTrailFirehoseRole Create delivery stream\nCreate vpc-dns-firehose-stream Stream name: vpc-dns-firehose-stream S3 bucket: processed-cloudwatch-logs-ACCOUNT_ID-REGION Prefix: vpc-logs/date=!{timestamp:yyyy-MM-dd}/ Error prefix: vpc-logs/errors/date=!{timestamp:yyyy-MM-dd}/error-type=!{firehose:error-output-type}/ IAM role: CloudWatchFirehoseRole (Same buffer/compression settings as above) Create vpc-flow-firehose-stream Stream name: vpc-flow-firehose-stream S3 bucket: processed-cloudwatch-logs-ACCOUNT_ID-REGION Prefix: eni-flow-logs/date=!{timestamp:yyyy-MM-dd}/ Error prefix: eni-flow-logs/errors/date=!{timestamp:yyyy-MM-dd}/error-type=!{firehose:error-output-type}/ IAM role: CloudWatchFirehoseRole (Same buffer/compression settings as above) "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.3-foundation-setup/5.3.3-create-iam-roles-and-policies/5.3.3.1-create-lambda-excecution-roles/","title":"Create Lambda Execution Roles","tags":[],"description":"","content":"Create CloudTrailETLLambdaServiceRole Open the IAM Console:\nNavigate to https://console.aws.amazon.com/iam/ Or: AWS Management Console → Search for \u0026ldquo;IAM\u0026rdquo; → Click \u0026ldquo;IAM\u0026rdquo; Navigate to Roles:\nIn the left sidebar, click \u0026ldquo;Roles\u0026rdquo; Click \u0026ldquo;Create role\u0026rdquo;\nSelect trusted entity:\nTrusted entity type: Select \u0026ldquo;AWS service\u0026rdquo; Use case: Select \u0026ldquo;Lambda\u0026rdquo; Click \u0026ldquo;Next\u0026rdquo; Add permissions:\nIn the search box, type AWSLambdaBasicExecutionRole Check the box next to \u0026ldquo;AWSLambdaBasicExecutionRole\u0026rdquo; Click \u0026ldquo;Next\u0026rdquo; Name, review, and create:\nRole name: Enter CloudTrailETLLambdaServiceRole Description: Enter Execution role for CloudTrail ETL Lambda function Click \u0026ldquo;Create role\u0026rdquo; Add inline policy:\nAfter creation, you\u0026rsquo;ll be on the role details page Click on the \u0026ldquo;Permissions\u0026rdquo; tab Click \u0026ldquo;Add permissions\u0026rdquo; → \u0026ldquo;Create inline policy\u0026rdquo; Create inline policy:\nClick on the \u0026ldquo;JSON\u0026rdquo; tab Paste the following policy (replace ACCOUNT_ID and REGION): { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;firehose:PutRecord\u0026#34;, \u0026#34;firehose:PutRecordBatch\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:firehose:REGION:ACCOUNT_ID:deliverystream/cloudtrail-firehose-stream\u0026#34; } ] } Click \u0026ldquo;Next\u0026rdquo;\nPolicy name:\nPolicy name: Enter CloudTrailETLPolicy Click \u0026ldquo;Create policy\u0026rdquo; Verify role creation:\nYou should see the role with both managed and inline policies attached Create Remaining Lambda Roles Follow the same process for each role below (steps 3-11):\nGuardDutyETLLambdaServiceRole\nRole name: GuardDutyETLLambdaServiceRole Description: Execution role for GuardDuty ETL Lambda function Managed policy: AWSLambdaBasicExecutionRole Inline policy name: GuardDutyETLPolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/*\u0026#34;, \u0026#34;arn:aws:s3:::processed-guardduty-findings-ACCOUNT_ID-REGION/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;kms:Decrypt\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:kms:REGION:ACCOUNT_ID:key/*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:CreatePartition\u0026#34;, \u0026#34;glue:GetPartition\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:glue:REGION:ACCOUNT_ID:catalog\u0026#34;, \u0026#34;arn:aws:glue:REGION:ACCOUNT_ID:database/security_logs\u0026#34;, \u0026#34;arn:aws:glue:REGION:ACCOUNT_ID:table/security_logs/processed_guardduty\u0026#34; ] } ] } CloudWatchETLLambdaServiceRole\nRole name: CloudWatchETLLambdaServiceRole Description: Execution role for VPC DNS logs ETL Lambda Managed policy: AWSLambdaBasicExecutionRole Inline policy name: CloudWatchETLPolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;firehose:PutRecord\u0026#34;, \u0026#34;firehose:PutRecordBatch\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:firehose:REGION:ACCOUNT_ID:deliverystream/vpc-dns-firehose-stream\u0026#34; } ] } CloudWatchENIETLLambdaServiceRole\nRole name: CloudWatchENIETLLambdaServiceRole Description: Execution role for VPC Flow logs ETL Lambda Managed policy: AWSLambdaBasicExecutionRole Inline policy name: CloudWatchENIETLPolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;firehose:PutRecord\u0026#34;, \u0026#34;firehose:PutRecordBatch\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:firehose:REGION:ACCOUNT_ID:deliverystream/vpc-flow-firehose-stream\u0026#34; } ] } CloudWatchExportLambdaServiceRole\nRole name: CloudWatchExportLambdaServiceRole Description: Execution role for CloudWatch log export Lambda Managed policy: AWSLambdaBasicExecutionRole Inline policy name: CloudWatchExportPolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateExportTask\u0026#34;, \u0026#34;logs:DescribeExportTasks\u0026#34;, \u0026#34;s3:PutObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:REGION:ACCOUNT_ID:log-group:*\u0026#34;, \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/*\u0026#34; ] } ] } ParseFindingsLambdaServiceRole\nRole name: ParseFindingsLambdaServiceRole Description: Execution role for parsing GuardDuty findings Managed policy: AWSLambdaBasicExecutionRole No inline policy needed IsolateEC2LambdaServiceRole\nRole name: IsolateEC2LambdaServiceRole Description: Execution role for isolating compromised EC2 instances Managed policy: AWSLambdaBasicExecutionRole Inline policy name: IsolateEC2Policy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:DescribeInstances\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } QuarantineIAMLambdaServiceRole\nRole name: QuarantineIAMLambdaServiceRole Description: Execution role for quarantining compromised IAM users Managed policy: AWSLambdaBasicExecutionRole Inline policy name: QuarantineIAMPolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iam:AttachUserPolicy\u0026#34;, \u0026#34;iam:ListAttachedUserPolicies\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:iam::ACCOUNT_ID:user/*\u0026#34;, \u0026#34;arn:aws:iam::ACCOUNT_ID:policy/IrQuarantineIAMPolicy\u0026#34; ] } ] } AlertDispatchLambdaServiceRole\nRole name: AlertDispatchLambdaServiceRole Description: Execution role for dispatching alerts via SNS/SES/Slack Managed policy: AWSLambdaBasicExecutionRole Inline policy name: AlertDispatchPolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;sns:Publish\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:sns:REGION:ACCOUNT_ID:IncidentResponseAlerts\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ses:SendEmail\u0026#34;, \u0026#34;ses:SendRawEmail\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.7-dashboard-setup/5.7.2-setup-lambda/5.7.2.1-create-iam-role-and-policy-for-lambda/","title":"Lambda IAM Role and Policy setup","tags":[],"description":"","content":"In this guide, you will setup IAM Role and Policy for Lambda.\nCreate IAM Role for Lambda Open the IAM Console\nNavigate to https://console.aws.amazon.com/iam/ Or: AWS Management Console → Services → IAM Create Role:\nChoose the Role option on the left menu panel. Then click Create role. Select trusted entity:\nTrusted entity type: AWS Service Use case: Lambda Click \u0026ldquo;Next\u0026rdquo; Attach permissions policies:\nIn the search box, type AWSLambdaBasicExecutionRole Check the box next to \u0026ldquo;AWSLambdaBasicExecutionRole\u0026rdquo; Click \u0026ldquo;Next\u0026rdquo; Name, review, and create:\nRole name: Enter dashboard-query-role Description: Enter Execution role for Lambda function Click \u0026ldquo;Create role\u0026rdquo; Add inline policy:\nAfter creation, you\u0026rsquo;ll be on the role details page Click on the \u0026ldquo;Permissions\u0026rdquo; tab Click \u0026ldquo;Add permissions\u0026rdquo; → \u0026ldquo;Create inline policy\u0026rdquo; Create inline policy:\nClick on the \u0026ldquo;JSON\u0026rdquo; tab Paste the following policy: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AthenaActions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;athena:StartQueryExecution\u0026#34;, \u0026#34;athena:GetQueryExecution\u0026#34;, \u0026#34;athena:GetQueryResults\u0026#34;, \u0026#34;athena:StopQueryExecution\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;GlueCatalogRead\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:GetDatabase\u0026#34;, \u0026#34;glue:GetDatabases\u0026#34;, \u0026#34;glue:GetTable\u0026#34;, \u0026#34;glue:GetTables\u0026#34;, \u0026#34;glue:GetPartitions\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;S3SourceAndResultAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::vel-athena-results\u0026#34;, \u0026#34;arn:aws:s3:::vel-athena-results/*\u0026#34;, \u0026#34;arn:aws:s3:::vel-processed-cloudtrail-logs\u0026#34;, \u0026#34;arn:aws:s3:::vel-processed-cloudtrail-logs/*\u0026#34;, \u0026#34;arn:aws:s3:::vel-processed-guardduty\u0026#34;, \u0026#34;arn:aws:s3:::vel-processed-guardduty/*\u0026#34;, \u0026#34;arn:aws:s3:::cloudwatch-formatted\u0026#34;, \u0026#34;arn:aws:s3:::cloudwatch-formatted/*\u0026#34; ] } ] } Click \u0026ldquo;Next\u0026rdquo;\nPolicy name:\nPolicy name: Enter lambda-query-policy Click \u0026ldquo;Create policy\u0026rdquo; Verify role creation:\nYou should see the role with both managed and inline policies attached "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.1-workshop-overview/","title":"Overview","tags":[],"description":"","content":"System Components Auto Incident Response and Forensics is an architecture that uses automation services to ingest, process, and automatically respond to security findings, minimizing the time required for human intervention and aids security personel in visualizing and analyzing logs. This system is built around AWS Security Services (CloudTrail, GuardDuty, VPC Flow Logs, CloudWatch) feeding data into a Centralized Data Lake (S3/Glue/Athena) for analysis. The core automation is driven by AWS EventBridge rules triggering AWS Step Functions workflows, which then execute AWS Lambda functions to perform isolation and alerting actions. Workshop overview In this workshop, you will deploy a multi-phase system to achieve end-to-end security automation. This includes:\nFoundation Setup: Creating dedicated S3 buckets and IAM roles to support all services. Monitoring Setup: Enabling and configuring key security logs (CloudTrail, GuardDuty, VPC Flow Logs) to direct data to the central log ingestion point. Processing Setup: Deploying Kinesis Firehose, Lambda ETLs, and Glue/Athena tables to transform raw logs into an easily queryable security data lake. Automation Setup: Creating the Isolation Security Group, SNS Topic, Incident Response Lambda Functions, and the Step Functions State Machine that executes automatic quarantine actions when GuardDuty detects findings. Dashboard Setup: Hosting a secure, S3-based static web interface accelerated by CloudFront and protected by Cognito to provide analysts with real-time visualization of forensic data and direct query capabilities via API Gateway. "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.2-prerequiste/","title":"Prerequisites","tags":[],"description":"","content":"Required Access and Information Before proceeding with the setup of the Automated AWS Incident Response and Forensics System, ensure you have gathered the required access credentials and information below.\n🔑 Access \u0026amp; Identifiers AWS Account with Administrative Access You need full administrative permissions to create resources across multiple AWS services. Access to the AWS Management Console. Your AWS Account ID Format: 12-digit number (e.g., 123456789012). Placeholder: Replace ACCOUNT_ID throughout the guide. Target AWS Region Choose the region where you\u0026rsquo;ll deploy the system (e.g., us-east-1). Placeholder: Replace REGION throughout the guide. VPC ID A VPC with at least one subnet is required for VPC Flow Logs. Placeholder: Replace YOUR_VPC_ID in the guide. Amazon SES Verified Email Address Required for sending and recieving email alerts. Verify this address in the SES Console. Placeholder: Replace YOUR_VERIFIED_EMAIL@example.com. Slack Webhook URL (Optional) If you want Slack notifications, obtain a webhook URL from your Slack workspace. Placeholder: Replace YOUR_SLACK_WEBHOOK_URL. "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.3-foundation-setup/5.3.1-set-up-s3-buckets/","title":"Set up S3 buckets","tags":[],"description":"","content":"In this section, you will create 5 S3 buckets that serve as the foundation for the Auto Incident Response system.\nImportant: Replace ACCOUNT_ID with your AWS Account ID and REGION with your target region (e.g., us-east-1) in all bucket names.\nBucket Names incident-response-log-list-bucket-ACCOUNT_ID-REGION - Primary log collection bucket processed-cloudtrail-logs-ACCOUNT_ID-REGION - Stores processed CloudTrail logs athena-query-results-ACCOUNT_ID-REGION - Stores Athena query results processed-cloudwatch-logs-ACCOUNT_ID-REGION - Stores processed CloudWatch logs processed-guardduty-findings-ACCOUNT_ID-REGION - Stores processed GuardDuty findings Bucket Creation Instructions Open the Amazon S3 Console Navigate to https://console.aws.amazon.com/s3/ Or: AWS Management Console → Services → S3 Click on \u0026ldquo;Create bucket\u0026rdquo; General configuration: Bucket name: Enter incident-response-log-list-bucket-ACCOUNT_ID-REGION Example: incident-response-log-list-bucket-123456789012-us-east-1 AWS Region: Select your target region (e.g., US East (N. Virginia) us-east-1) Object Ownership:\nKeep default: ACLs disabled (recommended) Block Public Access settings for this bucket:\nCheck \u0026ldquo;Block all public access\u0026rdquo; Ensure all 4 sub-options are checked: ✓ Block public access to buckets and objects granted through new access control lists (ACLs) ✓ Block public access to buckets and objects granted through any access control lists (ACLs) ✓ Block public access to buckets and objects granted through new public bucket or access point policies ✓ Block public and cross-account access to buckets and objects through any public bucket or access point policies Bucket Versioning:\nSelect \u0026ldquo;Enable\u0026rdquo; Tags (optional):\nAdd tags if desired Example: Key=Purpose, Value=IncidentResponse Default encryption:\nEncryption type: Select \u0026ldquo;Server-side encryption with Amazon S3 managed keys (SSE-S3)\u0026rdquo; Bucket Key: Keep default (Enabled) Advanced settings:\nKeep all defaults Click \u0026ldquo;Create bucket\u0026rdquo;\nVerify bucket creation:\nYou should see a success message The bucket should appear in your S3 buckets list Repeat steps 2-10 for the remaining 4 buckets:\nprocessed-cloudtrail-logs-ACCOUNT_ID-REGION athena-query-results-ACCOUNT_ID-REGION processed-cloudwatch-logs-ACCOUNT_ID-REGION processed-guardduty-findings-ACCOUNT_ID-REGION Verify all 5 buckets are created:\nNavigate to S3 Console You should see all 5 buckets listed "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.7-dashboard-setup/5.7.1-setup-s3/","title":"Setup S3 Bucket for Dashboard","tags":[],"description":"","content":"In this guide, you will setup a S3 to contain web files and folder. Important: Replace ACCOUNT_ID with your AWS Account ID and REGION with your target region (e.g., us-east-1) in all bucket names.\nBucket Names static-dashboard-bucket-ACCOUNT_ID-REGION - Store builded web files and folder\nBucket Creation Instructions Open the Amazon S3 Console\nNavigate to https://console.aws.amazon.com/s3/ Or: AWS Management Console → Services → S3 Click on \u0026ldquo;Create bucket\u0026rdquo;\nBucket create setting:\nKeep the setting like default: Bucket name: Enter static-dashboard-bucket-ACCOUNT_ID-REGION Example: static-dashboard-bucket-123456789012-us-east-1 Ownership: ACLs disabled Block Public Access: Block all public access Bucket versioning: Disable Tags(Optional): Add if you want Encryption: SSE-S3 Bucket key: Enable Click Create bucket Verify bucket creation:\nYou should see a success message The bucket should appear in your S3 buckets list Upload files and folder:\nGo to Github to get the web content and upload to S3 "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/2-proposal/","title":"Proposal","tags":[],"description":"","content":"\nAutomated AWS Incident Response and Forensics System Proposal Link: Proposal Doc 1. Executive Summary Our team is building an automated incident response and forensics solution as part of the AWS First Cloud Journey internship program. The idea is straightforward—when a security issue happens in AWS, we want the system to respond automatically without waiting for manual intervention.\nWe\u0026rsquo;re creating a platform that automatically detects security findings from GuardDuty, isolates affected resources, captures forensic evidence through comprehensive data collection, and provides analytics and dashboards where security teams can investigate what happened. Everything is built using Infrastructure-as-Code with AWS CDK, so customers can easily deploy it into their own AWS accounts.\n2. Problem Statement What’s the Problem? The increasing frequency and sophistication of cyber threats pose significant risks to organizations relying on cloud infrastructure. Manual incident response processes are often slow, inconsistent, and prone to human error, which can lead to prolonged system downtime, data breaches, and financial losses. The project aims to address these challenges by developing an automated, reliable, and scalable incident response system that minimizes response time, enhances forensic capabilities, and reduces operational costs.\nThe Solution The main use cases include detecting unauthorized AWS credential use, identifying compromised EC2 instances, and ensuring forensic data is properly collected, processed, and stored for investigation. Our architecture integrates VPC Flow Logs, CloudTrail, CloudWatch, and GuardDuty to detect threats, while Step Functions orchestrates the automated response workflow including EC2 isolation, ASG detachment, Create Snapshot and IAM quarantine. All evidence is collected and processed through custom ETL Lambda and Data Firehose, using Athena for forensic analysis. The system also includes alert dispatching, notification via messaging and email, and provides dashboards and analytics for security teams to investigate what happened.\nBenefits and Return on Investment Rapid threat detection: Automated response reduces the window of vulnerability. Comprehensive evidence gathering: Automated forensic data collection facilitates faster investigations. Cost-effective deployment: Leveraging AWS serverless services minimizes infrastructure expenses. Improved security posture: Continuous monitoring and real-time alerts. Actionable insights: Dashboards and analytics empower security teams. Scalability: Adaptable to organizations of various sizes and incident volumes. 3. Solution Architecture Our solution uses a comprehensive multi-stage architecture for automated incident response and forensics:\nAWS Services Used Amazon GuardDuty: Continuously monitors for security threats and suspicious activity. AWS Step Functions: Orchestrates the incident response workflow. AWS Lambda: Runs automation code for isolation and data processing. Amazon EventBridge: Routes findings from GuardDuty to Step Functions. Amazon S3: Stores forensic evidence and hosts static dashboard. Amazon Athena: Enables SQL queries against forensic datasets. Amazon API Gateway: Facilitates communication between dashboard and backend. Amazon Cognito: Secures access for dashboard users. Amazon CloudFront: Accelerates dashboard delivery across the globe. Amazon SNS \u0026amp; SES: Handles notifications via messaging and email. AWS CloudTrail: Logs all actions for auditing. Amazon CloudWatch: Monitoring and dashboards. Amazon EC2: Optional instances for analysis. AWS KMS: Key management for encryption. Amazon Kinesis Data Firehose: Streams data to S3. Component Design Data Collection \u0026amp; Detection Layer: Collects events from VPC Flow Logs, CloudTrail, CloudWatch, EC2, and GuardDuty. Event Processing Layer: Alert Dispatch, EventBridge routes findings to Step Functions; events are classified by type. Automated Response Orchestration: Step Functions handle parsing, decision making, EC2 isolation, termination protection, ASG detachment, snapshot creation, and IAM quarantine. Alerting \u0026amp; Notification Layer: SNS, Slack \u0026amp; SES handles notifications via messaging and email, Alert Dispatch. Data Processing \u0026amp; Analytics Layer: ETL pipeline with Lambda and Data Firehose processes raw logs into S3; Athena queries the data. Dashboard \u0026amp; Analysis Layer: S3-hosted React dashboard with Cognito auth, consuming data via API Gateway and Athena. 4. Technical Implementation Implementation Phases We use Agile Scrum with 1-week sprints over 6 weeks:\nSprint 1: Foundation \u0026amp; Setup (VPC, Security Groups, Training). Sprint 2: Core Orchestration (Step Functions, Lambda, GuardDuty integration). Sprint 3: Data \u0026amp; Analytics (S3, Athena, ETL pipeline). Sprint 4: Dashboard \u0026amp; UI (Static site, API Gateway, CloudFront). Sprint 5: Testing \u0026amp; Optimization (Cognito, Performance testing, Simulations). Sprint 6: Documentation \u0026amp; Handover (Guides, Demos, Final Polish). Technical Requirements Frontend \u0026amp; Dashboards: Custom HTML/CSS/JS hosted on S3, served via CloudFront. Backend \u0026amp; Processing: Python 3.12 for Lambda, Step Functions for orchestration. Data \u0026amp; Storage: S3 for evidence, Athena for querying, Firehose for streaming. Infrastructure: All defined in AWS CDK (Python). Security: GuardDuty for detection, IAM for least privilege, KMS for encryption. 5. Timeline \u0026amp; Milestones Project Timeline Project Timeline\nWeek 6-7 (Foundation \u0026amp; Setup) Activities: Team training on GuardDuty/Step Functions, architecture design review, VPC and security setup. Deliverables: Architecture document v1, team training completion, GitHub repository established. Week 7-9 (Core Orchestration) Activities: Step Functions workflow development, Lambda function coding for all response actions, EventBridge integration, SNS/SES setup, integration testing. Deliverables: Step Functions state machine definition, 7+ Lambda functions with documentation, GuardDuty integration, notification system, API Gateway. Week 10 (Data \u0026amp; Analytics) Activities: S3 forensic storage setup, Athena table creation, ETL pipeline development, SQL query library. Deliverables: 15+ Athena queries documented, forensic analysis runbooks, processed data storage. Week 11 (Dashboard \u0026amp; UI) Activities: Static dashboard development, Cognito authentication, API Gateway setup, CloudFront CDN configuration, dashboard integration. Deliverables: S3-hosted dashboard, authentication system, query interface, real-time results integration. Week 12 (Testing \u0026amp; Validation \u0026amp; Optimization) Activities: Manual testing, security scanning including simulated incident scenarios (5+ workflows), performance testing, attack simulation. Optimize data with Athena query and Data Firehose. Deliverables: Security scan results, incident simulation videos, data optimization. Week 13 (Documentation \u0026amp; Handover) Activities: Deployment guide, API documentation, knowledge transfer sessions, final demo, GitHub cleanup. Deliverables: Complete GitHub repository (public), deployment guide instructions, live workshop demonstration. 6. Budget Estimation You can find the detailed budget estimation on the AWS Pricing Calculator.\nInfrastructure Costs Typical monthly deployment cost (Free Tier / Low scale): ~$5.01\nGuardDuty: ~$1.80/month S3: ~$1.07/month KMS: ~$1.12/month CloudTrail: ~$0.55/month Athena: ~$0.29/month Amazon Simple Email Service (SES): ~$0.09/month Amazon API Gateway: ~$0.05/month Amazon Data firehose: ~$0.04/month Lambda, Step Functions, SNS: Generally within Free Tier limits for typical usage. Note: Costs assume typical usage of 20-150 incidents per month.\n7. Risk Assessment Risk Matrix Performance Bottlenecks: High data volume slowing down queries. Security Breaches: Compromise of the forensic data itself. Cost Overruns: Unchecked logging or infinite loops. Mitigation Strategies Performance: Monitor Athena/Firehose; optimize queries; dynamic resource adjustment. Security: Encryption (KMS), strict IAM roles, audit logging, compliance checks. Cost: AWS budget alerts, cost anomaly detection, auto-scaling limits. Disaster Recovery: Backups, failover procedures, and redundancy measures. 8. Expected Outcomes Technical Improvements Automated Response: Zero-touch isolation of compromised resources. Speed: Reduction of investigation time from hours to minutes. Reliability: Consistent, repeatable evidence collection without human error. Long-term Value Scalable Architecture: Foundation for future security automation. Knowledge: Team competency in advanced AWS security and serverless concepts. Reusable Asset: A deployable solution for other AWS customers or teams. Status: Ready for Review \u0026amp; Approval Project Code: AWS-FCJ-IR-FORENSICS-2025\n"},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: “AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS” Event Objectives Introduce AI/ML/GenAI on AWS Speakers Lam Tuan Kiet – Sr DevOps Engineer, FPT Software Danh Hoang Hieu Nghi - AI Engineer, Renova Cloud Dinh Le Hoang Anh - Cloud Engineer Trainee, First Cloud AI Journey Van Hoang Kha - Community Builder Key Highlights Explored Generative AI with Amazon Bedrock: - Foundation Models: Different from Traditional Model in a sense that it can be adapted for many tasks, provided many fully managed model from leading AI Companies such as: OpenAI, Claude, Anthropic, etc.\n- Promt Engineering: Crafting and Refining Instructions\nZero-Shot Prompting: A prompt with no prior context or example Few-shot Prompting: A prompt with a few prior context and example Chain of Thought: A prompt with thought processes and steps for the actual answer - Retrieval Augmented Generation(RAG): Retrieving relevant information from a data source R: Retrieval - Retrieves relevant infomation from a knowledge base or data sources A: Augmented - Adding the information retrieved as additional context in the user\u0026rsquo;s prompt before inputting it into the model G: Generation - Responses from the model for the augmented prompt Use cases: Improved content quality, contextual chatbots and question answering, personalized search and real time data summarization - Amazon Titan Embedding: Light weight model that excel in translates text into numerical representations(embeddings) for high accuracy retrieval tasks, with support for 100+ languages\n- Pretrained AI Services:\nAmazon Rekognition: Image and Video analysis Amazon Translate: Detect and translate text Amazon Textract: Extract Texts and Layouts from documents Amazon Transcribe: Speech-to-text Amazon Polly: Text-to-speech Amazon Comprehend: Extract Insights and Relationships from text Amazon Kendra: Intelligent Search Service Amazon Lookout: Detect Anomalies in business metrics, equipments and images Amazon Personalize: Tailor recommendations to user - Demo: AMZPhoto: Face recognitions from images using AI\n- Amazon Bedrock AgentCore: A comprehensive agentic platform designed to address challenges in bringing agents into production:\nSecurely execute and scale agent code. Incorporate memory (remembering past interactions and learning). Implement identity and access controls for agents and tools. Provide agentic tool use for complex workflows. Discover and connect with custom tools and resources. Understand and audit every interaction (observability). + Foundational Services: These services are categorized for running agents securely at scale. + Enhance with tools \u0026amp; memory: Includes Memory, Gateway, Browser tool, and Code Interpreter.\n+ Deploy securely at scale: Includes Runtime and Identity.\n+ Gain operational insights: Includes Observability.\n+ Enabling Agents at Scale (Architecture): Connects to the AgentCore Gateway (via MCP), Memory, Identity, Observability, Browser, and Code Interpreter.\n+ Frameworks for Building Agents: CrewAI, Google ADK, LangGraph/LangChain, LlamaIndex, OpenAI Agents SDK, and Strands Agents SDK.\nKey Takeaways Bedrock is the GenAI Hub: Amazon Bedrock provides fully managed Foundation Models from top companies for many different tasks.\nCustomization via Prompts and Data: Various ways to prompts (Zero-Shot, Few-shot, CoT) and ultilize RAG to add info for better model responses.\nEmbeddings Power Search: Amazon Titan Embedding is a key lightweight model for translating text to numbers, which helps achieve high accuracy in retrieval tasks (like RAG).\nPretrained Models: AWS offers many ready-to-use AI services for common needs, like Rekognition for images and Textract for documents.\nAgentCore Solves Production Issues: Amazon Bedrock AgentCore is the new comprehensive platform that handles the difficult parts of running AI Agents at scale (like Memory, Identity, and Observability).\nApplying to Work Very useful in our team\u0026rsquo;s later projects which could include more usage of AI Foundation Models in our architecture. Event Experience The speakers are very well spoken and informative Q\u0026amp;A: Team member ask a out of topic question but crucial to our project Q: The SNS in our architecture which is used to processes Guard Duty Findings have encountered a situation where over 1000+ alert appear at once, how would we resolve this? A: Add SQS to queue the events and ensure not one alert is to be missed Placed top 10 in the end of event Kahoot Quiz and got a picture with the speakers Created an unofficial group: \u0026ldquo;Mèo Cam Đeo Khăn\u0026rdquo;, a joint collaboration between my group \u0026ldquo;The Ballers\u0026rdquo; and \u0026ldquo;Vinhomies\u0026rdquo; Some event photos "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":" Week 2 Objectives: Understand and configure AWS Transit Gateway. Learn about AWS Storage services (EBS, EFS, S3). Discuss workshop ideas Practice data backup and recovery strategies. Host static websites using S3 and configure CloudFront. Complete Module 3. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Lab 20:\n- Successfully created AWS Transit Gateway to allow for connection between VPCs via a common hub - Fixed template file, changed EC2 instance type to t3.micro because the account still using free tier 15/09/2025 15/09/2025 3 - Verified the cost and budget plans worked as intended, notified over email.\nGet started on Module 3 theory Learnt about EBS, Instance store feature and check User and Meta Data - Learnt about Amazon Lightsail Learnt about Elastic File System(EFS) and FSx - Learnt about MGN - Complete Module 3\u0026rsquo;s labs (Part 1) - Lab 13: Successuly created Backup Plan and Vaults for data in S3 Bucket + Successfully setted up notification for Backup events + Successfully restored backup\n- Complete Module 3\u0026rsquo;s labs (Part 2) - Lab 24: + Created storage gateway + Succesfully completed file sharing - Complete Module 3\u0026rsquo;s labs (Part 3) - Lab 57: + Successfully hosted static website using S3 Bucket + Successfully configured access modifiers + Accelerate Static Websites with Cloudfront configuration did not work, skipping this step + Successfully created bucket versions + Moved objects between buckets + Replicated bucket across regions. 16/09/2025 20/09/2025 Lab 13 Lab 24 Lab 57 Week 2 Achievements: Successfully fixed an outdated CloudFormation template during the Transit Gateway lab. Successfully created AWS Transit Gateway to allow for connection between VPCs via a common hub. Learnt the hard way regarding resource cleanup and verified cost/budget alerts (charged $12 credits). Gained hands-on experience with S3 Buckets (static website hosting, versioning, replication). Successfully created Backup Plans and Vaults using AWS Backup, including restoration tests. Configured Storage Gateway and successfully completed file sharing tasks. Completed Module 3 theory and labs. "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"GIỚI THIỆU AWS TRUST CENTER Tác giả: Chris Betz Ngày: 14 tháng 2 năm 2025 Danh mục: Announcements, Featured, Security, Identity \u0026amp; Compliance\nTại Amazon Web Services (AWS), việc giành được niềm tin không chỉ là một mục tiêu – đó là một trong những Nguyên tắc Lãnh đạo cốt lõi (Leadership Principles) định hướng cho mọi quyết định mà chúng tôi đưa ra. Là Giám đốc An ninh Thông tin (CISO) của AWS, tôi đã chứng kiến tận mắt cam kết này định hình nên văn hóa, dịch vụ, và cách chúng tôi tương tác hàng ngày với khách hàng.\nKhách hàng chọn AWS thay vì các nhà cung cấp khác bởi vì họ tin tưởng rằng chúng tôi sẽ cung cấp hạ tầng và dịch vụ an toàn nhất, đồng thời minh bạch hoàn toàn về cách dữ liệu được bảo vệ. Để giúp việc tìm kiếm thông tin đó trở nên dễ dàng hơn, chúng tôi ra mắt AWS Trust Center, một nguồn tài nguyên trực tuyến mới chia sẻ cách chúng tôi tiếp cận vấn đề bảo mật tài sản của bạn trên nền tảng đám mây.\nAWS Trust Center là cửa sổ thể hiện các quy trình bảo mật, chương trình tuân thủ và biện pháp bảo vệ dữ liệu của chúng tôi - chứng minh cách chúng tôi làm việc mỗi ngày để xứng đáng với niềm tin của bạn.\nXây dựng trên nền tảng của sự tin cậy Bảo mật luôn là ưu tiên hàng đầu của chúng tôi từ ngày đầu tiên. Khi ra mắt AWS vào năm 2006, chúng tôi đã thiết kế hạ tầng trở thành môi trường điện toán đám mây an toàn nhất có thể. Chúng tôi biết rằng mình không thể chỉ cung cấp mức độ bảo mật tương đương với hạ tầng tại chỗ (on-premises); để giành được niềm tin của khách hàng, chúng tôi phải vượt qua và làm thay đổi các tiêu chuẩn bảo mật nghiêm ngặt nhất của các tổ chức hàng đầu thế giới.\nChúng tôi liên tục củng cố bảo mật trong từng quyết định hàng ngày. Với Trust Center, chúng tôi giúp bạn hiểu rõ hơn về cách chúng tôi bảo vệ khối lượng công việc, dữ liệu, và hỗ trợ bạn đạt được các mục tiêu tuân thủ. Trust Center phản ánh niềm tin của chúng tôi rằng việc cung cấp thông tin dễ tiếp cận hơn sẽ giúp xây dựng và duy trì niềm tin. Dù bạn đang tìm hiểu về kiểm soát trung tâm dữ liệu, kiểm tra chứng nhận tuân thủ, hay xem lại mô hình trách nhiệm chia sẻ (shared responsibility model), bạn đều có thể tìm thấy tất cả thông tin bảo mật và tuân thủ cần thiết tại một địa điểm duy nhất.\nMột nguồn thông tin duy nhất cho bảo mật và tuân thủ Trong Trust Center, bạn sẽ tìm thấy thông tin về cách chúng tôi tiếp cận bảo mật ở mọi cấp độ từ trung tâm dữ liệu vật lý, hạ tầng đám mây, cho đến danh mục dịch vụ AWS. Chúng tôi cung cấp tài liệu về các dịch vụ và công cụ bảo mật, giúp bạn hiểu được cách chúng tôi bảo mật đám mây và cách chúng tôi hỗ trợ bạn bảo mật khối lượng công việc của mình trong đó.\nBạn cũng sẽ tìm thấy thông tin về các chương trình tuân thủ, bao gồm chứng nhận và báo cáo đánh giá (attestations) mà chúng tôi duy trì trên toàn cầu. Điều này đặc biệt hữu ích cho các nhóm làm việc trong ngành được quản lý chặt chẽ, những người cần chứng minh tính tuân thủ với kiểm toán viên và cơ quan quản lý.\nTrust Center còn nhấn mạnh thông tin về thực hành bảo vệ và quyền riêng tư dữ liệu. Khách hàng có thể tìm hiểu cách chúng tôi bảo vệ dữ liệu và quản lý mã hóa. Bên cạnh đó, chúng tôi hiểu rằng khách hàng quan tâm đến việc ai có thể truy cập dữ liệu của họ và trong hoàn cảnh nào. Chúng tôi đã tổng hợp thông tin chi tiết về cơ chế kiểm soát quyền truy cập của nhân viên vận hành (operator access controls), được thiết kế dựa trên nguyên tắc đặc quyền tối thiểu (least privilege).\nBạn sẽ biết thêm về:\nThiết kế không truy cập (zero-access) đối với các dịch vụ cốt lõi như AWS Key Management Service (AWS KMS) và Amazon Elastic Compute Cloud (Amazon EC2). Forward Access Sessions (FAS) - phương thức mã hóa để thực thi ủy quyền của khách hàng. Hệ thống giám sát toàn cầu của chúng tôi. Trust Center cũng cung cấp một vị trí trung tâm để bạn tìm thấy thông tin về trạng thái dịch vụ và các sự kiện bảo mật, giúp bạn duy trì hiệu quả vận hành cao. Bạn có thể cập nhật các bản tin bảo mật, kiểm tra tình trạng dịch vụ theo thời gian thực, và nếu cần báo cáo mối quan ngại bảo mật hoặc tiến hành đánh giá bảo mật, các quy trình đó cũng được trình bày rõ ràng và dễ tìm hơn bao giờ hết. Các tài nguyên được sắp xếp logic và dễ truy cập, với liên kết trực tiếp đến các thỏa thuận, tài liệu và nguồn thông tin giúp bạn đưa ra quyết định chính xác về tư thế bảo mật trên đám mây của mình.\nTrao quyền cho khách hàng thúc đẩy đổi mới an toàn Điều khiến tôi phấn khích nhất về Trust Center chính là khả năng loại bỏ rào cản cho khách hàng. Với thông tin bảo mật chi tiết, liên kết dễ dàng đến tài liệu tuân thủ, và dữ liệu vận hành sẵn có, bạn có thể hành động nhanh hơn và đổi mới với sự tự tin.\nKhi chúng tôi tiếp tục đổi mới và mở rộng các dịch vụ AWS, chúng tôi cam kết nâng cấp Trust Center với những thông tin bảo mật mới nhất. Đây là một tài nguyên sống (living resource) sẽ phát triển song song với đám mây và dịch vụ của chúng tôi.\nDuy trì niềm tin của bạn không chỉ là những gì chúng tôi đã xây dựng hôm nay, mà còn là việc chứng minh – qua cam kết và hành động – rằng chúng tôi xứng đáng trở thành đối tác bảo mật đáng tin cậy của bạn. Đó chính là cam kết của chúng tôi dành cho bạn, và cũng là điều mà AWS Trust Center đại diện.\nKhám phá AWS Trust Center Chúng tôi mời bạn khám phá AWS Trust Center ngay hôm nay, và mong muốn tiếp tục giành được niềm tin của bạn mỗi ngày.\nNếu bạn có phản hồi về bài viết này, vui lòng để lại bình luận trong phần Comments bên dưới. Nếu bạn có câu hỏi liên quan, hãy liên hệ với AWS Support. Về tác giả Chris Betz là Giám đốc An ninh Thông tin (CISO) tại AWS. Ông phụ trách giám sát các nhóm bảo mật và lãnh đạo việc phát triển cũng như triển khai các chính sách bảo mật với mục tiêu quản lý rủi ro và điều chỉnh tư thế bảo mật của công ty phù hợp với các mục tiêu kinh doanh. Chris gia nhập Amazon vào tháng 8 năm 2023 sau khi đảm nhiệm các vị trí CISO và vai trò lãnh đạo bảo mật tại nhiều công ty hàng đầu. Ông hiện sinh sống cùng gia đình tại Bắc Virginia.\n"},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.10-cleanup/5.10.2-cdk-cleanup/","title":"CDK Cleanup","tags":[],"description":"","content":"Clean up (CDK) This guide ensures you correctly decommission all resources provisioned by the AWS CDK stack and clean up manually created data to avoid ongoing charges.\nPhase 1: Manual Data Cleanup (Before CDK Destroy) The CDK automatically deletes most resources failed in deleting content from S3 buckets. You must empty the contents of these buckets before running the cdk destroy command.\nResource Name Purpose Action Required incident-response-log-list-bucket Primary Log Source Empty Contents processed-cloudwatch-logs ETL Destination Empty Contents processed-guardduty-findings ETL Destination Empty Contents processed-cloudtrail-logs ETL Destination Empty Contents athena-query-results Athena Query Results Empty Contents aws-incident-response-automation-dashboard React Dashboard S3 Bucket Empty Contents Instructions for Emptying Buckets:\nOpen the Amazon S3 Console in your browser. For each of the buckets listed above (look for the names based on your AWS Account ID and Region): Click on the bucket name. Navigate to the \u0026ldquo;Objects\u0026rdquo; tab. Click the \u0026ldquo;Empty\u0026rdquo; button. Follow the prompts to confirm the permanent deletion of all objects. Phase 2: CDK Stack Destruction This step uses the CDK CLI to destroy all resources provisioned by the CloudFormation stack.\nEnsure Virtual Environment is Active\nIf you deactivated your Python environment, re-activate it (e.g., source .venv/bin/activate). Navigate to the Project Root\nEnsure you are in the main directory where the cdk.json file is located. Execute the Destroy Command\nRun the command to destroy all deployed stacks. When prompted, type y to approve the deletion. $ cdk destroy --all Phase 3: Post-Destruction Cleanup This step addresses remaining manual cleanup of lingering resources.\nDelete Remaining S3 Buckets\nThe cdk destroy command should remove the empty S3 buckets. If any remain (due to final checks or service protections), delete them now via the S3 Console. Disable Amazon GuardDuty\nGo to GuardDuty Console → Settings → General. Verify the service is disabled to ensure billing stops. Remove Cognito User and Pool\nGo to Cognito Console → User pools. Delete the test user you created. Delete the User Pool created for the dashboard. Remove SES Identity\nGo to Amazon SES Console → Verified Identities. Delete the sender email identity (sender_email) you verified. Deactivate Virtual Environment\nDeactivate the Python virtual environment: $ deactivate "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.5-processing-setup/5.5.2-create-aws-glue-database-and-tables/","title":"Create AWS Glue Database and Tables","tags":[],"description":"","content":"Create AWS Glue Database and Tables Create Database Open Glue Console → Databases → Add database\nDatabase name: security_logs\nCreate database\nCreate Tables (Using Athena DDL) Open Athena Console\nSet query result location: s3://athena-query-results-ACCOUNT_ID-REGION/\nSelect database: security_logs\nCreate processed_cloudtrail Table Run this DDL in Athena (replace ACCOUNT_ID and REGION):\nCREATE EXTERNAL TABLE IF NOT EXISTS security_logs.processed_cloudtrail ( `eventtime` string, `eventname` string, `eventsource` string, `awsregion` string, `sourceipaddress` string, `useragent` string, `useridentity` struct\u0026lt; type:string, invokedby:string, principalid:string, arn:string, accountid:string, accesskeyid:string, username:string, sessioncontext:struct\u0026lt; attributes:map\u0026lt;string,string\u0026gt;, sessionissuer:struct\u0026lt; type:string, principalid:string, arn:string, accountid:string, username:string \u0026gt; \u0026gt;, inscopeof:struct\u0026lt; issuertype:string, credentialsissuedto:string \u0026gt; \u0026gt;, `requestparameters` string, `responseelements` string, `resources` array\u0026lt;struct\u0026lt;arn:string,type:string\u0026gt;\u0026gt;, `recipientaccountid` string, `serviceeventdetails` string, `errorcode` string, `errormessage` string, `hour` string, `usertype` string, `username` string, `isconsolelogin` boolean, `isfailedlogin` boolean, `isrootuser` boolean, `isassumedrole` boolean, `ishighriskevent` boolean, `isprivilegedaction` boolean, `isdataaccess` boolean, `target_bucket` string, `target_key` string, `target_username` string, `target_rolename` string, `target_policyname` string, `new_access_key` string, `new_instance_id` string, `target_group_id` string, `identity_principalid` string ) PARTITIONED BY ( `date` string ) ROW FORMAT SERDE \u0026#39;org.openx.data.jsonserde.JsonSerDe\u0026#39; WITH SERDEPROPERTIES ( \u0026#39;serialization.format\u0026#39; = \u0026#39;1\u0026#39; ) LOCATION \u0026#39;s3://processed-cloudtrail-logs-ACCOUNT_ID-REGION/processed-cloudtrail/\u0026#39; TBLPROPERTIES ( \u0026#39;projection.enabled\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;projection.date.type\u0026#39; = \u0026#39;date\u0026#39;, \u0026#39;projection.date.format\u0026#39; = \u0026#39;yyyy-MM-dd\u0026#39;, \u0026#39;projection.date.range\u0026#39; = \u0026#39;2025-01-01,NOW\u0026#39;, \u0026#39;projection.date.interval\u0026#39; = \u0026#39;1\u0026#39;, \u0026#39;projection.date.interval.unit\u0026#39; = \u0026#39;DAYS\u0026#39;, \u0026#39;storage.location.template\u0026#39; = \u0026#39;s3://processed-cloudtrail-logs-ACCOUNT_ID-REGION/processed-cloudtrail/date=${date}/\u0026#39;, \u0026#39;classification\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;compressionType\u0026#39; = \u0026#39;gzip\u0026#39; ); Create processed_guardduty Table Run this DDL in Athena:\nCREATE EXTERNAL TABLE IF NOT EXISTS security_logs.processed_guardduty ( `finding_id` string, `finding_type` string, `title` string, `severity` double, `account_id` string, `region` string, `created_at` string, `event_last_seen` string, `remote_ip` string, `remote_port` int, `connection_direction` string, `protocol` string, `dns_domain` string, `dns_protocol` string, `scanned_ip` string, `scanned_port` int, `aws_api_service` string, `aws_api_name` string, `aws_api_caller_type` string, `aws_api_error` string, `aws_api_remote_ip` string, `target_resource_arn` string, `instance_id` string, `instance_type` string, `image_id` string, `instance_tags` string, `resource_region` string, `access_key_id` string, `principal_id` string, `user_name` string, `s3_bucket_name` string, `service_raw` string, `resource_raw` string, `metadata_raw` string ) PARTITIONED BY ( `date` string ) ROW FORMAT SERDE \u0026#39;org.openx.data.jsonserde.JsonSerDe\u0026#39; WITH SERDEPROPERTIES ( \u0026#39;serialization.format\u0026#39; = \u0026#39;1\u0026#39; ) LOCATION \u0026#39;s3://processed-guardduty-findings-ACCOUNT_ID-REGION/processed-guardduty/\u0026#39; TBLPROPERTIES ( \u0026#39;classification\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;compressionType\u0026#39; = \u0026#39;gzip\u0026#39;, \u0026#39;projection.enabled\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;projection.date.type\u0026#39; = \u0026#39;date\u0026#39;, \u0026#39;projection.date.range\u0026#39; = \u0026#39;2025-01-01,NOW\u0026#39;, \u0026#39;projection.date.format\u0026#39; = \u0026#39;yyyy-MM-dd\u0026#39;, \u0026#39;projection.date.interval\u0026#39; = \u0026#39;1\u0026#39;, \u0026#39;projection.date.interval.unit\u0026#39; = \u0026#39;DAYS\u0026#39;, \u0026#39;storage.location.template\u0026#39; = \u0026#39;s3://processed-guardduty-findings-ACCOUNT_ID-REGION/processed-guardduty/date=${date}/\u0026#39; ); Create vpc_logs Table Run this DDL in Athena:\nCREATE EXTERNAL TABLE IF NOT EXISTS security_logs.vpc_logs ( `version` string, `account_id` string, `region` string, `vpc_id` string, `query_timestamp` string, `query_name` string, `query_type` string, `query_class` string, `rcode` string, `answers` string, `srcaddr` string, `srcport` int, `transport` string, `srcids_instance` string, `timestamp` string ) PARTITIONED BY ( `date` string ) ROW FORMAT SERDE \u0026#39;org.openx.data.jsonserde.JsonSerDe\u0026#39; WITH SERDEPROPERTIES ( \u0026#39;serialization.format\u0026#39; = \u0026#39;1\u0026#39;, \u0026#39;ignore.malformed.json\u0026#39; = \u0026#39;true\u0026#39; ) LOCATION \u0026#39;s3://processed-cloudwatch-logs-ACCOUNT_ID-REGION/vpc-logs/\u0026#39; TBLPROPERTIES ( \u0026#39;projection.enabled\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;projection.date.type\u0026#39; = \u0026#39;date\u0026#39;, \u0026#39;projection.date.format\u0026#39; = \u0026#39;yyyy-MM-dd\u0026#39;, \u0026#39;projection.date.range\u0026#39; = \u0026#39;2025-01-01,NOW\u0026#39;, \u0026#39;projection.date.interval\u0026#39; = \u0026#39;1\u0026#39;, \u0026#39;projection.date.interval.unit\u0026#39; = \u0026#39;DAYS\u0026#39;, \u0026#39;storage.location.template\u0026#39; = \u0026#39;s3://processed-cloudwatch-logs-ACCOUNT_ID-REGION/vpc-logs/date=${date}/\u0026#39;, \u0026#39;classification\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;compressionType\u0026#39; = \u0026#39;gzip\u0026#39; ); Create eni_flow_logs Table Run this DDL in Athena:\nCREATE EXTERNAL TABLE IF NOT EXISTS security_logs.eni_flow_logs ( `version` int, `account_id` string, `interface_id` string, `srcaddr` string, `dstaddr` string, `srcport` int, `dstport` int, `protocol` int, `packets` bigint, `bytes` bigint, `start_time` bigint, `end_time` bigint, `action` string, `log_status` string, `timestamp_str` string ) PARTITIONED BY ( `date` string ) ROW FORMAT SERDE \u0026#39;org.openx.data.jsonserde.JsonSerDe\u0026#39; WITH SERDEPROPERTIES ( \u0026#39;serialization.format\u0026#39; = \u0026#39;1\u0026#39; ) LOCATION \u0026#39;s3://processed-cloudwatch-logs-ACCOUNT_ID-REGION/eni-flow-logs/\u0026#39; TBLPROPERTIES ( \u0026#39;projection.enabled\u0026#39; = \u0026#39;true\u0026#39;, \u0026#39;projection.date.type\u0026#39; = \u0026#39;date\u0026#39;, \u0026#39;projection.date.format\u0026#39; = \u0026#39;yyyy-MM-dd\u0026#39;, \u0026#39;projection.date.range\u0026#39; = \u0026#39;2025-01-01,NOW\u0026#39;, \u0026#39;projection.date.interval\u0026#39; = \u0026#39;1\u0026#39;, \u0026#39;projection.date.interval.unit\u0026#39; = \u0026#39;DAYS\u0026#39;, \u0026#39;storage.location.template\u0026#39; = \u0026#39;s3://processed-cloudwatch-logs-ACCOUNT_ID-REGION/eni-flow-logs/date=${date}/\u0026#39;, \u0026#39;classification\u0026#39; = \u0026#39;json\u0026#39;, \u0026#39;compressionType\u0026#39; = \u0026#39;gzip\u0026#39; ); "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.7-dashboard-setup/5.7.2-setup-lambda/","title":"Create IAM Roles and Policies","tags":[],"description":"","content":"In this section, you will create IAM role and Policy for Lambda. After that you will create Lambda Function to execute query\nContent Create Lambda Execution Roles and Policy Create Lambda Function "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.3-foundation-setup/5.3.3-create-iam-roles-and-policies/5.3.3.2-create-service-roles/","title":"Create Service Roles","tags":[],"description":"","content":"Create Firehose Roles Create CloudTrailFirehoseRole Open IAM Console → Roles → Create role\nSelect trusted entity:\nTrusted entity type: AWS service Use case: Select \u0026ldquo;Kinesis\u0026rdquo; → \u0026ldquo;Kinesis Firehose\u0026rdquo; Click \u0026ldquo;Next\u0026rdquo; Add permissions:\nSkip adding managed policies (we\u0026rsquo;ll add inline policy) Click \u0026ldquo;Next\u0026rdquo; Name and create:\nRole name: CloudTrailFirehoseRole Description: Allows Firehose to write CloudTrail logs to S3 Click \u0026ldquo;Create role\u0026rdquo; Add inline policy:\nPolicy name: FirehosePolicy Policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:PutObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::processed-cloudtrail-logs-ACCOUNT_ID-REGION\u0026#34;, \u0026#34;arn:aws:s3:::processed-cloudtrail-logs-ACCOUNT_ID-REGION/*\u0026#34; ] } ] } Create CloudWatchFirehoseRole Role name: CloudWatchFirehoseRole Description: Allows Firehose to write CloudWatch logs to S3 Trusted entity: Kinesis Firehose Inline policy name: FirehosePolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:PutObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::processed-cloudwatch-logs-ACCOUNT_ID-REGION\u0026#34;, \u0026#34;arn:aws:s3:::processed-cloudwatch-logs-ACCOUNT_ID-REGION/*\u0026#34; ] } ] } Create Step Functions Role Create StepFunctionsRole Create role:\nTrusted entity: Step Functions Role name: StepFunctionsRole Description: Execution role for Incident Response Step Functions Add TWO inline policies:\nPolicy 1: LambdaInvokePolicy\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:lambda:REGION:ACCOUNT_ID:function:ir-isolate-ec2-lambda\u0026#34;, \u0026#34;arn:aws:lambda:REGION:ACCOUNT_ID:function:ir-parse-findings-lambda\u0026#34;, \u0026#34;arn:aws:lambda:REGION:ACCOUNT_ID:function:ir-quarantine-iam-lambda\u0026#34; ] } ] } Policy 2: EC2AutoScalingPolicy\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;autoscaling:DescribeAutoScalingInstances\u0026#34;, \u0026#34;autoscaling:DetachInstances\u0026#34;, \u0026#34;autoscaling:UpdateAutoScalingGroup\u0026#34;, \u0026#34;ec2:CreateSnapshot\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:DescribeVolumes\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Create EventBridge Role Create IncidentResponseStepFunctionsEventRole Role name: IncidentResponseStepFunctionsEventRole Description: Allows EventBridge to trigger Step Functions Trusted entity: EventBridge Inline policy name: StartStepFunctionsPolicy Inline policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;states:StartExecution\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:REGION:ACCOUNT_ID:stateMachine:IncidentResponseStepFunctions\u0026#34; } ] } Create VPC Flow Logs Role Create FlowLogsIAMRole Create role:\nTrusted entity: EC2 (will edit trust policy) Role name: FlowLogsIAMRole Edit trust relationship to:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;vpc-flow-logs.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } Add inline policy: Policy name: FlowLogsPolicy Policy JSON: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:DescribeLogStreams\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Create Glue Role Create GlueCloudWatchRole Role name: GlueCloudWatchRole Description: Allows Glue to access S3 and CloudWatch Logs Trusted entity: Glue Managed policies (attach 3): AWSGlueServiceRole CloudWatchLogsReadOnlyAccess AmazonS3FullAccess No inline policies needed "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.11-appendices/5.11.2-guardduty-etl/","title":"GuardDuty ETL Code","tags":[],"description":"","content":" import json import boto3 import gzip import os from datetime import datetime from urllib.parse import unquote_plus s3_client = boto3.client(\u0026#39;s3\u0026#39;) DATABASE_NAME = os.environ.get(\u0026#34;DATABASE_NAME\u0026#34;, \u0026#34;security_logs\u0026#34;) TABLE_NAME_GUARDDUTY = os.environ.get(\u0026#34;TABLE_NAME_GUARDDUTY\u0026#34;, \u0026#34;processed_guardduty\u0026#34;) S3_LOCATION_GUARDDUTY = os.environ.get(\u0026#34;S3_LOCATION_GUARDDUTY\u0026#34;, \u0026#34;s3://vel-processed-guardduty/processed-guardduty/\u0026#34;) DESTINATION_BUCKET = os.environ.get(\u0026#34;DESTINATION_BUCKET\u0026#34;, \u0026#34;vel-processed-guardduty\u0026#34;) def promote_network_details(finding_service): if not finding_service: return {} action = finding_service.get(\u0026#39;action\u0026#39;, {}) net_conn_action = action.get(\u0026#39;networkConnectionAction\u0026#39;, {}) if net_conn_action: remote_ip = net_conn_action.get(\u0026#39;remoteIpDetails\u0026#39;, {}).get(\u0026#39;ipAddressV4\u0026#39;) or \\ net_conn_action.get(\u0026#39;remoteIpDetails\u0026#39;, {}).get(\u0026#39;ipAddressV6\u0026#39;) return { \u0026#39;remote_ip\u0026#39;: remote_ip, \u0026#39;remote_port\u0026#39;: net_conn_action.get(\u0026#39;remotePortDetails\u0026#39;, {}).get(\u0026#39;port\u0026#39;), \u0026#39;connection_direction\u0026#39;: net_conn_action.get(\u0026#39;connectionDirection\u0026#39;), \u0026#39;protocol\u0026#39;: net_conn_action.get(\u0026#39;protocol\u0026#39;), } dns_action = action.get(\u0026#39;dnsRequestAction\u0026#39;, {}) if dns_action: return {\u0026#39;dns_domain\u0026#39;: dns_action.get(\u0026#39;domain\u0026#39;), \u0026#39;dns_protocol\u0026#39;: dns_action.get(\u0026#39;protocol\u0026#39;)} port_probe_action = action.get(\u0026#39;portProbeAction\u0026#39;, {}) if port_probe_action and port_probe_action.get(\u0026#39;portProbeDetails\u0026#39;): detail = port_probe_action[\u0026#39;portProbeDetails\u0026#39;][0] return { \u0026#39;scanned_ip\u0026#39;: detail.get(\u0026#39;remoteIpDetails\u0026#39;, {}).get(\u0026#39;ipAddressV4\u0026#39;), \u0026#39;scanned_port\u0026#39;: detail.get(\u0026#39;localPortDetails\u0026#39;, {}).get(\u0026#39;port\u0026#39;), } return {} def promote_api_details(finding_service): if not finding_service: return {} action = finding_service.get(\u0026#39;action\u0026#39;, {}) aws_api_action = action.get(\u0026#39;awsApiCallAction\u0026#39;, {}) if aws_api_action: return { \u0026#39;aws_api_service\u0026#39;: aws_api_action.get(\u0026#39;serviceName\u0026#39;), \u0026#39;aws_api_name\u0026#39;: aws_api_action.get(\u0026#39;api\u0026#39;), \u0026#39;aws_api_caller_type\u0026#39;: aws_api_action.get(\u0026#39;callerType\u0026#39;), \u0026#39;aws_api_error\u0026#39;: aws_api_action.get(\u0026#39;errorCode\u0026#39;), \u0026#39;aws_api_remote_ip\u0026#39;: aws_api_action.get(\u0026#39;remoteIpDetails\u0026#39;, {}).get(\u0026#39;ipAddressV4\u0026#39;), } return {} def promote_resource_details(finding_resource): if not finding_resource: return {} instance_details = finding_resource.get(\u0026#39;instanceDetails\u0026#39;, {}) if instance_details: return { \u0026#39;target_resource_arn\u0026#39;: instance_details.get(\u0026#39;arn\u0026#39;), \u0026#39;instance_id\u0026#39;: instance_details.get(\u0026#39;instanceId\u0026#39;), \u0026#39;resource_region\u0026#39;: instance_details.get(\u0026#39;awsRegion\u0026#39;), \u0026#39;instance_type\u0026#39;: instance_details.get(\u0026#39;instanceType\u0026#39;), \u0026#39;image_id\u0026#39;: instance_details.get(\u0026#39;imageId\u0026#39;), \u0026#39;instance_tags\u0026#39;: instance_details.get(\u0026#39;tags\u0026#39;) } access_key_details = finding_resource.get(\u0026#39;accessKeyDetails\u0026#39;, {}) if access_key_details: return { \u0026#39;access_key_id\u0026#39;: access_key_details.get(\u0026#39;accessKeyId\u0026#39;), \u0026#39;principal_id\u0026#39;: access_key_details.get(\u0026#39;principalId\u0026#39;), \u0026#39;user_name\u0026#39;: access_key_details.get(\u0026#39;userName\u0026#39;), } s3_details = finding_resource.get(\u0026#39;s3BucketDetails\u0026#39;, []) if s3_details: return { \u0026#39;target_resource_arn\u0026#39;: s3_details[0].get(\u0026#39;arn\u0026#39;), \u0026#39;s3_bucket_name\u0026#39;: s3_details[0].get(\u0026#39;name\u0026#39;), } return {} def process_guardduty_log(bucket, key): response = s3_client.get_object(Bucket=bucket, Key=key) if key.endswith(\u0026#39;.gz\u0026#39;): content = gzip.decompress(response[\u0026#39;Body\u0026#39;].read()).decode(\u0026#39;utf-8\u0026#39;) else: content = response[\u0026#39;Body\u0026#39;].read().decode(\u0026#39;utf-8\u0026#39;) processed_findings = [] for line in content.splitlines(): if not line: continue try: finding = json.loads(line) except json.JSONDecodeError: print(f\u0026#34;Skipping malformed JSON line in {key}\u0026#34;); continue finding_type = finding.get(\u0026#39;type\u0026#39;, \u0026#39;UNKNOWN\u0026#39;) finding_service = finding.get(\u0026#39;service\u0026#39;, {}) network_fields = promote_network_details(finding_service) api_fields = promote_api_details(finding_service) resource_fields = promote_resource_details(finding.get(\u0026#39;resource\u0026#39;, {})) created_at_str = finding.get(\u0026#39;createdAt\u0026#39;) event_last_seen_str = finding_service.get(\u0026#39;eventLastSeen\u0026#39;) dt_obj = datetime.now() if event_last_seen_str: try: dt_obj = datetime.strptime(event_last_seen_str, \u0026#39;%Y-%m-%dT%H:%M:%S.%fZ\u0026#39;) except ValueError: try: dt_obj = datetime.strptime(event_last_seen_str, \u0026#39;%Y-%m-%dT%H:%M:%SZ\u0026#39;) except ValueError: pass elif created_at_str: try: dt_obj = datetime.strptime(created_at_str, \u0026#39;%Y-%m-%dT%H:%M:%S.%fZ\u0026#39;) except ValueError: try: dt_obj = datetime.strptime(created_at_str, \u0026#39;%Y-%m-%dT%H:%M:%SZ\u0026#39;) except ValueError: pass processed_record = { \u0026#39;finding_id\u0026#39;: finding.get(\u0026#39;id\u0026#39;), \u0026#39;finding_type\u0026#39;: finding_type, \u0026#39;title\u0026#39;: finding.get(\u0026#39;title\u0026#39;), \u0026#39;severity\u0026#39;: finding.get(\u0026#39;severity\u0026#39;), \u0026#39;account_id\u0026#39;: finding.get(\u0026#39;accountId\u0026#39;), \u0026#39;region\u0026#39;: finding.get(\u0026#39;region\u0026#39;), \u0026#39;created_at\u0026#39;: created_at_str, \u0026#39;event_last_seen\u0026#39;: event_last_seen_str, **network_fields, **api_fields, **resource_fields, \u0026#39;date\u0026#39;: dt_obj.strftime(\u0026#39;%Y-%m-%d\u0026#39;), \u0026#39;service_raw\u0026#39;: json.dumps(finding_service), \u0026#39;resource_raw\u0026#39;: json.dumps(finding.get(\u0026#39;resource\u0026#39;, {})), \u0026#39;metadata_raw\u0026#39;: json.dumps(finding.get(\u0026#39;metadata\u0026#39;, {})), } processed_findings.append(processed_record) return processed_findings def save_processed_data(processed_events, source_key): if not processed_events: return first_event = processed_events[0] date_str = first_event.get(\u0026#39;date\u0026#39;, datetime.now().strftime(\u0026#39;%Y-%m-%d\u0026#39;)) original_filename = source_key.split(\u0026#39;/\u0026#39;)[-1].replace(\u0026#39;.gz\u0026#39;, \u0026#39;\u0026#39;).replace(\u0026#39;.json\u0026#39;, \u0026#39;\u0026#39;) output_key = f\u0026#34;processed-guardduty/date={date_str}/{original_filename}_processed.jsonl.gz\u0026#34; json_lines = \u0026#34;\u0026#34; for event in processed_events: event_to_dump = event.copy() json_lines += json.dumps(event_to_dump) + \u0026#34;\\n\u0026#34; compressed_data = gzip.compress(json_lines.encode(\u0026#39;utf-8\u0026#39;)) s3_client.put_object( Bucket=DESTINATION_BUCKET, Key=output_key, Body=compressed_data, ContentType=\u0026#39;application/jsonl\u0026#39;, ContentEncoding=\u0026#39;gzip\u0026#39; ) print(f\u0026#34;Saved processed data to: s3://{DESTINATION_BUCKET}/{output_key}\u0026#34;) def lambda_handler(event, context): for record in event[\u0026#39;Records\u0026#39;]: bucket = record[\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] key = unquote_plus(record[\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;]) print(f\u0026#34;Processing GuardDuty finding file: s3://{bucket}/{key}\u0026#34;) try: processed_findings = process_guardduty_log(bucket, key) save_processed_data(processed_findings, key) print(f\u0026#34;Successfully processed {len(processed_findings)} findings from {key}\u0026#34;) except Exception as e: print(f\u0026#34;Error processing {key}: {str(e)}\u0026#34;) raise e return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;GuardDuty findings processed successfully\u0026#39;) } "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.7-dashboard-setup/5.7.2-setup-lambda/5.7.2.2-create-lambda-function/","title":"Lambda setup","tags":[],"description":"","content":"In this guide, you will setup a Lambda using Python to execute query using Athena service.\nCreate Lambda Function Open the Lambda Console\nNavigate to https://console.aws.amazon.com/lambda/ Or: AWS Management Console → Services → Lambda Create Function:\nClick the Create Function In the create setting use the following setting: Choose Author from scratch Name: dashboard-query Runtime: Python 3.12 Architecture: x86_64 Change default execution role: Use an existing role Choose dashboard-query-role Click Create Add code:\nIn the code editor copy and paste the codes below then click Deply: import boto3 import time import os import json athena = boto3.client(\u0026#39;athena\u0026#39;) RESOURCE_MAP = { \u0026#39;/logs/cloudtrail\u0026#39;: { \u0026#39;db\u0026#39;: \u0026#39;security_logs\u0026#39;, \u0026#39;table\u0026#39;: \u0026#39;processed_cloudtrail\u0026#39; }, \u0026#39;/logs/guardduty\u0026#39;: { \u0026#39;db\u0026#39;: \u0026#39;security_logs\u0026#39;, \u0026#39;table\u0026#39;: \u0026#39;processed_guardduty\u0026#39; }, \u0026#39;/logs/vpc\u0026#39;: { \u0026#39;db\u0026#39;: \u0026#39;security_logs\u0026#39;, \u0026#39;table\u0026#39;: \u0026#39;vpc_logs\u0026#39; }, \u0026#39;/logs/eni_logs\u0026#39;:{ \u0026#39;db\u0026#39;: \u0026#39;security_logs\u0026#39;, \u0026#39;table\u0026#39;: \u0026#39;eni_flow_logs\u0026#39; } } OUTPUT_BUCKET_NAME = os.environ.get(\u0026#34;ATHENA_OUTPUT_BUCKET\u0026#34;) REGION = os.environ.get(\u0026#34;REGION\u0026#34;) OUTPUT_BUCKET = f\u0026#39;s3://{OUTPUT_BUCKET_NAME}/\u0026#39; def lambda_handler(event, context): print(\u0026#34;Received event:\u0026#34;, json.dumps(event)) resource_path = event.get(\u0026#39;resource\u0026#39;) config = RESOURCE_MAP.get(resource_path) if not config: return api_response(400, {\u0026#39;error\u0026#39;: f\u0026#39;Unknown resource path: {resource_path}\u0026#39;}) database_name = config[\u0026#39;db\u0026#39;] table_name = config[\u0026#39;table\u0026#39;] query_params = event.get(\u0026#39;queryStringParameters\u0026#39;, {}) or {} if config[\u0026#39;table\u0026#39;] == \u0026#39;processed_cloudtrail\u0026#39;: query_string = f\u0026#34;\u0026#34;\u0026#34;SELECT * FROM {table_name} where \u0026#34;date\u0026#34; \u0026gt;= cast((current_date - interval \u0026#39;3\u0026#39; day) as varchar) order by eventtime desc\u0026#34;\u0026#34;\u0026#34; elif config[\u0026#39;table\u0026#39;] == \u0026#39;processed_guardduty\u0026#39;: query_string = f\u0026#34;\u0026#34;\u0026#34;SELECT * FROM {table_name} where \u0026#34;date\u0026#34; \u0026gt;= cast((current_date - interval \u0026#39;3\u0026#39; day) as varchar) order by date desc\u0026#34;\u0026#34;\u0026#34; elif config[\u0026#39;table\u0026#39;] == \u0026#39;vpc_logs\u0026#39;: query_string = f\u0026#34;\u0026#34;\u0026#34;SELECT * FROM {table_name} where \u0026#34;date\u0026#34; \u0026gt;= cast((current_date - interval \u0026#39;3\u0026#39; day) as varchar) order by timestamp desc\u0026#34;\u0026#34;\u0026#34; elif config[\u0026#39;table\u0026#39;] == \u0026#39;eni_flow_logs\u0026#39;: query_string = f\u0026#34;\u0026#34;\u0026#34;SELECT * FROM {table_name} where \u0026#34;date\u0026#34; \u0026gt;= cast((current_date - interval \u0026#39;3\u0026#39; day) as varchar) order by timestamp_str desc\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;Querying DB: {database_name}, Table: {table_name}, Output: {OUTPUT_BUCKET}\u0026#34;) try: response = athena.start_query_execution( QueryString=query_string, QueryExecutionContext={\u0026#39;Database\u0026#39;: database_name}, ResultConfiguration={\u0026#39;OutputLocation\u0026#39;: OUTPUT_BUCKET} ) query_execution_id = response[\u0026#39;QueryExecutionId\u0026#39;] status = \u0026#39;RUNNING\u0026#39; while status in [\u0026#39;RUNNING\u0026#39;, \u0026#39;QUEUED\u0026#39;]: response = athena.get_query_execution(QueryExecutionId=query_execution_id) status = response[\u0026#39;QueryExecution\u0026#39;][\u0026#39;Status\u0026#39;][\u0026#39;State\u0026#39;] if status in [\u0026#39;FAILED\u0026#39;, \u0026#39;CANCELLED\u0026#39;]: reason = response[\u0026#39;QueryExecution\u0026#39;][\u0026#39;Status\u0026#39;].get(\u0026#39;StateChangeReason\u0026#39;, \u0026#39;Unknown\u0026#39;) return api_response(500, {\u0026#39;error\u0026#39;: f\u0026#39;Query Failed: {reason}\u0026#39;}) time.sleep(1) results = athena.get_query_results(QueryExecutionId=query_execution_id) return api_response(200, results) except Exception as e: print(f\u0026#34;Error: {str(e)}\u0026#34;) return api_response(500, {\u0026#39;error\u0026#39;: str(e)}) def api_response(code, body): return { \u0026#34;statusCode\u0026#34;: code, \u0026#34;headers\u0026#34;: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Access-Control-Allow-Origin\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Access-Control-Allow-Methods\u0026#34;: \u0026#34;GET, OPTIONS\u0026#34; }, \u0026#34;body\u0026#34;: json.dumps(body) } "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.3-foundation-setup/5.3.2-set-up-s3-buckets-policies/","title":"Set up S3 buckets policies","tags":[],"description":"","content":"In this section, you will configure the bucket policy for the primary log bucket to allow CloudTrail, GuardDuty, and CloudWatch Logs to write logs.\nConfigure Bucket Policy Navigate to the primary log bucket: In S3 Console, click on incident-response-log-list-bucket-ACCOUNT_ID-REGION Open the Permissions tab:\nClick on the \u0026ldquo;Permissions\u0026rdquo; tab Scroll to Bucket policy:\nScroll down to the \u0026ldquo;Bucket policy\u0026rdquo; section Click \u0026ldquo;Edit\u0026rdquo; Paste the bucket policy: Copy the following JSON policy Important: Replace ACCOUNT_ID and REGION with your actual values in the policy { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowGuardDutyPutObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;guardduty.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:PutObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceAccount\u0026#34;: \u0026#34;ACCOUNT_ID\u0026#34; }, \u0026#34;ArnLike\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:guardduty:REGION:ACCOUNT_ID:detector/*\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowGuardDutyGetBucketLocation\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;guardduty.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceAccount\u0026#34;: \u0026#34;ACCOUNT_ID\u0026#34; }, \u0026#34;ArnLike\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:guardduty:REGION:ACCOUNT_ID:detector/*\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCloudWatchLogsGetBucketAcl\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;logs.REGION.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceAccount\u0026#34;: \u0026#34;ACCOUNT_ID\u0026#34; }, \u0026#34;ArnLike\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:logs:REGION:ACCOUNT_ID:log-group:*\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCloudWatchLogsPutObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;logs.REGION.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:PutObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceAccount\u0026#34;: \u0026#34;ACCOUNT_ID\u0026#34; }, \u0026#34;ArnLike\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:logs:REGION:ACCOUNT_ID:log-group:*\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCloudTrailAclCheck\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudtrail.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceAccount\u0026#34;: \u0026#34;ACCOUNT_ID\u0026#34; }, \u0026#34;ArnLike\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:cloudtrail:REGION:ACCOUNT_ID:trail/*\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCloudTrailWrite\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudtrail.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:PutObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/AWSLogs/ACCOUNT_ID/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;s3:x-amz-acl\u0026#34;: \u0026#34;bucket-owner-full-control\u0026#34;, \u0026#34;aws:SourceAccount\u0026#34;: \u0026#34;ACCOUNT_ID\u0026#34; }, \u0026#34;ArnLike\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:cloudtrail:REGION:ACCOUNT_ID:trail/*\u0026#34; } } } ] } Click \u0026ldquo;Save changes\u0026rdquo;\nVerify policy is saved: You should see the policy displayed in the Bucket policy section\n"},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Summary Report: “AWS Cloud Mastery Series #2 - DevOps on AWS” Event Objectives Introduce AWS DevOps Services – CI/CD Pipeline Introduce Infrastructure as Code (IaC) and related tools Introduce Container Services on AWS Ensure Monitoring and Observability capability using AWS Services Speakers Truong Quang Tinh – AWS Community Builder, Platform Engineer - TymeX Bao Huynh – AWS Community Builder Nguyen Khanh Phuc Thinh – AWS Community Builder Tran Dai Vi – AWS Community Builder Huynh Hoang Long – AWS Community Builder Pham Hoang Quy – AWS Community Builder Nghiem Le – AWS Community Builder Dinh Le Hoang Anh - Cloud Engineer Trainee, First Cloud AI Journey Key Highlights DevOps Mindset - Culture: Collaboration, Automation, Continuous Learning and Measurement - DevOps Roles: DevOps Engineer, Cloud Engineer, Platform Engineer, Site Reliability Engineer - Success Metrics: + Ensure deployment health + Improve agility + System stability + Optimize customer Experience + Justify technology investments\nDO DON\u0026rsquo;T Start with Fundamentals Stay in Tutorial Hell Learn by Building Real Projects Copy-paste blindly Document Everything Compare Your Progress to Others Master one thing at a time Give Up After Failures Soft Skills Enhancement - Continuous Integration: Team members integrate their work frequently, aims for continuous Delivery and Deployment\nInfrastructure as Code (IaC) -Benefits: Automation, Scalability, Reproducibility and better Collaboration\nAWS CloudFormation AWS\u0026rsquo;s own built in IaC tool, use templates written with YAML or JSON, can build every AWS Infrastructure automatically\n- Stack: A set of AWS Resources defined in a template, can be used by CloudFormation to create, update or delete said resources\n- CloudFormation Template: A YAML/JSON file that define an AWS Infrastructure, act like a blueprint to deploy and configure resources\n- How it works: Create template -\u0026gt; Store in S3 Bucket or Local storage -\u0026gt; Use CloudFormation to create Stacks based on template -\u0026gt; CloudFormation built resources\n- Drift Detection: Detect changes in the infrastructure compared to the Stack =\u0026gt; Update Stack or revert change, useful for versioning\nAWS Cloud Development Kit(CDK) Open-source software development framework, support IaC using real programming languages(Python,Java,C#.Net, Type/JavaScript and Go)\n- Construct: Building blocks, comprised of components that represent AWS Resources and their Configuration, have 3 construct level:\nL1 Construct: Low-level resources maps directly to a single AWS CloudFormation resource L2 Construct: Provide a higher-level abstraction through an intuitive intent-based API,encapsulate best practices and security defaults L3 Construct: Complete architecture patterns with multiple resources, opinionated implementation and fast deployment AWS Amplify AWS platform that makes it easy to build, deploy, and scale web and mobile apps, uses CloudFormation under the hood: Stacks deployed to built infrastructure programmatically\nTerraform IaC tool, start by defining infrastructure in Terraform code and plan then apply the infrastructure on multiple cloud platforms like Azure, AWS, Google Cloud, etc..\n- Strength: Multi-Cloud support, State tracking with the same configuration\nHow to choose IaC Tools? -Criteria:\nPlan using one Cloud or many? Role as Developer or Ops? Does the Cloud and Ecosystem support the tool? Container Services on AWS Dockerfile A Dockerfile defines how to build a container image, which describe the environment, dependencies, build steps, and final runtime configuration, ensuring that the application run consistently across any system that support Dockers\n- Images: A packaged blueprint of an application, build from a Dockerfile using layered file system, used to create containers consistently across environments\n- Workflow: Dockerfile build a Docker Image which can be used to run Container and push to ECR/Docker Hub\nAmazon ECR A fully managed container registry that make it easy to store, manage, and securely share Docker container image. AWS\u0026rsquo;s own secure and scalable private container registry\n-Features:\nImage Scanning Immutable Tags Lifecycle Policies Encryption \u0026amp; IAM - Orchestration: Orchestrate many containers processes: restart containers, scale up automatically under high load, distribute traffic efficiently, manage where containers are placed and run\nKubernetes Open source, automates deployment, scaling, healing, and load balancing - Components:\nMaster Node: Control Plane, manage worker nodes and pods Worker Node: Run application workloads inside pods Pod: Smallest deployable unit, can contain one or more containers Service ECS vs EKS\nFeature Amazon ECS (Elastic Container Service) Amazon EKS (Elastic Kubernetes Service) Core Technology AWS-native container orchestration Kubernetes-based (open-source standard) Complexity Simpler, easier to operate Highly flexible but more complex Knowledge Required No Kubernetes knowledge needed Requires Kubernetes knowledge (pods, deployments, etc.) AWS Integration Deep AWS integration (ALB, IAM, CloudWatch, etc.) Standard Kubernetes integration Use Case/Benefits Great for fast deployments \u0026amp; lower ops overhead Multi-cluster, multi-cloud portability Ecosystem/Community AWS-native tools and community Larger ecosystem \u0026amp; community tools Summary ECS = easier, faster to run, lower operational overhead EKS = more flexibility, more control, more complexity App Runner Suitable for quick deployment of web applications and REST APIS, ideal for small to medium production workload\nMonitoring \u0026amp; Observability CloudWatch Monitor AWS Resources and Applications running on AWS in real time Provide observabilty Alarms and automated responses Dashboard to help with operational and cost optimization - CloudWatch metrics: Data of the performance of system on AWS or on premise with CloudWatch Agent, integrate well with EventBridge, Auto Scaling and DevOps workflow\nAWS X-Ray - Distributed Tracing: Tracks requests end-to-end, and draw maps and paths between service visited, add SDK to code to trace IDs\n- Performance Insight: Root cause analysis for latency and errors, deduce insights from traces and provide Real User Monitoring\nEvent Experience This event was very important for our project as it tackle our plan of adding IaC using CDK, instead of using ClickOps for maintainability and reproducibility. Also some more insights on CloudWatch helped greatly with our data monitoring feature\nThe speakers answered our team\u0026rsquo;s question:\nQ: Our project up until now have been purely built with ClickOps, and we are planning to use CDK. Are there any tool that could scan and turn our existing infrastructure into CDK or CloudFormation rather than reproducing the infrastructure from scratch with IaC?\nA: Unfortunately no, there isn\u0026rsquo;t a tool that can assist with that problem yet, your team is going to have to built the infrastructure from scratch again. If there by any chance that you do found a tool that can assist with please share with us too.\nQ: We noticed that AWS X-Ray used with CloudWatch is similar to CloudTrail in its tracing method, can you explain more on what differentiate them?\nA: X-Ray is used for CloudWatch and used to trail the resources and services that the system interacted with, meanwhile CloudTrail is commonly used to trail the AWS user\u0026rsquo;s actions\nQ: Our project is built around Guard Duty Findings, do you have any experiences on how to reliably trigger Findings for a demo scenarios?\nA: In my experience I know that Guard Duty Findings can be triggered by port scanning activities but i\u0026rsquo;m sure there are other ways too\nA: Guard Duty can be configured to have a threat list containing custom rules to trigger findings upon activities relating the configured malicious domains or IPs\nThis event is also the first time some of the speaker\u0026rsquo;s first time presenting a topic:\nThe DevOps and IaC sections was well presented Monitoring \u0026amp; Observability wasn\u0026rsquo;t as great and we can notice the speaker\u0026rsquo;s nervousness but still delivered great values regardless Some event photos "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Finish Module 5 Support teammates with earlier labs Redo labs that were previously blocked due to Free Tier limitations Discuss project idea Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Lab 25 could not be completed because the account was still on the Free Tier.\n- Upgraded the account to a paid plan.\n- Retried Lab 25:\n+ The provided template used the deprecated Lambda runtime nodejs12.x, so it was updated to nodejs20.x.\n+ Set up the FSx file system.\n+ The default S3 endpoint for testing was only accessible in the US region, so the command needed to be changed to: Read-S3Object -BucketName nasanex -KeyPrefix /AVHRR -Folder Z:/nasanex/AVHRR -Region us-west-2.\n+ Created file shares.\n+ Configured both HDD and SSD FSx.\n+ Updated outdated tool version in section 25.4.\n+ Successfully performed drive benchmarking using different parameters.\n+ Used CloudWatch to monitor performance — throughput reached its 400 MB maximum, triggering the alarm.\n+ Learned FSx deduplication:\n• Default schedule runs every Saturday.\n• Initial dedup run made no optimizations due to fileAge defaults → set fileAge to 0 → ~50% of files optimized.\n+ Created shadow copies for backup.\n+ Learned how to manage and force-close open files.\n+ Configured user quotas to limit storage consumption.\n+ Enabled Continuously Available (CA) file shares so multiple users can access simultaneously.\n+ Scaled throughput and storage from the AWS Console.\n- Reviewed the Shared Responsibility Model: both AWS and the customer share security duties.\n- Module 5-2: Best practice is to create an admin IAM user instead of relying on the root account.\n- IAM Principal: An identity allowed to access AWS resources.\n- IAM Policy: Can be identity-based or resource-based.\n- IAM Role: A set of permissions for users/services, and can be used for cross-account access.\n- School subject:\n+ ENW493c: Completed Understanding Research Methods course. 22/09/2025 22/09/2025 Lab25 3 - Module 5:\n- Amazon Cognito: An authentication and user management service with two primary components:\n+ User Pools: Store user info and support third-party identity providers.\n+ Identity Pools: Map users to permissions and temporary credentials.\n- AWS Organizations: Manage multiple AWS accounts centrally using OUs and Service Control Policies.\n- AWS Identity Center (SSO): Manage application and AWS account access through permission sets.\n- AWS KMS: Create and manage encryption keys (CMKs), used to generate and encrypt/decrypt data keys.\n- AWS Security Hub: Evaluate security posture against AWS best practices.\n- Continued Lab 14:\n+ Set up IAM role and S3 bucket.\n+ Ubuntu 25.04 failed due to unsupported kernel → attempted Ubuntu 24.04 → still unsupported → installed Ubuntu 22.04 successfully.\n+ Imported VM into AWS.\n+ Connected to EC2 instance created from the AMI using the VM’s username/password. 23/09/2025 23/09/2025 Lab 14 4 Continue with lab 14:\n+ Created export bucket, configured permission + Successfully exported instances into .OVA format for usage - Lab 18: Enabled Security Hub and configured AWS Config to record data for analyzing (It can take a long for a score to be calculated) 24/09/2025 24/09/2025 Lab 14 5 - Lab 22:\n+ Built Lambda functions to automatically start/stop EC2 instances using schedules and tags.\n+ Sent notifications to Slack.\n- Lab 28:\n+ Created IAM policies and roles to allow access only from the Singapore region.\n+ Blocked EC2 access from outside the permitted region.\n+ Blocked EC2 instance creation without valid tags.\n- Lab 30: Restricted IAM user so they can only access EC2 in a specific region.\n- Lab 18 update: Security Hub scan completed — score: 85%, with 1 critical issue (an IAM user still had admin access).\n- Lab 33:\n+ Set up KMS.\n+ Configured CloudTrail to send logs to S3.\n+ Queried logs using Athena.\n+ Verified that KMS correctly denied access to unauthorized users. 25/09/2025 25/09/2025 Lab 22 Lab 28 Lab 30 Lab 18 Lab 33 6 - Lab 44: Configured role conditions, restricting access by IP, Time and others\n- Lab 48:\n+ Used IAM access key to upload file to S3 via EC2 Instance + Uploaded file to S3 via EC2 Instance without access key by using IAM Roles - Lab 12: + Created AWS Organization + Created accounts and move them into units + Invited accounts to organization + Switched roles for accounts under the organization + Setted up policies for the accounts under the organization + Installed Python to continue with the lab 26/09/2025 26/09/2025 Lab 44 Lab 48 Lab 12 Week 3 Achievements Upgraded AWS account and successfully completed labs previously blocked due to Free Tier limitations. Resolved multiple compatibility issues (e.g., outdated Lambda runtimes, unsupported kernels). Completed advanced Amazon FSx setup in Lab 25: Deduplication, shadow copies, user quotas, scaling, CloudWatch monitoring. Completed Module 5 theory: IAM, Cognito, Organizations, Identity Center, KMS. Applied real-world IAM security: Region restriction (Labs 28 \u0026amp; 30) IP/time-based IAM conditions (Lab 44) Secure S3 uploads using IAM Roles (Lab 48) Enabled Security Hub \u0026amp; AWS Config (Lab 18) → Achieved 85% security posture score. Built Lambda automation for EC2 scheduling with Slack integration (Lab 22). Successfully imported a VM, created AMI, and exported EC2 to .OVA (Lab 14). Set up AWS Organization with OUs, SCPs, role switching, and user/group management (Lab 12). Completed Microsoft Workloads labs: AD management EC2 troubleshooting via volume repair License attachment with Microsoft AD Configured CloudTrail, stored logs in S3, and used Athena for log analysis (Lab 33), with KMS enforcing access control. "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section will list and introduce the blogs you have translated.\nBlog 1 - Giới thiệu AWS Trust Center Blog này giới thiệu về AWS Trust Center, một nguồn tài nguyên trực tuyến thống nhất giúp khách hàng dễ dàng tiếp cận thông tin về bảo mật, tuân thủ và quyền riêng tư của AWS. Bài viết giải thích cách Trust Center hỗ trợ người dùng tìm hiểu về các biện pháp kiểm soát trung tâm dữ liệu, xem báo cáo đánh giá tuân thủ, và hiểu rõ cơ chế kiểm soát truy cập, từ đó xây dựng niềm tin vững chắc vào sự minh bạch của nền tảng đám mây.\nBlog 2 - Cách Derive mở rộng nền tảng giao dịch phi tập trung độ trễ thấp Bài viết này chia sẻ câu chuyện thành công của Derive trong việc xây dựng nền tảng giao dịch phi tập trung lai (hybrid DEX) với độ trễ cực thấp và khả năng xử lý hàng tỷ đô la khối lượng giao dịch. Bạn sẽ khám phá kiến trúc sử dụng AWS Graviton trên Amazon EKS kết hợp với Amazon Aurora để tối ưu hóa hiệu năng tính toán, quản lý rủi ro và xử lý khớp lệnh tốc độ cao, đồng thời tìm hiểu về mô hình kết hợp giữa hiệu quả tập trung và bảo mật phi tập trung.\nBlog 3 - SQL to NoSQL: Hiện đại hóa lớp truy cập dữ liệu với Amazon DynamoDB Đây là phần cuối trong loạt bài về di chuyển từ SQL sang NoSQL, tập trung vào việc thiết kế lớp truy cập dữ liệu hiệu quả cho Amazon DynamoDB. Blog hướng dẫn chi tiết về cách xử lý các mô hình truy cập phức tạp như thiết kế lớp trừu tượng API, xử lý bộ lọc (filtering), thực hiện phân trang (pagination), quản lý các trường hợp biên và thực hiện các phép tổng hợp dữ liệu (aggregations) để tối ưu hóa hiệu suất ứng dụng.\n"},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"CÁCH DERIVE MỞ RỘNG NỀN TẢNG GIAO DỊCH PHI TẬP TRUNG ĐỘ TRỄ THẤP BẰNG AWS GRAVITON, AMAZON EKS VÀ AMAZON AURORA Tác giả: Daniel Wirjo, Christoph Niemann, Dillon Lin, và Joshua Kim Ngày: 04/06/2025 Chuyên mục: Amazon Aurora, Amazon Elastic Kubernetes Service, Blockchain, Customer Solutions, Graviton, Intermediate (200)\nĐây là bài viết cộng tác của Joshua Kim, Trưởng nhóm Kỹ thuật tại Derive, và Dillon Lin, Trưởng nhóm Phát triển tại Derive, phối hợp cùng AWS.\nThị trường phái sinh được dự báo sẽ tăng trưởng mạnh mẽ trong thập kỷ tới, được thúc đẩy bởi sự gia tăng tham gia của các tổ chức, đổi mới công nghệ và xu hướng chuyển dịch sang mô hình on-chain, tokenization và tài chính phi tập trung (DeFi). Khi thị trường toàn cầu đối mặt với nhiều biến động, phái sinh đang trở thành công cụ thiết yếu để quản lý rủi ro và tận dụng cơ hội. Sự hội tụ giữa các công cụ tài chính truyền thống và công nghệ blockchain này được kỳ vọng sẽ định hình lại toàn cảnh thị trường phái sinh, mang đến những cơ hội tăng trưởng và đổi mới mới trong ngành.\nTrong bài viết này, chúng tôi chia sẻ cách Derive đã mở rộng thành công nền tảng giao dịch phi tập trung lai (hybrid decentralized trading platform) của họ - đạt được hàng tỷ đô la khối lượng giao dịch và độ trễ cực thấp, bằng cách sử dụng cơ sở hạ tầng tính toán và cơ sở dữ liệu mạnh mẽ, kết hợp AWS Graviton trên Amazon Elastic Kubernetes Service (Amazon EKS) và Amazon Aurora. Chúng tôi cũng khám phá mô hình sàn giao dịch lai của Derive và vai trò quan trọng mà AWS đã đóng góp trong quá trình tăng trưởng và mở rộng quy mô của họ.\nTổng quan: Mô hình sàn giao dịch lai độc đáo của Derive Derive là một giao thức DeFi tạo ra các sản phẩm phái sinh on-chain có thể lập trình và tùy chỉnh, bao gồm options (quyền chọn), perpetuals (hợp đồng vĩnh viễn) và các sản phẩm cấu trúc khác.\nNền tảng này vận hành theo mô hình lai (hybrid model) – kết hợp hiệu quả của việc khớp lệnh tập trung với tính bảo mật và minh bạch của việc thanh toán phi tập trung và quản lý ký quỹ trên Derive chain, được xây dựng trên OP Stack, một blockchain Ethereum Layer 2 mã nguồn mở. Cách tiếp cận này cho phép Derive mang đến trải nghiệm giao dịch liền mạch tương tự như các sàn tập trung (CEX), đồng thời vẫn đảm bảo người dùng kiểm soát tài sản của mình.\nĐiều này được hỗ trợ bởi hệ thống ký quỹ dựa trên đấu giá mở, giúp tăng tính minh bạch và giảm rủi ro thường thấy trong các hệ thống tập trung truyền thống nơi tài sản dự trữ do một bên trung tâm quản lý.\nMặc dù Derive cung cấp các sản phẩm giao dịch tiêu chuẩn như perpetuals và spot trading, nhưng trọng tâm cốt lõi của họ là phục vụ các nhà giao dịch chuyên nghiệp và tổ chức. Giải pháp của Derive cung cấp thanh khoản on-chain cho các quyền chọn phi tập trung (decentralized options), thông qua order book và hệ thống Request-for-Quote (RFQ) mạnh mẽ - cho phép thực hiện các chiến lược giao dịch nâng cao và tối ưu hóa việc khám phá giá (price discovery) trong thị trường phi tập trung.\nVới sự tăng trưởng nhanh chóng của cơ sở người dùng và khối lượng giao dịch, Derive cần đảm bảo rằng hạ tầng của họ có thể xử lý nhu cầu ngày càng tăng mà không ảnh hưởng đến hiệu suất, ngay cả khi đội ngũ kỹ thuật tinh gọn và trong những giai đoạn biến động thị trường cao.\nTổng quan giải pháp Sơ đồ sau minh họa kiến trúc tổng thể cho nền tảng có hiệu suất cao và độ trễ thấp của Derive.\nKiến trúc tổng thể bao gồm nhiều thành phần chính, được trình bày chi tiết bên dưới.\nTính toán hiệu năng cao\nKiến trúc của Derive bao gồm hệ thống tính toán hiệu năng cao (high-performance compute) chạy trên Graviton nodes, được triển khai theo kiến trúc microservices trong Amazon EKS. Điều này mang lại độ trễ thấp và khả năng mở rộng cao, giúp xử lý giao dịch nhanh chóng.\nCác microservice được thiết kế theo mô-đun và có thể mở rộng ngang (horizontally scalable), giúp hệ thống xử lý khối lượng giao dịch lớn một cách hiệu quả, với các dịch vụ chuyên biệt cho những chức năng cốt lõi sau:\nCore matching engine: Được xây dựng bằng Rust để đạt độ trễ thấp; chịu trách nhiệm xử lý lệnh, khớp giao dịch và quản lý trạng thái. Risk management: Các microservice chuyên dụng đảm nhận tính toán rủi ro tiêu chuẩn và danh mục đầu tư, đảm bảo tuân thủ và bảo vệ nền tảng lẫn người dùng. Emitters: Các dịch vụ như Orderbook Emitter và Ticker Emitter phát sóng các thay đổi trạng thái và cập nhật giá đến hệ thống và người dùng bên ngoài. Additional microservices: Kiến trúc có thể mở rộng, hỗ trợ việc bổ sung các microservice mới khi cần cho sự phát triển trong tương lai. Cơ sở dữ liệu\nCác microservice giao dịch dữ liệu trên PostgreSQL trong Amazon Aurora, tận dụng khả năng mở rộng và độ tin cậy cao của hệ thống này.\n“Trong ngày ra mắt, chúng tôi có hơn 50.000 người dùng chỉ trong một ngày, thực hiện hơn 50 triệu yêu cầu API khi giao dịch. Amazon Aurora đã đóng vai trò then chốt trong việc giúp chúng tôi nhanh chóng mở rộng cơ sở dữ liệu để xử lý lượng yêu cầu đọc khổng lồ.” — Josh Kim, Trưởng nhóm Kỹ thuật tại Derive.\nTổng hợp và thanh toán blockchain\nMô hình sàn giao dịch lai của Derive yêu cầu một thành phần quản lý việc tổng hợp (aggregation rollup) và thanh toán (settlement) giao dịch, kết nối với các microservice để hoàn tất giao dịch.\nĐể thực hiện thanh toán, Derive xây dựng blockchain Layer 2 dựa trên Ethereum, dùng OP Stack mã nguồn mở làm giải pháp mở rộng quy mô. Cách tiếp cận này mang lại tốc độ xử lý cao, chi phí thấp và kế thừa bảo mật từ Ethereum, cân bằng giữa tốc độ và an toàn.\nKim giải thích: \u0026ldquo;Các sàn lai thêm một tầng phức tạp mới cho giao dịch độ trễ thấp, vì chúng yêu cầu tự quản lý tài sản (self-custody) thông qua chữ ký lệnh mật mã và xác thực rủi ro on-chain theo thời gian thực, ngay cả khi việc khớp lệnh diễn ra off-chain. Một ví dụ điển hình là nhà giao dịch tần suất cao bị thanh lý on-chain trong khi đang gửi hàng nghìn lệnh đến sàn Derive. Engine khớp lệnh cần có khả năng truy cập nhanh vào trạng thái giao thức on-chain và xử lý song song một số khối lượng công việc nhất định.\u0026rdquo;\nKiến trúc hướng sự kiện (Event-driven architecture)\nCác yêu cầu của người dùng được xử lý thông qua các endpoint WebSocket và REST API, đảm bảo bảo mật và hiệu quả khi quản lý lệnh và dữ liệu thị trường. Hệ thống hàng đợi dựa trên Valkey điều phối việc xếp hàng và xử lý lệnh đến, đồng thời cơ chế giới hạn tốc độ (rate limiter) đảm bảo công bằng và ngăn chặn lạm dụng.\nĐể phát dữ liệu thị trường theo thời gian thực và cập nhật lệnh với độ trễ tối thiểu, cơ chế publish/subscribe dựa trên Valkey giúp truyền tải sự kiện đến tất cả các client được kết nối một cách hiệu quả. Kiến trúc còn bao gồm event streaming để theo dõi và ghi lại thay đổi trạng thái phục vụ các microservice hạ tầng. Cách tiếp cận hướng sự kiện này sử dụng NATS, được triển khai trên Amazon EKS, giúp đảm bảo tính mở rộng và độ tin cậy cho toàn bộ nền tảng.\nKim cho biết: \u0026ldquo;Ngoài việc xây dựng engine bằng Rust hiệu năng cao, chúng tôi còn phát triển nhiều thành phần nội bộ đặc thù – một trong số đó là hệ thống hàng đợi tùy chỉnh để định tuyến lệnh trong phạm vi độ trễ cực thấp. Đội ngũ AWS đã hỗ trợ tuyệt vời qua nhiều phiên brainstorm kỹ thuật trực tiếp với các chuyên gia nội bộ, giúp cải thiện tốc độ thực thi đáng kể. Ngay sau khi ra mắt, sàn giao dịch đã xử lý khối lượng 3,7 tỷ đô la và giao dịch gần 1 triệu lệnh on-chain chỉ trong một tháng.\u0026rdquo;\nBảo mật\nBên cạnh lớp bảo mật vốn có của việc thanh toán phi tập trung, Derive còn sử dụng các dịch vụ bảo mật của AWS như AWS Control Tower (Quản trị đám mây tập trung), AWS IAM Identity Center (Quản lý danh tính đơn giản hóa), AWS Key Management Service (AWS KMS – Mã hóa an toàn), Amazon GuardDuty (Phát hiện mối đe dọa thông minh). Nhờ đó, Derive có thể đáp ứng các yêu cầu bảo mật nghiêm ngặt và tiêu chuẩn cao mà các nhà đầu tư tổ chức kỳ vọng.\nHợp tác với AWS Derive đã hợp tác với AWS ngay từ khi khởi đầu, tham gia chương trình AWS Activate dành cho startup. Sau khi nhận đầu tư từ Coinbase Ventures và Framework Ventures (đều là nhà cung cấp AWS Activate), Derive đã có thể truy cập tín dụng AWS hỗ trợ cho phát triển sản phẩm.\nDominic Romanowski, Đồng sáng lập Derive, chia sẻ: “Đội ngũ AWS Startups đóng vai trò cực kỳ quan trọng trong hành trình của chúng tôi - mang lại nguồn tài trợ, công cụ và chuyên môn giúp chúng tôi đổi mới trong không gian DeFi. Bằng cách tận dụng sức mạnh của hạ tầng AWS, chúng tôi không chỉ tối ưu chi phí, mà còn đảm bảo nền tảng mở rộng liền mạch để đáp ứng nhu cầu ngày càng tăng, đồng thời giữ vững bảo mật. Điều đó giúp chúng tôi tập trung vào điều quan trọng nhất: xây dựng nền tảng giao dịch hiệu năng cao cho cả khách hàng tổ chức và bán lẻ.”\nDerive cũng làm việc chặt chẽ với các chuyên gia chuyên môn của AWS trong nhiều lĩnh vực, bao gồm tính toán, cơ sở dữ liệu và Web3, để giải quyết các thách thức kỹ thuật độc đáo và tăng tốc phát triển. Romanowski nói thêm: \u0026ldquo;Thật yên tâm khi biết rằng chúng tôi có thể gọi và nhận tư vấn chuyên sâu bất cứ khi nào cần.\u0026rdquo;.\nLộ trình tiếp theo Nhìn về phía trước, Derive cam kết liên tục nâng cao năng lực và giảm độ trễ. Trong lộ trình sắp tới, Derive tập trung vào hai sáng kiến chính:\nKhả năng phát lại và kiểm toán toàn diện (replay \u0026amp; audit): Cho phép người dùng phát lại chính xác các lệnh và giao dịch, phục vụ khách hàng tổ chức cần dữ liệu lịch sử chi tiết để debug thuật toán và đảm bảo công bằng. Tích hợp sâu hơn với Ethereum Virtual Machine (EVM): Biến Derive thành một phần mở rộng tự nhiên của node EVM. Cách tiếp cận này giúp giảm sai lệch giữa số dư trên sàn và trạng thái on-chain, đơn giản hóa cấu trúc dữ liệu thị trường, tài khoản và giao dịch, và trong tương lai cho phép người dùng tích hợp liền mạch các thành phần của Derive với các giao thức khác.\nDillon Lin, Trưởng nhóm Phát triển tại Derive, cho biết: “Chúng tôi tin rằng những phát triển này sẽ đưa Derive trở thành người dẫn đầu trong giao dịch blockchain-native, đồng thời mở đường cho những đổi mới rộng hơn trong ngành.”.\nKết luận Bằng việc sử dụng các dịch vụ AWS, Derive có thể tập trung vào đổi mới và mở rộng danh mục sản phẩm trong tokenization và phái sinh phi tập trung. Sự kết hợp này giúp Derive mở rộng quy mô thành công, mang lại môi trường giao dịch an toàn và hiệu quả, đáp ứng nhu cầu ngày càng phát triển của các tổ chức tài chính.\nLin khẳng định: “Khi chúng tôi tiếp tục đổi mới trong lĩnh vực DeFi và đáp ứng nhu cầu ngày càng tăng về phái sinh on-chain, sự hợp tác với AWS vẫn là yếu tố then chốt. Sự hỗ trợ của họ đóng vai trò cực kỳ quan trọng trong hành trình của chúng tôi, giúp mở rộng hiệu quả và cung cấp giải pháp phù hợp với nhu cầu của các nhà đầu tư tổ chức. Chúng tôi mong muốn tiếp tục hợp tác với AWS để mở rộng nền tảng hơn nữa.”\nTài nguyên tham khảo:\nTìm hiểu thêm về phái sinh phi tập trung tại Derive và Developer Hub của họ. Tìm hiểu thêm về Web3 trên AWS. Tìm hiểu thêm về AWS Startups tại startups.aws. Về tác giả Joshua Kim: Trưởng bộ phận Kỹ thuật tại Derive, lãnh đạo phát triển hạ tầng phái sinh. Từng làm việc tại Apple và sở hữu nhiều bằng sáng chế.\nDillon Lin: Trưởng bộ phận Phát triển tại Derive, phụ trách chiến lược go-to-market và phân phối cho người dùng bán lẻ và tổ chức.\nDaniel Wirjo: Kiến trúc sư Giải pháp tại AWS, hỗ trợ các startup và từng là CTO của một startup.\nChristoph Niemann: Kiến trúc sư Blockchain Cao cấp tại AWS, đam mê công nghệ Blockchain.\n"},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.7-dashboard-setup/5.7.3-setup-api-gateway/","title":"API Gateway Setup","tags":[],"description":"","content":"In this guide, you will setup an API Gateway to route api call from dashboard to Lambda.\nCreate API Gateway Open the API Gateway Console\nNavigate to https://console.aws.amazon.com/apigateway/ Or: AWS Management Console → Services → API Gateway Create API:\nClick Create API Choose REST API and click Build Use this setting for creation: Choose New API Name: dashboard-api API endpoint type: Regional Security policy: SecurityPolicy_TLS13_1_3_2025_09 Endpoint access mode: Basic IP address type: IPv4 Create Resources:\nEnable CORS for the root resource Click Create resource and name it logs Then click on /logs resource that just created and click Create Resource to create child resource of /logs Name it cloudtrail and enable CORS Repeat this three more times for eni_logs, guardduty and vpc Create methods:\nClick on /cloudtrail that just created and click Cretae method\nIn method creation, use this setting:\nMethod type: GET Intergration type: Lambda function Enable Lambda proxy intergration choose Buffered Lambda function: select your region search for dashboard-query and choose it Timout: 29000 Repeat this three more time for eni_logs, guardduty and vpc\nDeploy API:\nClick the Deploy API on the right corner In deploy API, use this setting: Stage: New stage Name: prod Click Deploy "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.11-appendices/5.11.3-cloudwatch-etl/","title":"CloudWatch ETL Code","tags":[],"description":"","content":"import json import boto3 import gzip import re import os from datetime import datetime, timezone s3 = boto3.client(\u0026#34;s3\u0026#34;) firehose= boto3.client(\u0026#34;firehose\u0026#34;) # -------------------------------------------------- # CONFIG # -------------------------------------------------- SOURCE_PREFIX = \u0026#34;exportedlogs/vpc-dns-logs/\u0026#34; FIREHOSE_STREAM_NAME = os.environ.get(\u0026#34;FIREHOSE_STREAM_NAME\u0026#34;) VPC_RE = re.compile(r\u0026#34;/(vpc-[0-9A-Za-z\\-]+)\u0026#34;) ISO_TS_RE = re.compile(r\u0026#34;^\\d{4}-\\d{2}-\\d{2}T\u0026#34;) def read_gz(bucket, key): obj = s3.get_object(Bucket=bucket, Key=key) with gzip.GzipFile(fileobj=obj[\u0026#34;Body\u0026#34;]) as f: return f.read().decode(\u0026#34;utf-8\u0026#34;, errors=\u0026#34;replace\u0026#34;) def flatten_once(d): out = {} for k, v in (d or {}).items(): if isinstance(v, dict): for k2, v2 in v.items(): out[f\u0026#34;{k}_{k2}\u0026#34;] = v2 else: out[k] = v return out def safe_int(x): try: return int(x) except: return None def parse_dns_line(line): raw = line.strip() if not raw: return None json_part = raw prefix_ts = None if ISO_TS_RE.match(raw): try: prefix_ts, rest = raw.split(\u0026#34; \u0026#34;, 1) json_part = rest except: pass if not json_part.startswith(\u0026#34;{\u0026#34;): idx = json_part.find(\u0026#34;{\u0026#34;) if idx != -1: json_part = json_part[idx:] try: obj = json.loads(json_part) except: return None flat = flatten_once(obj) if prefix_ts: flat[\u0026#34;_prefix_ts\u0026#34;] = prefix_ts return flat def lambda_handler(event, context): print(f\u0026#34;Received S3 Event. Records: {len(event.get(\u0026#39;Records\u0026#39;, []))}\u0026#34;) firehose_records = [] for record in event.get(\u0026#34;Records\u0026#34;, []): if \u0026#34;s3\u0026#34; not in record: continue bucket = record[\u0026#34;s3\u0026#34;][\u0026#34;bucket\u0026#34;][\u0026#34;name\u0026#34;] key = record[\u0026#34;s3\u0026#34;][\u0026#34;object\u0026#34;][\u0026#34;key\u0026#34;] if not key.startswith(SOURCE_PREFIX) or not key.endswith(\u0026#34;.gz\u0026#34;): print(f\u0026#34;Skipping file: {key}\u0026#34;) continue print(f\u0026#34;Processing S3 file: {key}\u0026#34;) # Extract VPC ID from file path vpc_id_match = VPC_RE.search(key) vpc_id = vpc_id_match.group(1) if vpc_id_match else \u0026#34;unknown\u0026#34; # Read and process file content content = read_gz(bucket, key) if not content: continue for line in content.splitlines(): r = parse_dns_line(line) if not r: continue # Create flattened JSON record out = { \u0026#34;version\u0026#34;: r.get(\u0026#34;version\u0026#34;), \u0026#34;account_id\u0026#34;: r.get(\u0026#34;account_id\u0026#34;), \u0026#34;region\u0026#34;: r.get(\u0026#34;region\u0026#34;), \u0026#34;vpc_id\u0026#34;: r.get(\u0026#34;vpc_id\u0026#34;, vpc_id), \u0026#34;query_timestamp\u0026#34;: r.get(\u0026#34;query_timestamp\u0026#34;), \u0026#34;query_name\u0026#34;: r.get(\u0026#34;query_name\u0026#34;), \u0026#34;query_type\u0026#34;: r.get(\u0026#34;query_type\u0026#34;), \u0026#34;query_class\u0026#34;: r.get(\u0026#34;query_class\u0026#34;), \u0026#34;rcode\u0026#34;: r.get(\u0026#34;rcode\u0026#34;), \u0026#34;answers\u0026#34;: json.dumps(r.get(\u0026#34;answers\u0026#34;), ensure_ascii=False), \u0026#34;srcaddr\u0026#34;: r.get(\u0026#34;srcaddr\u0026#34;), \u0026#34;srcport\u0026#34;: safe_int(r.get(\u0026#34;srcport\u0026#34;)), \u0026#34;transport\u0026#34;: r.get(\u0026#34;transport\u0026#34;), \u0026#34;srcids_instance\u0026#34;: r.get(\u0026#34;srcids_instance\u0026#34;), \u0026#34;timestamp\u0026#34;: (r.get(\u0026#34;query_timestamp\u0026#34;) or r.get(\u0026#34;timestamp\u0026#34;) or r.get(\u0026#34;_prefix_ts\u0026#34;)) } # Add newline for JSONL format json_row = json.dumps(out, ensure_ascii=False) + \u0026#34;\\n\u0026#34; firehose_records.append({\u0026#39;Data\u0026#39;: json_row}) # Send to Firehose in batches of 500 if firehose_records: total_records = len(firehose_records) print(f\u0026#34;Sending {total_records} records to Firehose...\u0026#34;) batch_size = 500 for i in range(0, total_records, batch_size): batch = firehose_records[i:i + batch_size] try: response = firehose.put_record_batch( DeliveryStreamName=FIREHOSE_STREAM_NAME, Records=batch ) if response[\u0026#39;FailedPutCount\u0026#39;] \u0026gt; 0: print(f\u0026#34;Warning: {response[\u0026#39;FailedPutCount\u0026#39;]} records failed\u0026#34;) except Exception as e: print(f\u0026#34;Firehose error: {e}\u0026#34;) return {\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;, \u0026#34;total_records\u0026#34;: len(firehose_records)} "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.3-foundation-setup/5.3.3-create-iam-roles-and-policies/5.3.3.3-create-iam-policy/","title":"Create IAM Policy","tags":[],"description":"","content":"Create IAM Quarantine Policy Create IrQuarantineIAMPolicy Navigate to IAM Console → Policies → Create policy\nPolicy JSON:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Policy name: IrQuarantineIAMPolicy Description: Deny-all policy for quarantining compromised IAM users "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.3-foundation-setup/5.3.3-create-iam-roles-and-policies/","title":"Create IAM Roles and Policies","tags":[],"description":"","content":"In this section, you will create 17 IAM roles with their associated policies for Lambda functions, Firehose streams, Step Functions, and other services.\nOverview of IAM Roles Lambda Execution Roles (9 roles):\nCloudTrailETLLambdaServiceRole GuardDutyETLLambdaServiceRole CloudWatchETLLambdaServiceRole CloudWatchENIETLLambdaServiceRole CloudWatchExportLambdaServiceRole ParseFindingsLambdaServiceRole IsolateEC2LambdaServiceRole QuarantineIAMLambdaServiceRole AlertDispatchLambdaServiceRole Service Roles (6 roles): 10. CloudTrailFirehoseRole 11. CloudWatchFirehoseRole 12. StepFunctionsRole 13. IncidentResponseStepFunctionsEventRole 14. FlowLogsIAMRole 15. GlueCloudWatchRole\nIAM Policy (1 policy): 16. IrQuarantineIAMPolicy\nContent Create Lambda Execution Roles Create Service Roles Create IAM Policy "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.5-processing-setup/5.5.3-create-lambda-function-etl-processing/","title":"Create Lambda Function - ETL Processing","tags":[],"description":"","content":"Create Lambda Functions - ETL Processing In this section, you will create 5 Lambda functions that process logs and send them to Kinesis Firehose or S3.\nincident-response-cloudtrail-etl Runtime: Python 3.12 Handler: CloudTrailETL.lambda_handler Role: CloudTrailETLLambdaServiceRole Timeout: 300s, Memory: 128MB Env: FIREHOSE_STREAM_NAME=cloudtrail-firehose-stream Code: cloudtrail-etl incident-response-guardduty-etl Runtime: Python 3.12 Handler: guardduty_etl.lambda_handler Role: GuardDutyETLLambdaServiceRole Timeout: 300s, Memory: 128MB Env: DESTINATION_BUCKET, S3_LOCATION_GUARDDUTY, DATABASE_NAME, TABLE_NAME_GUARDDUTY Code: guardduty-etl cloudwatch-etl-lambda Runtime: Python 3.12 Handler: cloudwatch_etl.lambda_handler Role: CloudWatchETLLambdaServiceRole Env: FIREHOSE_STREAM_NAME=vpc-dns-firehose-stream Code: cloudwatch-etl cloudwatch-eni-etl-lambda Runtime: Python 3.12 Handler: cloudwatch_eni_etl.lambda_handler Role: CloudWatchENIETLLambdaServiceRole Env: FIREHOSE_STREAM_NAME=vpc-flow-firehose-stream Code: cloudwatch-eni-etl cloudwatch-export-lambda Runtime: Python 3.12 Handler: cloudwatch_autoexport.lambda_handler Role: CloudWatchExportLambdaServiceRole Env: DESTINATION_BUCKET=incident-response-log-list-bucket-ACCOUNT_ID-REGION Code: cloudwatch-autoexport Configure CloudWatch Logs Subscription Filter Configure Subscription Filter Open the CloudWatch Console.\nIn the left navigation pane, select Log Management.\nClick on the centralized log group: /aws/incident-response/centralized-logs.\nCreate Subscription Filter:\nClick the \u0026ldquo;Subscription filters\u0026rdquo; tab. Click \u0026ldquo;Create Lambda subscription filter\u0026rdquo;. Configure Destination:\nDestination Lambda function: Select the function cloudwatch-export-lambda. Log format: Select \u0026ldquo;Other\u0026rdquo;. (This ensures the raw log data is passed efficiently for Lambda processing). Configure Log Format and Filter:\nSubscription filter name: Enter a descriptive name, e.g., VPC-Log-Export-Filter. Filter pattern: Leave this field blank. (Ensures all logs in the group are processed). Click \u0026ldquo;Start streaming\u0026rdquo;.\nConfigure S3 Event Notifications S3 Console → incident-response-log-list-bucket-ACCOUNT_ID-REGION → Properties → Event notifications\nCreate 4 notifications with Event types/Object creation/✅All object create events:\nCloudTrailETLTrigger: Prefix AWSLogs/ACCOUNT_ID/CloudTrail/ → Lambda incident-response-cloudtrail-etl VPCDNSLogsTrigger: Prefix exportedlogs/vpc-dns-logs/ → Lambda cloudwatch-etl-lambda VPCFlowLogsTrigger: Prefix exportedlogs/vpc-flow-logs/ → Lambda cloudwatch-eni-etl-lambda GuardDutyFindingsTrigger: Prefix AWSLogs/ACCOUNT_ID/GuardDuty/ → Lambda incident-response-guardduty-etl "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.3-foundation-setup/","title":"Foundation Setup","tags":[],"description":"","content":"This initial Foundation Setup phase establishes the core prerequisites for the Auto Incident Response System, concentrating on the deployment of dedicated storage and essential security authorization. This mandates the creation of five secure Amazon S3 buckets for centralized log ingestion and processing, applying a necessary Bucket Policy for secure log delivery, and defining 17 IAM roles and a quarantine policy to enforce least-privilege access across all integrated AWS services.\nContent Set up Amazon S3 Bucket Configure S3 Bucket Policy for Primary Log Bucket Create IAM Roles and Policies "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Summary Report: “AWS Cloud Mastery Series #3: AWS Well-Architected – Security Pillar Workshop” Event Objectives AWS Cloud Club Introduction Pillar 1: Identity a\u0026amp; Access Management (IAM) Pillar 2: Detection \u0026amp; Continuous Monitoring Pillar 3: Infrastructure Protection Pillar 4: Data Protection Pillar 5: Incident Response Speakers Le Vu Xuan An - AWS Cloud Club Captain HCMUTE\nTran Duc Anh - AWS Cloud Club Captain SGU\nTran Doan Cong Ly - AWS Cloud Club Captain PTIT\nDanh Hoang Hieu Nghi - AWS Cloud Club Captain HUFLIT\nHuynh Hoang Long - AWS Community Builders\nDinh Le Hoang Anh - AWS Community Builders\nNguyen Tuan Thinh - Cloud Engineer Trainee\nNguyen Do Thanh Dat - Cloud Engineer Trainee\nVan Hoang Kha - Cloud Security Engineer, AWS Community Builder\nThinh Lam - FCJ Member\nViet Nguyen - FCJ Member\nMendel Grabski (Long) - Ex-Head of Security \u0026amp; DevOps, Cloud Security Solution Architect\nTinh Truong - Platform Engineer at TymeX, AWS Community Builder\nKey Highlights AWS Cloud Club An introduction to AWS Cloud Club:\nHelps explore and grow cloud computing skills Develop technical leadership Build meaningful connections globally Provides hands-on AWS Experiences, mentorship with AWS Professional and long-term career support AWS Cloud Clubs that are part of FCJA:\nAWS Cloud Club HCMUTE AWS Cloud Club SGU AWS Cloud Club PTIT AWS Cloud Club HUFLIT Benefits: Build Skills, Community and Opportunities\nIdentity \u0026amp; Access Management (IAM) IAM is an essential AWS service, responsible for controlling secure access. IAM manages Users, Groups, Roles, and Permissions, and ensures both authentication and authorization.\nBest Practices include:\nLeast Privilege Principle.\nDelete root access keys post-creation.\nAvoid \u0026ldquo;*\u0026rdquo; in Actions/Resources\nUse Single Sign-On (SSO) for multi-account integration and centralized access management.\nService Control Policies (SCPs): Organization-level policies that set the maximum available permissions for all accounts within an Organization. SCPs only filter permissions; they never grant permissions.\nPermission Boundaries: Sets the maximum permissions that an identity-based policy can grant to a specific User/Role within an account.\nMFA :\nTOTP (Time-based One-Time Password) FIDO2 (Fast Identity Online 2) Shared secret Public-key cryptography Requires manually typing a 6-digit code Requires a simple touch or biometric scan Free Variable Flexible backups and recovery Strict backups and zero recovery Credential Rotation with AWS Secrets Manager:\nCredential Updater uses Secrets Manager functions in a cycle: Create Secret, Set Secret (e.g., every 7 days) Test Secret, and Finish Secret Rotation events can be sent to an EventBridge Schedule for timing control. Finally deprecate previous Secret Detection and Continous Monitoring Multi-Layer Security Visibility:\nManagement Events:** API calls and console actions across all organization accounts. Data Events:** S3 object access and Lambda executions at scale. Network Activity Events:** VPC Flow Logs integration for network-level monitoring. Organization Coverage:** Unified logging across all member accounts and regions. Alerting \u0026amp; Automation with EventBridge\nReal-time Events: CloudTrail events flow to EventBridge for immediate processing. This is the foundation of Event-Driven Architecture (EDA), allowing systems to react to changes as they occur.\nAutomated Alerting: Detect suspicious activities across all organization accounts.\nCross-account Event Routing: Centralized event processing and automated response. EventBridge makes this seamless, routing events based on rules to targets across accounts or regions.\nIntegration \u0026amp; Workflows: Lambda, SNS, SQS integration for automated security workflows.\nDetection-as-Code:\nCloudTrail Lake Queries: Creating and using SQL-based detection rules for advanced threat hunting.\nVersion-Controlled Logic: Detection rules and logic are tracked and managed through code repositories.\nAutomated Deployment: The trails and detection rules are deployed automatically across all relevant organization accounts, ensuring uniform security coverage.\nInfrastructure-as-Code (IaC): Uses IaC tools for the automated setup and configuration of the organization\u0026rsquo;s logging and event trails\nGuard Duty GuardDuty is an Always-On, Intelligent Threat Detection solution\nHow GuardDuty Works – Rely on continuous analysis of Three Pillars of Detection:\nData Source What It Monitors Real-World Example CloudTrail Events IAM actions, permission changes, API calls Attacker disables logging to cover tracks. VPC Flow Logs Network traffic to/from your resources EC2 sending data to a botnet C2 server. DNS Logs DNS queries from your infrastructure Malware-infected queries cryptomining sites. Advanced Protection Plans: GuardDuty offers specialized detection add-ons for complete coverage:\nS3 Protection: Detects abnormal S3 access patterns and scans malware in S3 objects at upload time.\nEKS Protection: Monitors Kubernetes audit logs for unauthorized access and chains findings with S3 to map the full attack path.\nMalware Protection: Automatically scans EBS volumes of EC2 instances when compromise is suspected.\nRDS Protection: Analyzes login activity logs to databases (Aurora/RDS) and detects brute-force attacks (multiple failed login attempts from a single IP).\nLambda Protection: Monitors network logs flowing from Lambda function invocations and detects if a compromised function sends data to malicious IPs.\nRuntime Monitoring – Deep Inside Your OS: Achieved using a GuardDuty Agent installed on EC2/EKS/ECS Fargate. It monitors running processes, file access patterns, system calls, and attempts at privilege escalation or reverse shells.\nCompliance Standards:\nAWS Foundational Security Best Practices: Developed by AWS and covers a range of AWS services.\nCIS AWS Foundations Benchmark: developed by: AWS and industry professionals focusing on Identity (IAM), Logging \u0026amp; Monitoring, and Networking.\nCompliance Enforcement with Detection-as-Code\nIaC Tool: AWS CloudFormation is used to deploy configurations.\nCompliance Engine: AWS CloudFormation pushes configuration checks to AWS Security Hub CSPM.\nCompliance Standards Applied: Security Hub performs checks against the listed standards (AWS Foundational Security Best Practices, CIS AWS Foundations Benchmark, PCI DSS, NIST).\nResources Covered: Amazon S3, Amazon EC2, and Amazon RDS.\nNetwork Security Controls Attack Vectors: Threats are categorized into Ingress Attacks (DDoS, SQL injection), Egress Attacks (data exfiltration, DNS tunneling), and Inside Attacks (lateral movement).\nSecurity Groups (SG): Act as a Stateful firewall at the instance/interface level. They only support allow rules and have an implicit deny all.\nNetwork ACLs (NACLs): Operate at the subnet level. They are stateless and use numbered rules to explicitly ALLOW or DENY traffic.\nAWS TGW Security Group Referencing: Allows Transit Gateway (TGW) VPCs to define Inbound rules using only SG references.\nRoute 53 Resolver: Routes DNS queries to Private DNS (private hosted zones), VPC DNS, or Public DNS.\nAWS Network Firewall:\nUse Cases: Egress filtering (blocking bad domains, malicious protocols), Environment segmentation (VPC to VPC), and Intrusion prevention (IDS/IPS rules).\nActive Defense: Can automatically block malicious traffic using Amazon Threat Intelligence, where GuardDuty findings are marked for automated blocking.\nData Protection \u0026amp; Governance Encryption (KMS): Data is encrypted using a Data Key, which is protected by a Master Key (CMK). KMS policies enforce the second secure layer with Condition keys to define When encryption/decryption is allowed.\nCertificate Management (ACM): Provides free public certificates and automatically renews certificates 60 days before expiration. DNS Validation is the recommended validation method.\nSecrets Manager: Solves the problem of hardcoded credentials. It uses a 4-step Lambda logic (createSecret, setSecret, testSecret, finishSecret) for automatic credential rotation without downtime.\nAPI Service Security (S3 \u0026amp; DynamoDB): S3 requires TLS 1.2+ and bucket policies with aws:SecureTransport for enforcement. DynamoDB is secure by default with mandatory HTTPS.\nDatabase Service Security (RDS): Requires client-side trust in the AWS Root CA Bundle to verify server identity, and server-side enforcement (e.g., rds.force_ssl=1 for PostgreSQL).\nIncident Response \u0026amp; Prevention Prevention Best Practices: Key steps include using temporary credentials, never exposing S3 buckets directly, placing sensitive services behind private subnets, managing everything through Infrastructure as Code, and using double-gate high-risk changes (PR approval, pipeline deployment).\nIncident Response Process: A structured 5-step approach: Preparation, Detection \u0026amp; Analysis, Containment (isolate, revoke credentials), Eradication \u0026amp; Recovery, and Post-Incident (lessons learned).\nEvent Experience The event is extremely useful for our team, aligns directly with our project of Automated Incident Response and Forensics\nQ: Our team\u0026rsquo;s project is an Automated Incident Response and Forensics tool with Guard Duty being the main focus of our incident response, but from our testing we can see that Guard Duty can take up to 5 minutes to generate a finding when an incident occured, we want to ask are there any solutions to reduce this latency?\nA: Guard Duty takes 5 minutes to generate findings is just something you have to accept, as its the way Guard Duty is configured to work: Guard Duty have to go through a large amount of security data set to determine the exact threat and then generate a finding. If you want to reduce latency however, one of the ways you solve it is with 3rd party security services integration such as: Open Clarity Free for almost realtime findings and also you can detech anomalies and unsual user behavior with CloudTrail\nMr.Mendel Grabski was very keen to offer his support when we asked about our project after the event\nSome event photos Group Picture With Speaker Mendel Grabski and Speaker Van Hoang Kha\n"},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Complete Module 6 Started on proposal Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Module 6: Database Concept review:\n+ Database + Session + Primary/Foreign Key + Index + Partitions + Execution/Query Plan + Log \u0026amp; Buffer + RDBMS (Relational Database Manangement System) + NOSQL + OLTP(Online Transaction Processing): For payments, transactions + OLAP (Online Analytical Processing): Analyze data, predict trends and patterns - AWS RDS (Relational Database Serive): Include Aurora, MySQL, Postgres SQL , MSSQL, Oracle , Maria + Automatic backup + Generate read replica +Read replica can be turned into primary code + Auto Fail Over/Multi AZ (Backups on mutiple AZs) + Commonly used for OLTP + Encrypt data while at rest/in transit + Protected my Security Group and NACL + Can change instance size + Storage Auto SCaling - Amazon Aurora: Optimized underlying storage infrastructure, uses MySQL and PostgreSQL + Back track: revert to previous state + Clone + Global Database (Multi Region) + Multi Master: Many Master Databases - Amazon Redshift: Data warehouse service: PostgreSQL core, optimized for OLAP + Uses MMP Database: data is partitioned and saved at computer nodes, a Leader node is used to coordinate and compile queries + Stores data in a columnar storage format, useful for OLAP applications + Uses SQL and drivers like JDBC and ODBC + Provide cost effective services (Transient Cluster/ Redshift spectrum) - Amazon ElastiCache: Creates Cluster Caching Engines (Redis/Memcached) + Detects and replaces failed nodes + Put before CSDL layer in order to cache data + Recommended to use Redis for new workloads + Using ElastiCache requires caching logic on applications, not recommended to use default system caching - Formulated a proposal for workshop with teammates - School subject: + KS57: Completed Quản trị dữ liệu và an toàn thông tin 29/09/2025 29/09/2025 Quản trị dữ liệu và an toàn thông tin 3 - Lab 43: Guide is broken, the link doesnt go anywhere, going by video\n+ Downloaded Schema Conversion Tool + Downloaded MSSQL in EC2 Instance + No SQL script was given, trying with custom basic MSSQL Database + No CloudFormation Stack was given, skipping Oracle Database connection + Installed MySQL on EC2 Instance + Migrated custom MSSQL Database to MySQL Database using AWS Schema Conversion Tool + Created custom RDS to test migration task + Attempted to migrate from local machine to RDS + Tried to use AWS Replication Agent: Unsuccessful due to it being made for Window/Linux server only, not OS + Tried to portforward PC to be used as an endpoint + Failed portforwarding, not allowed by ISP 30/09/2025 30/09/2025 Lab 43 Application Mirgation Service Guide 4 - Found out AWS account\u0026rsquo;s credits are all expired from doing lab 12\n- Wrote a support case - Stopping labs for now - Focus on researching about team\u0026rsquo;s proposal 01/10/2025 01/10/2025\n- School subject: + ENW439c: Completed Research Methodologies Research Methodologies 5 - Continued doing labs by aquiring help from team member: Created an IAM User with admin privilege for me to log in and use their account\n- Translate first blog 02/10/2025 02/10/2025 Blog 1 6 - Joined the AI-Driven Development Life Cycle: Reimagining Software Engineering event\n- Translated second and third blog 03/10/2025 04/10/2025 Blog 2 Blog 3 Week 4 Achievements: Completed a comprehensive review of core database concepts including RDBMS, keys, indexes, partitioning, OLTP/OLAP, and AWS-specific database services.\nGained theoretical knowledge of the features and use cases for AWS RDS, Amazon Aurora (e.g., Backtrack, Global Database), Amazon Redshift (Data Warehouse for OLAP), and Amazon ElastiCache (caching with Redis/Memcached).\nDatabase Migration: Attempted a complex database migration lab, demonstrating resourcefulness by:\nSourcing a custom MSSQL Database and installing necessary services on an EC2 instance due to broken lab guides. Successfully migrating the custom MSSQL database to MySQL using the AWS Schema Conversion Tool (SCT). Identified and addressed the issue of expired AWS credits by raising a support case.\nSecured continuation of lab work by setting up an IAM User with admin privileges on a team member\u0026rsquo;s account.\nFormulated a proposal for the team\u0026rsquo;s upcoming workshop with teammates.\nCompleted the translation of three blogs.\nAttended the AI-Driven Development Life Cycle: Reimagining Software Engineering event.\n"},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.7-dashboard-setup/5.7.4-setup-cloudfront/","title":"Cloudfront Setup","tags":[],"description":"","content":"In this guide, you will setup a Cloudfront for cache, routing and web accessing.\nCreate Cloudfront Distribution Open the Cloudfront Console\nNavigate to https://console.aws.amazon.com/cloudfront/ Or: AWS Management Console → Services → Cloudfront Create Distribution:\nClick the Create distribution button In distribution creation, use this setting: Choose a plan: Free plan Name: Static Dashboard Website CloudFront Origin type: Amazom S3 S3 Origin: Choose the static-dashboard-bucket Keep the rest like default Enable security: Use this if you choose free plan Review and click Create distribution General setting:\nAfter creation complete, on your Cloudfront General tab click on Edit At the Default root object enter index.html Description: Static Dashboard Distribution Click Save change Create API Gateway origin:\nClick Origins on the menu tabs Then click Create origin In orogin creation, use this setting: Origin domain: choose dashboard-api Protocol: HTTPS only HTTPS port: 443 Minimum Origin SSL protocol: TLSv1.2 Origin path: /prod Click Create origin Create behaviors for API Gateway:\nClick Behaviors on the menu tabs Then click Create behavior In behavior creation, use this setting: Path pattern: /logs/* Origin and origin groups: choose dashboard-api Leave the rest setting like default Click Create behavior Update S3 policy to work with Cloudfront:\nClick Origins on the menu tabs, choose the s3-static-dashboard origin name Click Edit At Origin access controll section press Go to S3 bucket permissions Check if your S3 permission look like this, if don\u0026rsquo;t then copy and paste it to your S3 permission (Change the ACCOUNT_ID, ACCOUNT_REGION and CLOUDFRONT_ID to your): { \u0026#34;Version\u0026#34;: \u0026#34;2008-10-17\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;PolicyForCloudFrontPrivateContent\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCloudFrontServicePrincipal\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudfront.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::s3-static-dashboard-[ACCOUNT_ID]-[ACCOUNT_REGION]/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;ArnLike\u0026#34;: { \u0026#34;AWS:SourceArn\u0026#34;: \u0026#34;arn:aws:cloudfront::[ACCOUNT_ID]:distribution/[CLOUDFRONT_ID]\u0026#34; } } } ] } Click Save change Create error pages:\nClick Error pages on the menu tabs Click Create custom error page In custom error page creation, use this setting: HTTP error code: 403: Forbident Error caching minimum TTL: 300 Customize error response: Yes Response page path: /index.html HTTP Response code: 200: OK Repeat this for 404 code "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.11-appendices/5.11.4-cloudwatch-eni-etl/","title":"CloudWatch ENI ETL Code","tags":[],"description":"","content":" import json import boto3 import gzip import os from datetime import datetime s3 = boto3.client(\u0026#34;s3\u0026#34;) firehose = boto3.client(\u0026#34;firehose\u0026#34;) # -------------------------------------------------- # CONFIGURATION # -------------------------------------------------- FIREHOSE_STREAM_NAME = os.environ.get(\u0026#34;FIREHOSE_STREAM_NAME\u0026#34;) # ----------------------------- UTILS ----------------------------- def read_gz(bucket, key): obj = s3.get_object(Bucket=bucket, Key=key) with gzip.GzipFile(fileobj=obj[\u0026#34;Body\u0026#34;]) as f: return f.read().decode(\u0026#34;utf-8\u0026#34;, errors=\u0026#34;replace\u0026#34;) def safe_int(x): try: return int(x) except: return None def parse_flow_log_line(line): parts = line.strip().split(\u0026#39; \u0026#39;) if len(parts) \u0026lt; 14: return None try: start_timestamp = safe_int(parts[10]) time_str = None if start_timestamp: dt_object = datetime.fromtimestamp(start_timestamp) time_str = dt_object.strftime(\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;) record = { \u0026#34;version\u0026#34;: safe_int(parts[0]), # Cột 1: version (int) \u0026#34;account_id\u0026#34;: parts[1], # Cột 2: account_id (STRING) \u0026#34;interface_id\u0026#34;: parts[2], # Cột 3: eni-... \u0026#34;srcaddr\u0026#34;: parts[3], \u0026#34;dstaddr\u0026#34;: parts[4], \u0026#34;srcport\u0026#34;: safe_int(parts[5]), \u0026#34;dstport\u0026#34;: safe_int(parts[6]), \u0026#34;protocol\u0026#34;: safe_int(parts[7]), \u0026#34;packets\u0026#34;: safe_int(parts[8]), \u0026#34;bytes\u0026#34;: safe_int(parts[9]), \u0026#34;start_time\u0026#34;: start_timestamp, # Cột 11 \u0026#34;end_time\u0026#34;: safe_int(parts[11]), \u0026#34;action\u0026#34;: parts[12], \u0026#34;log_status\u0026#34;: parts[13], \u0026#34;timestamp_str\u0026#34;: time_str } return record except Exception as e: print(f\u0026#34;Error parsing line: {e}\u0026#34;) return None def lambda_handler(event, context): print(f\u0026#34;Received S3 Event. Records: {len(event.get(\u0026#39;Records\u0026#39;, []))}\u0026#34;) firehose_records = [] # Duyệt qua các file S3 gửi về for record in event.get(\u0026#34;Records\u0026#34;, []): if \u0026#34;s3\u0026#34; not in record: continue bucket = record[\u0026#34;s3\u0026#34;][\u0026#34;bucket\u0026#34;][\u0026#34;name\u0026#34;] key = record[\u0026#34;s3\u0026#34;][\u0026#34;object\u0026#34;][\u0026#34;key\u0026#34;] # Chỉ xử lý file .gz if not key.endswith(\u0026#34;.gz\u0026#34;): print(f\u0026#34;Skipping non-gz: {key}\u0026#34;) continue print(f\u0026#34;Processing: {key}\u0026#34;) # Đọc nội dung content = read_gz(bucket, key) if not content: continue # Parse từng dòng log for line in content.splitlines(): rec = parse_flow_log_line(line) if not rec: continue # Chuyển thành JSON string và thêm xuống dòng (\\n) json_row = json.dumps(rec) + \u0026#34;\\n\u0026#34; firehose_records.append({\u0026#39;Data\u0026#39;: json_row}) # Đẩy sang Firehose (Batching 500 dòng) if firehose_records: total = len(firehose_records) print(f\u0026#34;Flushing {total} records to Firehose...\u0026#34;) batch_size = 500 for i in range(0, total, batch_size): batch = firehose_records[i:i + batch_size] try: response = firehose.put_record_batch( DeliveryStreamName=FIREHOSE_STREAM_NAME, Records=batch ) if response[\u0026#39;FailedPutCount\u0026#39;] \u0026gt; 0: print(f\u0026#34;Warning: {response[\u0026#39;FailedPutCount\u0026#39;]} records failed.\u0026#34;) except Exception as e: print(f\u0026#34;Firehose API Error: {e}\u0026#34;) return {\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;, \u0026#34;count\u0026#34;: len(firehose_records)} "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in four events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AI-Driven Development Life Cycle: Reimagining Software Engineering\nDate \u0026amp; Time: 09:00, October 3rd, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS\nDate \u0026amp; Time: 08:30, November 15th, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AWS Cloud Mastery Series #2 - DevOps on AWS\nDate \u0026amp; Time: 08:30, November 17th, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 4 Event Name: AWS Cloud Mastery Series #3: AWS Well-Architected – Security Pillar Workshop\nDate \u0026amp; Time: 08:30, November 29th, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.4-monitoring-setup/","title":"Monitoring Setup","tags":[],"description":"","content":"This Monitoring Setup phase activates and configures the three core log sources for threat detection. It involves enabling CloudTrail for comprehensive management and data events, activating GuardDuty to export security findings to the primary S3 bucket, and setting up VPC Flow Logs on your network to send all traffic metadata to the dedicated CloudWatch Log Group. This ensures a constant, centralized stream of log data is available for processing and automated response.\nCreate CloudWatch Log Group Open CloudWatch Console → Log Management → Create log group Configure:\nLog group name: /aws/incident-response/centralized-logs Retention: 90 days KMS key: None Click \u0026ldquo;Create\u0026rdquo;\nEnable AWS CloudTrail Open CloudTrail Console → Trail → Create trail Trail attributes:\nTrail name: incident-responses-cloudtrail-ACCOUNT_ID-REGION Storage location: Use existing S3 bucket S3 bucket: Choose your incident-response-log-list-bucket-ACCOUNT_ID-REGION Log file SSE-KMS encryption: Disable Log file validation: Enabled Click next Choose log events:\nEvents Choose Management events, Data events Management events: All (Read + Write) Data events: S3 - Log all events Click next till step 4 and Create Trail Advanced event selectors: Exlcude log buckets:\nClick the Trail then scroll down to Data Event and click Edit Setup like picture with the under format: -arn:aws:s3:::incident-response-log-list-bucket-ACCOUNT_ID-REGION/\n-arn:aws:s3:::processed-guardduty-findings-ACCOUNT_ID-REGION/\n-arn:aws:s3:::processed-cloudtrail-logs-ACCOUNT_ID-REGION\n-arn:aws:s3:::athena-query-results-ACCOUNT_ID-REGION/\n-arn:aws:s3:::processed-cloudwatch-logs-ACCOUNT_ID-REGION/\nSave change Enable Amazon GuardDuty Open GuardDuty Console → Get Started → Enable GuardDuty\nConfigure settings:\nFinding export frequency: Update CWE and S3 every 15 minutes S3 export: incident-response-log-list-bucket-ACCOUNT_ID-REGION KMS encryption: Choose or create KMS key Enable VPC Flow Logs Open VPC Console → Your VPCs → Select your VPC\nActions → Create flow log\nConfigure:\nFilter: All Aggregation interval: 10 minutes Destination: CloudWatch Logs Log group: /aws/incident-response/centralized-logs IAM role: FlowLogsIAMRole Log format: Default Create flow log\nEnable VPC DNS Query Logging Configure Resolver Query Logging Open the Amazon Route 53 Console.\nIn the left navigation pane, select VPC Resolver -\u0026gt; Query logging.\nClick \u0026ldquo;Configure query logging\u0026rdquo;.\nConfigure:\nName: Enter a descriptive name, e.g., IR-DNS-Query-Log-Config. Destination for query logs: CloudWatch Logs log group Log group: Select \u0026ldquo;Existing log group\u0026rdquo; and choose: /aws/incident-response/centralized-logs Click \u0026ldquo;Configure query logging\u0026rdquo;.\n"},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Continue building and planning proposal Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 4 - Created and research my parts based on team\u0026rsquo;s workshop architecture diagram 08/10/2025 08/10/2025 5 - Lab 35:\n+ Succesfully setted up data stream using Kinesis + Successfully sent sample data to the S3 using Kinesis Data Generator with Amazon Cognito + Learnt how to use AWS Glue Crawler to map data to S3 Bucket + Used Athena to query data + Used AWS Glue Notebook to build dataset based on sample data + Used Athena to analyze data and visualized with QuickSight - Updated the architecure diagram based on changes in the workshop proposal - Started researching Lambda argorithms for team\u0026rsquo;s project. 09/10/2025 09/10/2025 Lab 35 6 - Lab 40:\n+ Praticed more with AWS Glue and Athena, used it to analyze AWS Monthy Cost data. 10/10/2025 10/10/2025 Lab 40 Week 5 Achievements: Workshop Architecture \u0026amp; Research:\nCreated and researched specific components for the team\u0026rsquo;s workshop architecture. Updated the architecture diagram to reflect the latest proposal changes. Initiated research on AWS Lambda algorithms to be implemented in the team\u0026rsquo;s project. Data Streaming and Analytics (Lab 35): Completed a complex data pipeline lab:\nSuccessfully set up real-time data streaming using Amazon Kinesis. Generated and sent sample data to S3 using the Kinesis Data Generator and Amazon Cognito. Utilized AWS Glue Crawlers to map data and Glue Notebooks to build datasets. Performed data querying with Amazon Athena and visualization with Amazon QuickSight. Cost Analysis (Lab 40): Practiced advanced analytics by using AWS Glue and Athena to analyze AWS Monthly Cost data.\n"},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.11-appendices/5.11.5-cloudwatch-autoexport/","title":"CloudWatch Autoexport Code","tags":[],"description":"","content":" import json import base64 import gzip from io import BytesIO import boto3 import os import time s3 = boto3.client(\u0026#39;s3\u0026#39;) # --- CONFIGURATION --- RAW_S3_BUCKET = os.environ.get(\u0026#34;DESTINATION_BUCKET\u0026#34;) # The log group pattern constant is no longer used for filtering, but is kept for reference. # VPC_DNS_LOG_PATTERN = \u0026#39;/aws/route53/query/\u0026#39; def is_vpc_dns_log(log_message): try: json_body = json.loads(log_message.strip()) if \u0026#39;query_name\u0026#39; in json_body and \u0026#39;query_type\u0026#39; in json_body: return True return False except Exception: return False def lambda_handler(event, context): try: compressed_payload = base64.b64decode(event[\u0026#39;awslogs\u0026#39;][\u0026#39;data\u0026#39;]) f = BytesIO(compressed_payload) decompressed_data = gzip.GzipFile(fileobj=f).read() log_data = json.loads(decompressed_data.decode(\u0026#39;utf-8\u0026#39;)) log_lines = [] for log_event in log_data.get(\u0026#39;logEvents\u0026#39;, []): log_lines.append(log_event.get(\u0026#39;message\u0026#39;, \u0026#39;\u0026#39;)) if not log_lines: print(f\u0026#34;Batch skipped: No log events found in payload. Log Group: {log_data.get(\u0026#39;logGroup\u0026#39;)}\u0026#34;) return {\u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: \u0026#39;Log batch ignored (No events).\u0026#39;} is_dns_log = is_vpc_dns_log(log_lines[0]) if is_dns_log: key_prefix = \u0026#39;vpc-dns-logs\u0026#39; filename_prefix = \u0026#39;vpc-\u0026#39; # Add vpc- to the filename else: key_prefix = \u0026#39;vpc-flow-logs\u0026#39; filename_prefix = \u0026#39;eni-\u0026#39; # Keep filename blank for other logs output_content = \u0026#39;\\n\u0026#39;.join(log_lines) full_log_group_name = log_data.get(\u0026#39;logGroup\u0026#39;, \u0026#39;unknown-group\u0026#39;) log_group_name_safe = full_log_group_name.strip(\u0026#39;/\u0026#39;).replace(\u0026#39;/\u0026#39;, \u0026#39;_\u0026#39;) final_filename = f\u0026#34;{filename_prefix}{context.aws_request_id}.gz\u0026#34; s3_key = f\u0026#39;exportedlogs/{key_prefix}/{log_group_name_safe}/{final_filename}\u0026#39; buffer = BytesIO() with gzip.GzipFile(fileobj=buffer, mode=\u0026#39;w\u0026#39;) as gz: gz.write(output_content.encode(\u0026#39;utf-8\u0026#39;)) gzipped_data = buffer.getvalue() s3.put_object( Bucket=RAW_S3_BUCKET, Key=s3_key, Body=gzipped_data, ContentType=\u0026#39;application/x-gzip\u0026#39; ) num_logs = len(log_lines) print(f\u0026#34;Exported {num_logs} raw log lines to s3://{RAW_S3_BUCKET}/{s3_key}\u0026#34;) return {\u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: f\u0026#39;Logs exported. {num_logs} events processed. Key Prefix: {key_prefix}\u0026#39;} except Exception as e: print(f\u0026#34;Error in CW Export Lambda: {e}\u0026#34;) raise e "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.7-dashboard-setup/5.7.5-setup-cognito/","title":"Cognito Setup","tags":[],"description":"","content":"In this guide, you will create a Cognito user pool for dashboard login.\nCreate Cognito User Pool Open the Amazon Cognito Console\nNavigate to https://console.aws.amazon.com/cognito/ Or: AWS Management Console → Services → Cognito Create user pool:\nClick Create user pool In user pool creation, use this setting: Application type: Single-page application (SPA) Application name: dashboard-user-pool-client Options for sign-in identifiers: Email and Username Self-registration: Enable self-registration Required attributes for sign-up: email Add a return URL: Go to Cloudfront, choose the one that you just created and copy the Distribution domain name and paste it here (Example: https://d2bvvvpr6s4eyd.cloudfront.net) Click Create user directory After create, scroll down and click Go to overview User pool App clients configuration:\nSelect App clients on the left menu panel Choose dashboard-user-pool-client In App client information section, click Edit Change the setting like the image below: Click Save change Managed login pages configuration:\nIn Managed login pages configuration section, click Edit Click Add sign-out URL at Allowed sign-out URLs section Copy the URL on the callbacks URL and paste to Allowed sign-out URLs Scroll down to OpenID Connect scopes add Profile to the scopes Click Save change Create a user:\nOn the left menu panel, select User option Click Create user Enter your user information Click Create user "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.5-processing-setup/","title":"Processing Setup","tags":[],"description":"","content":"This Processing Setup phase establishes the core data pipeline for structuring raw logs and preparing them for queryable analysis. It mandates the deployment of three Kinesis Data Firehose streams for buffering and delivering CloudTrail and VPC logs to target S3 buckets. Concurrently, you will configure the AWS Glue Database and four Athena tables via DDL to make the structured data queryable. This pipeline relies on five ETL Lambda functions triggered by S3 Event Notifications to perform the necessary data transformation upon log arrival.\nContent Create Kinesis Data Firehose Delivery Streams Create AWS Glue Database and Tables Create Lambda Functions - ETL Processing "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/","title":"Workshop","tags":[],"description":"","content":"AWS Auto Incident Response System Setup Overview This guide provides a complete, step-by-step procedure for deploying our automated incident response and forensic system in AWS. This system leverages CloudTrail, GuardDuty, VPC Flow Logs, Kinesis Firehose, Glue, Athena, and Lambda functions orchestrated by AWS Step Functions to automatically detect, analyze, and quarantine compromised resources like EC2 instances and IAM users. Futher log forensics capacity is added by setting up a Security Dashboard hosted on S3 and accessed via CloudFront and Cognito, query log using API Gateway and Lambda.\nContent Overview Prerequisites Phase 1: Foundation Setup Phase 2: Monitoring Setup Phase 3: Processing Setup Phase 4: Automation Setup Phase 5: Dashboard Setup Verify Use CDK Cleanup Appendices "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/6-self-evaluation/","title":"Self-Evaluations","tags":[],"description":"","content":"During the internship at Amazon Web Services from 9/2025 to 12/2025, I had the opportunity to learn, practice, and apply the knowledge acquired at school in a real working environment. I participated in Autonomous Incident Response Development with tasks involving EventBridge (Trigger), AWS Lambda Functions, and AWS Step Functions (Core Logic).\nSummary Report of Skills \u0026amp; Implemented Items During the project implementation process, I performed the following items:\nOrchestration with AWS Step Functions:\nEstablished direct AWS SDK integration (Service Integration) to call EC2 and Auto Scaling APIs, eliminating dependency on intermediate Lambdas. Handled Data Flow between states: used ResultPath: null to preserve data and States.Array to handle dynamic arrays. Configured complex navigation logic: branching (Choice), looping (Map), and automatic retry mechanisms (Retry). Auto Scaling Group (ASG) Operations:\nExecuted the process of detaching instances from ASG using the parameter ShouldDecrementDesiredCapacity to maintain application operations. Handled Capacity Constraints, specifically fixing the MinSize logic error by adding the UpdateASGConfiguration step. Debugging \u0026amp; Troubleshooting:\nAnalyzed root causes (Root Cause Analysis) by checking the Input/Output of each Step. Performed Isolation Testing for each logic branch (ASG and Non-ASG) and used mock data (Mock events). Identified and handled logic errors that did not report system errors (Silent Failure) and environmental errors (wrong Region, ID format). Security \u0026amp; Permission Management (IAM):\nApplied the Least Privilege principle: Granted limited permissions via Inline Policy for specific actions (such as DetachInstances, CreateSnapshot). Deployed the technical Incident Response process in sequence: Detect $\\rightarrow$ Isolate (Security Group) $\\rightarrow$ Protect (Termination Protection) $\\rightarrow$ Collect Evidence (Snapshot). Automation:\nBuilt an automation system to replace manual operations. Designed workflows ensuring Idempotency (allowing multiple re-runs without causing duplicate errors) and flexible handling capabilities for both instances within and outside ASG. Regarding demeanor, I always strive to complete tasks well, adhere to regulations, and actively exchange with colleagues to improve work efficiency.\nTo objectively reflect on the internship process, I would like to self-assess based on the criteria below:\nNo. Criteria Description Good Fair Average 1 Professional knowledge and skills Understanding of the industry, applying knowledge to reality, tool usage skills, work quality ☐ ✅ ☐ 2 Learning ability Absorbing new knowledge, learning quickly ☐ ✅ ☐ 3 Proactivity Self-researching, accepting tasks without waiting for instructions ☐ ✅ ☐ 4 Sense of responsibility Completing work on time, ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedule, regulations, work processes ☐ ✅ ☐ 6 Progressiveness Ready to receive feedback and improve oneself ✅ ✅ ☐ 7 Communication Presenting ideas, reporting work clearly ☐ ☐ ☐ 8 Teamwork Working effectively with colleagues, participating in the team ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, working environment ✅ ☐ ☐ 10 Problem-solving mindset Identifying problems, proposing solutions, creativity ✅ ☐ ☐ 11 Contribution to project/organization Work efficiency, improvement initiatives, recognition from team ✅ ☐ ☐ 12 Overall General assessment of the entire internship process ✅ ☐ ☐ Needs Improvement Enhance discipline, strictly adhere to the regulations of the company or any organization. Improve problem-solving mindset. Learn to communicate better in daily interactions and at work, and in handling situations. "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Complete and submit proposal Assign tasks with team member to get started on the workshop Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Reformmatted and refined the worklog, adding information and summaries\n- Succesfully deployed worklog to Github Page 13/10/2025 13/10/2025 3 - Team meeting\n- Revised workshop proposal: Focused on using Guard Duty for intrusion dectection instead of a custom Lambda function due to the need for a large dataset and extensive development time. - Redrew AWS Architecture: Added Guard Duty replacing CloudWatch Alarm - Wrote a draft of the proposal with outlining basic function and providing a rough cost estimate. 14/10/2025 14/10/2025 4 - Team meeting\n- Revised workshop proposal: + Incorporated the use of EventBridge + Recalculated costs by reducing the EC2 instance type and active hours - Updated AWS Architecture: Include the EventBridge icon and connections 15/10/2025 15/10/2025 5 - Updated AWS Arhitecture:\n+ Rearranged icons for clearer connections. + Moved SSM inside of region group + Added public subnet group for EC2 Instance - Installed AmazonQ for enhanced proposal analytics - Revised workshop proposal: Recalculated cost using AWS Pricing Calculator - Translated proposal draft into markdown code and successfully deployed it to Github Pages - Joined the online seminar 𝗗𝗫\u0026lt;𝗶𝗻𝗔𝗰𝘁𝗶𝗼𝗻\u0026gt; 𝗧𝗮𝗹𝗸#𝟳: Reinventing DevSecOps with AWS Generative AI 16/10/2025 16/10/2025 6 - Compiled study materials for midterm exam\n17/10/2025 17/10/2025 Week 6 Achievements: Proposal Refinement:\nCompleted multiple revisions of the workshop proposal, shift from a custom Lambda function to using GuardDuty for intrusion detection. Successfully recalculated and reduced estimated costs by optimizing the EC2 instance type and active hours. Translated to markdown, and deployed the proposal draft to GitHub Pages. Architecture and System Updates:\nRevised the AWS Architecture diagram, incorporating GuardDuty, EventBridge, and refining icon arrangements and subnet groups for clarity and accuracy. Updated the worklog and successfully deployed the refined worklog to GitHub Pages. Installed AmazonQ. Attended the online seminar \u0026lsquo;DX Talk#7: Reinventing DevSecOps with AWS Generative AI\u0026rsquo;.\nCompiled study materials for the midterm exam.\n"},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.6-automation-setup/","title":"Automation Setup","tags":[],"description":"","content":"Phase 4: Automation Setup Create Isolation Security Group EC2 Console → Security Groups → Create security group Name: IR-Isolation-SG Description: Denies all inbound and outbound traffic for compromised instances VPC: Select your VPC Inbound rules: None (deny all) Outbound rules: Remove default (deny all) Create and note Security Group ID (e.g., sg-0078026b70389e7b3) Create SNS Topic SNS Console → Create topic Type: Standard, Name: IncidentResponseAlerts Access policy: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;events.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sns:Publish\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:sns:ap-southeast-1:831981618496:IncidentResponseAlerts\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;AWSEvents_IncidentResponseAlert_Target0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;events.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;SNS:Publish\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:sns:ap-southeast-1:831981618496:IncidentResponseAlerts\u0026#34; } ] } Create Lambda Functions - Incident Response ir-parse-findings-lambda Handler: parse_findings.lambda_handler Role: ParseFindingsLambdaServiceRole Code: parse-findings ir-isolate-ec2-lambda Handler: isolate_ec2.lambda_handler Role: IsolateEC2LambdaServiceRole Env: ISOLATION_SG_ID=sg-XXXXXXX (from step 12) Code: isolate-ec2 ir-quarantine-iam-lambda Handler: quarantine_iam.lambda_handler Role: QuarantineIAMLambdaServiceRole Env: QUARANTINE_POLICY_ARN=arn:aws:iam::ACCOUNT_ID:policy/IrQuarantineIAMPolicy Code: quarantine-iam ir-alert-dispatch Handler: alert_dispatch.lambda_handler Role: AlertDispatchLambdaServiceRole Env: SENDER_EMAIL, RECIPIENT_EMAIL, SLACK_WEBHOOK_URL Add SNS trigger: Topic IncidentResponseAlerts Code: alert-dispatch Update SNS Topic Subscription SNS Console → IncidentResponseAlerts → Subscriptions Verify: Protocol=AWS Lambda, Endpoint=ir-alert-dispatch, Status=Confirmed Create Step Functions State Machine Step Functions Console → Create state machine Type: Standard, Name: IncidentResponseStepFunctions Definition: Step Functions Definition Role: StepFunctionsRole Create Create EventBridge Rule EventBridge Console → Rules → Create rule Name: IncidentResponseAlert Event pattern: { \u0026#34;source\u0026#34;: [\u0026#34;aws.guardduty\u0026#34;], \u0026#34;detail-type\u0026#34;: [\u0026#34;GuardDuty Finding\u0026#34;] } Targets (2): SNS topic: IncidentResponseAlerts Step Functions: IncidentResponseStepFunctions with role IncidentResponseStepFunctionsEventRole Configure Athena Workgroup Athena Console → Workgroups → primary → Edit Query result location: s3://athena-query-results-ACCOUNT_ID-REGION/ Save "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.11-appendices/5.11.6-parse-findings/","title":"Parse Findings Code","tags":[],"description":"","content":" import json import logging logger = logging.getLogger() logger.setLevel(logging.INFO) def lambda_handler(event, context): instance_ids = [] detail = event.get(\u0026#39;detail\u0026#39;, {}) region = event.get(\u0026#39;region\u0026#39;) or detail.get(\u0026#39;region\u0026#39;) or \u0026#39;ap-southeast-1\u0026#39; instance_id_primary = detail.get(\u0026#39;resource\u0026#39;, {}).get(\u0026#39;instanceDetails\u0026#39;, {}).get(\u0026#39;instanceId\u0026#39;) if instance_id_primary: instance_ids.append(instance_id_primary) # --- 2. Extract from the older/secondary \u0026#39;resources\u0026#39; array structure --- for r in detail.get(\u0026#34;resources\u0026#34;, []): if r.get(\u0026#34;type\u0026#34;) == \u0026#34;AwsEc2Instance\u0026#34;: id_from_details = r.get(\u0026#39;details\u0026#39;, {}).get(\u0026#39;instanceId\u0026#39;) if id_from_details: instance_ids.append(id_from_details) else: arn_id = r.get(\u0026#39;id\u0026#39;) if arn_id and arn_id.startswith(\u0026#39;arn:aws:ec2:\u0026#39;): instance_ids.append(arn_id.split(\u0026#39;/\u0026#39;)[-1]) unique_instance_ids = list(set([id for id in instance_ids if id])) return { \u0026#34;InstanceIds\u0026#34;: unique_instance_ids, \u0026#34;Region\u0026#34;: region } "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Design and prototype automated incident response workflows and refine the project architecture based on mentor feedback. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 3 - Researching and testing algorithms for ec2 isolation by using SG with no outbound inbound rule 20/10/2025 21/10/2025 4 - Team meeting:\n+ Quick AWS Services knowledge revision + Conversed about changes in the proposal - Updated AWS Architecture: Added AWS Detective - Revised proposal: + Added the usage of AWS Detective + Added plan for CDK after finishing the workshop - Mentor recommendations: + Visualize data but without using Quicksight, instead make a custom-coded dashboard (Researching) + Save GuardDuty findings in S3 bucket for analyzing (Researching) - Successfully configured EventBridge to trigger upon specific GuardDuty findings and: + Sent SNS emails to all of team members + Triggered a simple Lambda script - Formulated an idea to add to workshop: Make a simple data graphing page hosted in S3 and use API Gateway and Lambda to pull forensics data from Amazon Athena (Researching) 22/10/2025 22/10/2025 5 - Tried out AWS Card Clash with team members: Surprisingly good for learning services and their functions, their placement in Architectures\n- Reviewed AWS Services Knowledge for Mid-Term: Using Google Gemini to generate quizzes based on the given requirements 23/10/2025 23/10/2025 AWS Card Clash 6 - Successfully configured GuardDuty threat list to trigger findings from EC2 Instance activities 08/15/2025 08/15/2025 Pháp luật và đạo đức trong công nghệ số Week 7 Achievements: GuardDuty Hands-on Practice:\nCompleted the \u0026ldquo;Getting Hands on with Amazon GuardDuty - AWS Virtual Workshop\u0026rdquo; and an in-depth lab generated with Amazon Q. Successfully created, tested, and triggered various GuardDuty findings through console settings, EC2 activity, and CloudTrail API access. Established an easier testing environment by successfully triggering sample alerts with different severities and types via CloudShell CLI. Successfully configured a GuardDuty threat list to trigger findings from EC2 Instance activities. Workshop Proposal and Architecture Advancement:\nUpdated the proposal and AWS Architecture to incorporate AWS Detective for further investigation capabilities. Added a plan for CDK implementation following the completion of the workshop. Initiated research into mentor recommendations, including custom-coded data visualization and saving GuardDuty findings to S3 for analysis. Formulated a new workshop idea for a simple data graphing page hosted in S3 using API Gateway and Lambda. Service Integration and Automation:\nSuccessfully configured EventBridge to act upon specific GuardDuty findings. Automated notifications by sending SNS emails to team members and triggering a Lambda script based on GuardDuty alerts. Network Security Research \u0026amp; Isolation:\nResearched and validated algorithms for EC2 isolation by testing Security Groups (SG) with restricted inbound/outbound rules to effectively contain compromised instances. "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Lê Trần Gia Huy\nPhone Number: 0822746264\nEmail: huyletran188205@gmail.com\nUniversity: FPT University Ho Chi Minh city\nMajor: Information Assurance\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment The working atmosphere at FCJ is remarkably welcoming and open. Members are consistently willing to offer support whenever I face technical challenges, often extending their help beyond standard working hours. The workspace is well-organized and comfortable, which significantly aids my concentration. However, I believe increasing the frequency of social gatherings or team bonding activities would be beneficial to further strengthen team cohesion.\n2. Support from Mentor / Team Admin My mentor provided exceptional guidance—offering clear explanations for complex concepts while encouraging a culture of inquiry. I particularly value the pedagogical approach where I was encouraged to attempt solving problems independently before receiving the final solution, which greatly enhanced my critical thinking. Additionally, the admin team was instrumental in handling administrative tasks and providing necessary documentation, creating favorable conditions for a seamless workflow.\n3. Relevance of Work to Academic Major The assigned tasks were highly relevant to my university curriculum, effectively bridging the gap between academic theory and practical application. Beyond reinforcing foundational knowledge, the internship introduced me to advanced domains I had not previously encountered (such as Cloud Security automation), allowing me to expand my skillset significantly.\n4. Learning \u0026amp; Skill Development Opportunities Throughout this internship, I have acquired a diverse set of skills, ranging from proficiency in project management tools to professional communication within a corporate setting. Furthermore, the real-world experiences and career advice shared by my mentor have been invaluable in helping me shape my future career path in Cloud Engineering.\n5. Company Culture \u0026amp; Team Spirit The company culture strikes a perfect balance between professionalism and approachability. Mutual respect is evident, and while work is taken seriously, the atmosphere remains enjoyable. I was particularly impressed by the collective spirit during urgent projects, where everyone, regardless of position, collaborated to support one another. This inclusivity made me feel like a true member of the team, not just an intern.\n6. Internship Policies / Benefits The internship program offers practical benefits, including a fair flexible working hours when necessary. Additionally, the opportunity to participate in internal training sessions is a significant advantage that demonstrates the company\u0026rsquo;s investment in talent development.\nAdditional Questions What did you find most satisfying during your internship? The most satisfying moment was successfully deploying the Autonomous Incident Response system using AWS Step Functions without relying on intermediate Lambda functions. Seeing the system automatically detect, isolate, and snapshot an instance in a real-world scenario gave me a profound sense of accomplishment and validated the technical skills I had learned.\nWhat do you think the company should improve for future interns? While the technical guidance is excellent, I believe having a more structured orientation regarding professional conduct and organizational discipline in the first week would be beneficial. This would help interns better adapt to the company\u0026rsquo;s internal regulations and workflow standards right from the start.\nIf recommending to a friend, would you suggest they intern here? Why or why not? I would definitely recommend interning here. This environment allows you to get your hands dirty with real cloud infrastructure rather than just observing. The mentorship is supportive but challenging enough to force you to grow, which is ideal for anyone serious about a career in DevOps or Cloud Security.\nSuggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? I suggest incorporating weekly or bi-weekly \u0026ldquo;Lightning Talk\u0026rdquo; sessions where interns can present what they learned or the bugs they fixed that week. This would improve our presentation skills and allow for cross-learning between different teams.\nWould you like to continue this program in the future? Yes, absolutely. I am very keen on continuing to contribute to the company, potentially in an advanced internship role or a fresher position, to further develop the automation IR projects I have started.\nAny other comments (free sharing): I want to express my sincere gratitude to the mentors and the FCJ team for their patience and dedication. This internship has not only improved my technical coding skills but also matured my mindset regarding system architecture and professional responsibility.\n"},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.7-dashboard-setup/","title":"Dashboard Setup","tags":[],"description":"","content":"This guide will show you how to setup the security dashboard. The security dashboard will be using S3 to contain the web files and folder, Lambda to query data using Athena, API Gateway to routing api to Lambda and Cloudfront to caching and access to the web using it\u0026rsquo;s URL.\nContent Setup S3 Setup Lambda Setup API Gateway Setup Cloudfront Setup Cognito "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.11-appendices/5.11.7-isolate-ec2/","title":"Isolate EC2 Code","tags":[],"description":"","content":" import json import boto3 import os from botocore.exceptions import ClientError ISOLATION_SG_ID = os.getenv(\u0026#39;ISOLATION_SG_ID\u0026#39;) def lambda_handler(event, context): print(\u0026#34;=== ISOLATE EVENT RECEIVED ===\u0026#34;) print(json.dumps(event, indent=2)) instance_id = event.get(\u0026#39;InstanceId\u0026#39;) region = event.get(\u0026#39;Region\u0026#39;, \u0026#39;ap-southeast-1\u0026#39;) if not instance_id or not ISOLATION_SG_ID: print(\u0026#34;[ERROR] Missing InstanceId or IsolationSGId in input. Cannot isolate.\u0026#34;) return {\u0026#34;status\u0026#34;: \u0026#34;isolation_failed\u0026#34;, \u0026#34;InstanceId\u0026#34;: instance_id, \u0026#34;error\u0026#34;: \u0026#34;Missing input data\u0026#34;} try: ec2 = boto3.client(\u0026#39;ec2\u0026#39;, region_name=region) response = ec2.describe_instances(InstanceIds=[instance_id]) instance = response[\u0026#39;Reservations\u0026#39;][0][\u0026#39;Instances\u0026#39;][0] current_sgs = [sg[\u0026#39;GroupId\u0026#39;] for sg in instance.get(\u0026#39;SecurityGroups\u0026#39;, [])] if ISOLATION_SG_ID in current_sgs: print(f\u0026#34;[INFO] {instance_id} already has isolation SG {ISOLATION_SG_ID}\u0026#34;) return { **event, \u0026#34;status\u0026#34;: \u0026#34;already_isolated\u0026#34;, \u0026#34;InstanceId\u0026#34;: instance_id, \u0026#34;Region\u0026#34;: region, \u0026#34;IsolationSG\u0026#34;: None } print(f\u0026#34;[ACTION] Isolating {instance_id} in {region} with SG {ISOLATION_SG_ID}\u0026#34;) ec2.modify_instance_attribute( InstanceId=instance_id, Groups=[ISOLATION_SG_ID] ) print(f\u0026#34;[SUCCESS] {instance_id} isolated with SG {ISOLATION_SG_ID}\u0026#34;) return { **event, \u0026#34;status\u0026#34;: \u0026#34;isolation_complete\u0026#34;, \u0026#34;InstanceId\u0026#34;: instance_id, \u0026#34;Region\u0026#34;: region, \u0026#34;IsolationSG\u0026#34;: ISOLATION_SG_ID } except ClientError as e: error_code = e.response.get(\u0026#39;Error\u0026#39;, {}).get(\u0026#39;Code\u0026#39;) print(f\u0026#34;[ERROR] Isolation FAILED for {instance_id} ({error_code}): {str(e)}\u0026#34;) return { \u0026#34;status\u0026#34;: \u0026#34;isolation_failed\u0026#34;, \u0026#34;InstanceId\u0026#34;: instance_id, \u0026#34;error\u0026#34;: str(e) } except Exception as e: print(f\u0026#34;[ERROR] Isolation FAILED (General) for {instance_id}: {str(e)}\u0026#34;) raise e "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Review AWS knowledge. Complete FCJ Mid-Term exam. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Rewatched FCJ Bootcamp study videos\n- Completed the AWS Cloud Essentials Quiz - Deep dived in AWS Services previously learnt and compared similar services to eachother - Checked out some AWS Architected Labs to better understand each of the main pillars - Successfully created a Lambda function for isolating ec2 and currently waiting to test - AWS Architecture: + Researched how to incoporate AWS Step Functions for automation , rather than using only one Lambda for all IR actions 27/10/2025 27/10/2025 AWS Cloud Essentials Quiz AWS Well Architected Lab 3 - Created 500 AWS Flashcards together with team members for learning 28/10/2025 28/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Studied for Midterm Exam 29/10/2025 29/10/2025 Introduction to Research for Essay Writing 5 - Practiced using AWS Certified Cloud Practitioner notes by other learners online: Did 5 practice tests\n- Practiced using AWS Certified Solutions Architect Associate practice questions: Practiced 40 questions 30/10/2025 30/10/2025 AWS Certified Cloud Practitioner notes AWS Certified Solutions Architect Associate practice 6 - Participated in FCJ Midterm Exam 31/10/2025 31/10/2025 Week 8 Achievements: FCJ Midterm Exam:\nCompleted extensive practice by taking five AWS Certified Cloud Practitioner practice tests and answering 40 AWS Certified Solutions Architect Associate practice questions. Collaborated with team members to create 500 AWS Flashcards for concentrated learning. Completed the AWS Cloud Essentials Quiz and rewatched FCJ Bootcamp study videos. Successfully participated in the FCJ Midterm Exam and achieved a score of 320/650. Reviewed key AWS services and the AWS Well-Architected Labs to understand the main pillars. Workshop Architecture Research:\nResearched the architectural integration of AWS Step Functions to orchestrate Incident Response actions, replacing a single Lambda function. Development \u0026amp; Service Analysis:\nAutomated Containment: Successfully coded and deployed a Lambda function specifically for EC2 instance isolation, moving the project from research to active implementation. Architectural Decision Making: Conducted a deep-dive comparison between AWS Lambda and AWS Step Functions to determine the optimal orchestration strategy for the Incident Response workflow. "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.11-appendices/5.11.8-quarantine-iam/","title":"Quarantine IAM Code","tags":[],"description":"","content":" import json import boto3 import os QUARANTINE_POLICY_ARN = os.environ.get(\u0026#34;QUARANTINE_POLICY_ARN\u0026#34;) def lambda_handler(event, context): print(\u0026#34;=== EVENT RECEIVED ===\u0026#34;) print(json.dumps(event, indent=2)) try: finding = event.get(\u0026#39;detail\u0026#39;, {}) user_name = ( finding.get(\u0026#39;resource\u0026#39;, {}) .get(\u0026#39;accessKeyDetails\u0026#39;, {}) .get(\u0026#39;userName\u0026#39;) ) if not user_name: print(\u0026#34;[WARNING] No IAM user found in this finding. Skipping.\u0026#34;) return {\u0026#34;status\u0026#34;: \u0026#34;no_user\u0026#34;} print(f\u0026#34;[ACTION] Quarantining IAM User \u0026#39;{user_name}\u0026#39;...\u0026#34;) iam = boto3.client(\u0026#39;iam\u0026#39;) # Kiểm tra nếu policy đã được gán attached_policies = iam.list_attached_user_policies(UserName=user_name)[\u0026#39;AttachedPolicies\u0026#39;] policy_arns = [p[\u0026#39;PolicyArn\u0026#39;] for p in attached_policies] if QUARANTINE_POLICY_ARN in policy_arns: print(f\u0026#34;[INFO] Policy {QUARANTINE_POLICY_ARN} is already attached to user {user_name}.\u0026#34;) else: iam.attach_user_policy( UserName=user_name, PolicyArn=QUARANTINE_POLICY_ARN ) print(f\u0026#34;[SUCCESS] Policy attached. User {user_name} is now quarantined.\u0026#34;) except Exception as e: print(f\u0026#34;[ERROR] Failed to quarantine user: {str(e)}\u0026#34;) raise e return {\u0026#34;status\u0026#34;: \u0026#34;processed\u0026#34;, \u0026#34;action\u0026#34;: \u0026#34;iam_quarantined\u0026#34;} "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.8-verify-setup/","title":"Verify Setup","tags":[],"description":"","content":"After all the setup phase, please refer to the checklist to ensure complete resources creation\nVerify Setup Complete Verification Checklist:\nIncident Response and Forensics:\n✅ S3 Buckets: All 5 created with versioning/encryption ✅ IAM Roles: All 17 roles with correct policies ✅ CloudTrail: Logging enabled ✅ GuardDuty: Enabled with S3 export ✅ VPC Flow Logs: Active ✅ Lambda Functions: All 9 deployed ✅ Firehose Streams: All 3 active ✅ Glue Tables: All 4 created ✅ S3 Events: All 4 triggers configured ✅ SNS Topic: Created with subscription ✅ Step Functions: Active ✅ EventBridge Rule: Enabled with 2 targets Security Dashboard:\n✅ S3 Buckets: Bucket is created with dashboard file stored and enabled hosting ✅ Query Lambda: Lambda is created with the appropriate roles ✅ API Gateway: API Gateway is created with the correct API and resources ✅ CloudFront: Distribution is created with API and S3 origins configured ✅ Cognito: Linked to CloudFront distribution and created user in user pool End-to-End Test\nGenerate sample GuardDuty findings: 1.1 GuardDuty Console → Settings → Generate sample findings (200+ findings) or 1.2 Trigger single finding via CloudShell (Dectector Id is in GuardDuty Console → Settings ) aws guardduty create-sample-findings --detector-id [$dectector-id] --finding-types \u0026#34;Recon:EC2/PortProbeUnprotectedPort\u0026#34; Monitor workflow: Check EventBridge, SNS, Step Functions, Lambda logs Verify alerts: Check email and Slack Query data in Athena: "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Keep working on the workshop Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - AWS Architecture revised:\n+ Removed AWS Detective + Updated with Step Function Workflow instead of a singular AWS Lambda Function + Added Custom Dashboard: A static custom dashboard website hosted with S3 and use Athena to query from data lake 03/11/2025 03/11/2025 3 - Start on writing and reviewing inline poilicies for Lambdas and User Roles. 04/11/2025 04/11/2025 4 - Team meetings: Progress report:\n+ IR Workflow: Halfway done, EC2 quarantine function is finished, not tested with findings yet. 05/11/2025 05/11/2025 5 - Team meetings\n- Researched on ETL Pipeline approach: + Instead of using Glue ELT Jobs, we use custom Lambda ELT pipeline for CloudTrail and CloudWatch logs + Store raw logs into a Raw Log S3 Bucket then use ETL Lambda to process the data and write it to a Proccesed Data S3 to then be Crawled - AWS Architecture revised: Added a new group: DATA PREP group which contain the Raw Log S3 Bucket and the ETL Lambda - School subject: 06/11/2025 06/11/2025 Advanced Writing 6 - Researched on Kinesis Data Firehose to collect log: Good for future usage, not suitable for current project because real time streaming data was not necessary, using batch processing is better\n- Succesfully build an ETL Pipeline for CloudTrail logs: Triggered by object creation in CloudTrail Raw Log Bucket and reformatted the raw logs into JSONL and save it into Proccessed S3 - Succesfully crawled and queried the processed log to show CloudTrail Events 07/11/2025 07/11/2025 Week 9 Achievements: Architecture Refinement \u0026amp; Decision Making:\nUpdated the Incident Response (IR) mechanism to use a Step Functions Workflow instead of a single Lambda function to improve orchestration. Introduced a Custom Dashboard strategy (static website hosted on S3) that uses Athena to query the data lake. Conducted a comparative analysis between AWS Kinesis Data Firehose and batch processing; selected the batch approach for cost-efficiency as real-time streaming was not required. Data Pipeline Implementation:\nCreated a new DATA PREP architectural group, featuring a Raw Log S3 Bucket and a custom ETL Lambda. Built and deployed a serverless ETL Lambda pipeline to process CloudTrail logs, automatically triggered by new object creation in the Raw Log S3 Bucket. Successfully crawled and queried the processed CloudTrail logs (formatted as JSONL) using AWS Glue and Athena. Security \u0026amp; IAM Configuration:\nStarted the security hardening process by writing and reviewing inline policies for Lambda functions and User Roles to ensure least-privilege access. Project Progress:\nAchieved 50% completion of the IR Step Functions Workflow, with the EC2 quarantine function fully coded (pending findings test). "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.11-appendices/5.11.9-alert-dispatch/","title":"Alert Dispatch Code","tags":[],"description":"","content":" import os import json import logging import urllib.request import boto3 from botocore.exceptions import ClientError import html # --- Telegram ENV --- # BOT_TOKEN = os.environ.get(\u0026#39;BOT_TOKEN\u0026#39;) # CHAT_ID = os.environ.get(\u0026#39;CHAT_ID\u0026#39;) # MESSAGE_THREAD_ID = os.environ.get(\u0026#39;MESSAGE_THREAD_ID\u0026#39;) # --- Slack ENV --- SLACK_WEBHOOK_URL = os.environ.get(\u0026#34;SLACK_WEBHOOK_URL\u0026#34;) # --- SES ENV --- SENDER_EMAIL = os.environ.get(\u0026#39;SENDER_EMAIL\u0026#39;) RECIPIENT_EMAIL = os.environ.get(\u0026#39;RECIPIENT_EMAIL\u0026#39;) # Can now be \u0026#34;a@b.com, c@d.com\u0026#34; AWS_REGION = os.environ.get(\u0026#39;AWS_REGION\u0026#39;, \u0026#39;ap-southeast-1\u0026#39;) # --- Setup --- # TELEGRAM_URL = f\u0026#34;https://api.telegram.org/bot{BOT_TOKEN}/sendMessage\u0026#34; if BOT_TOKEN else None logger = logging.getLogger() logger.setLevel(logging.INFO) # Initialize SES Client ses_client = boto3.client(\u0026#39;ses\u0026#39;, region_name=AWS_REGION) # ==================================================================== # SEND TO TELEGRAM # ==================================================================== # def send_to_telegram(finding, chat_id, thread_id): # logger.info(\u0026#34;Formatting message for Telegram...\u0026#34;) # severity_num = finding.get(\u0026#39;severity\u0026#39;, 0) # if severity_num \u0026gt;= 7.0: # severity = \u0026#34;🔴 HIGH\u0026#34; # elif severity_num \u0026gt;= 4.0: # severity = \u0026#34;🟠 MEDIUM\u0026#34; # else: # severity = \u0026#34;🔵 LOW\u0026#34; # title = finding.get(\u0026#39;title\u0026#39;, \u0026#39;N/A\u0026#39;) # description = finding.get(\u0026#39;description\u0026#39;, \u0026#39;N/A\u0026#39;) # account_id = finding.get(\u0026#39;accountId\u0026#39;, \u0026#39;N/A\u0026#39;) # region = finding.get(\u0026#39;region\u0026#39;, \u0026#39;N/A\u0026#39;) # finding_type = finding.get(\u0026#39;type\u0026#39;, \u0026#39;N/A\u0026#39;) # message_text = ( # f\u0026#34;🚨 *GuardDuty Finding* 🚨\\n\\n\u0026#34; # f\u0026#34;*Severity:* {severity}\\n\u0026#34; # f\u0026#34;*Account:* {account_id}\\n\u0026#34; # f\u0026#34;*Region:* {region}\\n\u0026#34; # f\u0026#34;*Title:* {title}\\n\u0026#34; # f\u0026#34;*Description:* {description}\\n\\n\u0026#34; # f\u0026#34;*Finding Type:* `{finding_type}`\u0026#34; # ) # payload = {\u0026#39;chat_id\u0026#39;: chat_id, \u0026#39;text\u0026#39;: message_text, \u0026#39;parse_mode\u0026#39;: \u0026#39;Markdown\u0026#39;} # if thread_id: # payload[\u0026#39;message_thread_id\u0026#39;] = thread_id # try: # req = urllib.request.Request( # TELEGRAM_URL, # data=json.dumps(payload).encode(\u0026#39;utf-8\u0026#39;), # headers={\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;} # ) # with urllib.request.urlopen(req) as response: # logger.info(\u0026#34;Telegram response: \u0026#34; + response.read().decode(\u0026#39;utf-8\u0026#39;)) # except Exception as e: # logger.error(f\u0026#34;TELEGRAM FAILED: {e}\u0026#34;) # ==================================================================== # SEND TO SLACK # ==================================================================== def send_to_slack(finding): if not SLACK_WEBHOOK_URL: logger.warning(\u0026#34;Slack ENV missing. Skipping.\u0026#34;) return severity_num = finding.get(\u0026#34;severity\u0026#34;, 0) title = finding.get(\u0026#34;title\u0026#34;, \u0026#34;No Title\u0026#34;) description = finding.get(\u0026#34;description\u0026#34;, \u0026#34;No Description\u0026#34;) region = finding.get(\u0026#34;region\u0026#34;, \u0026#34;N/A\u0026#34;) account_id = finding.get(\u0026#34;accountId\u0026#34;, \u0026#34;N/A\u0026#34;) finding_type = finding.get(\u0026#34;type\u0026#34;, \u0026#34;N/A\u0026#34;) if severity_num \u0026gt;= 7: color = \u0026#34;#ff0000\u0026#34; sev = \u0026#34;🔴 HIGH\u0026#34; elif severity_num \u0026gt;= 4: color = \u0026#34;#ffa500\u0026#34; sev = \u0026#34;🟠 MEDIUM\u0026#34; else: color = \u0026#34;#007bff\u0026#34; sev = \u0026#34;🔵 LOW\u0026#34; payload = { \u0026#34;text\u0026#34;: f\u0026#34;🚨 {sev} – {title}\u0026#34;, \u0026#34;attachments\u0026#34;: [{ \u0026#34;color\u0026#34;: color, \u0026#34;blocks\u0026#34;: [ {\u0026#34;type\u0026#34;: \u0026#34;header\u0026#34;, \u0026#34;text\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;plain_text\u0026#34;, \u0026#34;text\u0026#34;: f\u0026#34;🚨 GuardDuty Finding: {title}\u0026#34;}}, {\u0026#34;type\u0026#34;: \u0026#34;section\u0026#34;, \u0026#34;fields\u0026#34;: [ {\u0026#34;type\u0026#34;: \u0026#34;mrkdwn\u0026#34;, \u0026#34;text\u0026#34;: f\u0026#34;*Severity:*\\n{sev}\u0026#34;}, {\u0026#34;type\u0026#34;: \u0026#34;mrkdwn\u0026#34;, \u0026#34;text\u0026#34;: f\u0026#34;*Region:*\\n{region}\u0026#34;} ]}, {\u0026#34;type\u0026#34;: \u0026#34;section\u0026#34;, \u0026#34;text\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;mrkdwn\u0026#34;, \u0026#34;text\u0026#34;: f\u0026#34;*Description:*\\n{description}\u0026#34;}}, {\u0026#34;type\u0026#34;: \u0026#34;divider\u0026#34;}, {\u0026#34;type\u0026#34;: \u0026#34;context\u0026#34;, \u0026#34;elements\u0026#34;: [ {\u0026#34;type\u0026#34;: \u0026#34;mrkdwn\u0026#34;, \u0026#34;text\u0026#34;: f\u0026#34;*Account:* `{account_id}`\u0026#34;}, {\u0026#34;type\u0026#34;: \u0026#34;mrkdwn\u0026#34;, \u0026#34;text\u0026#34;: f\u0026#34;*Type:* `{finding_type}`\u0026#34;} ]} ] }] } try: req = urllib.request.Request( SLACK_WEBHOOK_URL, data=json.dumps(payload).encode(\u0026#34;utf-8\u0026#34;), headers={\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;} ) with urllib.request.urlopen(req) as response: logger.info(\u0026#34;Slack response: \u0026#34; + response.read().decode(\u0026#34;utf-8\u0026#34;)) except Exception as e: logger.error(f\u0026#34;SLACK FAILED: {e}\u0026#34;) # ==================================================================== # SEND TO SES EMAIL (UPDATED FOR MULTIPLE RECIPIENTS) # ==================================================================== def send_to_ses(finding): if not SENDER_EMAIL or not RECIPIENT_EMAIL: logger.warning(\u0026#34;SES Env vars missing. Skipping Email.\u0026#34;) return logger.info(\u0026#34;Formatting message for SES Email...\u0026#34;) recipient_list = [email.strip() for email in RECIPIENT_EMAIL.split(\u0026#39;,\u0026#39;)] severity_num = finding.get(\u0026#34;severity\u0026#34;, 0) title = finding.get(\u0026#34;title\u0026#34;, \u0026#34;No Title\u0026#34;) description = finding.get(\u0026#34;description\u0026#34;, \u0026#34;No Description\u0026#34;) region = finding.get(\u0026#34;region\u0026#34;, \u0026#34;N/A\u0026#34;) account_id = finding.get(\u0026#34;accountId\u0026#34;, \u0026#34;N/A\u0026#34;) finding_type = finding.get(\u0026#34;type\u0026#34;, \u0026#34;N/A\u0026#34;) finding_id = finding.get(\u0026#34;id\u0026#34;, \u0026#34;N/A\u0026#34;) if severity_num \u0026gt;= 7: color = \u0026#34;#ff0000\u0026#34; sev = \u0026#34;HIGH\u0026#34; elif severity_num \u0026gt;= 4: color = \u0026#34;#ffa500\u0026#34; sev = \u0026#34;MEDIUM\u0026#34; else: color = \u0026#34;#007bff\u0026#34; sev = \u0026#34;LOW\u0026#34; html_body = f\u0026#34;\u0026#34;\u0026#34; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;style\u0026gt; body {{ font-family: Arial, sans-serif; line-height: 1.6; color: #333; }} .container {{ width: 100%; max-width: 600px; margin: 0 auto; border: 1px solid #ddd; border-radius: 8px; overflow: hidden; }} .header {{ background-color: {color}; color: white; padding: 15px; text-align: center; }} .content {{ padding: 20px; }} .footer {{ background-color: #f4f4f4; padding: 10px; text-align: center; font-size: 12px; color: #666; }} .label {{ font-weight: bold; color: #555; }} \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;header\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;🚨 GuardDuty Alert: {sev}\u0026lt;/h2\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;content\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;{title}\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;{description}\u0026lt;/p\u0026gt; \u0026lt;hr\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span class=\u0026#34;label\u0026#34;\u0026gt;Account ID:\u0026lt;/span\u0026gt; {account_id}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span class=\u0026#34;label\u0026#34;\u0026gt;Region:\u0026lt;/span\u0026gt; {region}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span class=\u0026#34;label\u0026#34;\u0026gt;Type:\u0026lt;/span\u0026gt; {finding_type}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;span class=\u0026#34;label\u0026#34;\u0026gt;Finding ID:\u0026lt;/span\u0026gt; {finding_id}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;footer\u0026#34;\u0026gt; Generated by AWS Lambda Alert Dispatch \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; \u0026#34;\u0026#34;\u0026#34; try: response = ses_client.send_email( Source=SENDER_EMAIL, Destination={\u0026#39;ToAddresses\u0026#39;: recipient_list}, # Uses the list now Message={ \u0026#39;Subject\u0026#39;: {\u0026#39;Data\u0026#39;: f\u0026#34;GuardDuty Alert [{sev}]: {title}\u0026#34;, \u0026#39;Charset\u0026#39;: \u0026#39;UTF-8\u0026#39;}, \u0026#39;Body\u0026#39;: {\u0026#39;Html\u0026#39;: {\u0026#39;Data\u0026#39;: html_body, \u0026#39;Charset\u0026#39;: \u0026#39;UTF-8\u0026#39;}} } ) logger.info(f\u0026#34;SES Email sent to {len(recipient_list)} recipients! MessageId: {response[\u0026#39;MessageId\u0026#39;]}\u0026#34;) except ClientError as e: logger.error(f\u0026#34;SES FAILED: {e.response[\u0026#39;Error\u0026#39;][\u0026#39;Message\u0026#39;]}\u0026#34;) # ==================================================================== # MAIN HANDLER # ==================================================================== def lambda_handler(event, context): logger.info(f\u0026#34;Event received: {json.dumps(event)}\u0026#34;) try: sns_message_raw = event[\u0026#34;Records\u0026#34;][0][\u0026#34;Sns\u0026#34;][\u0026#34;Message\u0026#34;] message_data = json.loads(sns_message_raw) # Normalization Logic finding = {} if \u0026#34;detail-type\u0026#34; in message_data and message_data[\u0026#34;detail-type\u0026#34;] == \u0026#34;GuardDuty Finding\u0026#34;: detail = message_data[\u0026#34;detail\u0026#34;] finding = { \u0026#34;severity\u0026#34;: detail.get(\u0026#34;severity\u0026#34;, 0), \u0026#34;title\u0026#34;: detail.get(\u0026#34;title\u0026#34;, \u0026#34;GuardDuty Finding\u0026#34;), \u0026#34;description\u0026#34;: detail.get(\u0026#34;description\u0026#34;, \u0026#34;No description provided\u0026#34;), \u0026#34;accountId\u0026#34;: detail.get(\u0026#34;accountId\u0026#34;, \u0026#34;N/A\u0026#34;), \u0026#34;region\u0026#34;: detail.get(\u0026#34;region\u0026#34;, \u0026#34;N/A\u0026#34;), \u0026#34;type\u0026#34;: detail.get(\u0026#34;type\u0026#34;, \u0026#34;N/A\u0026#34;), \u0026#34;id\u0026#34;: detail.get(\u0026#34;id\u0026#34;, \u0026#34;N/A\u0026#34;) } elif \u0026#34;AlarmName\u0026#34; in message_data: state = message_data.get(\u0026#34;NewStateValue\u0026#34;) severity = 8 if state == \u0026#34;ALARM\u0026#34; else 0 finding = { \u0026#34;severity\u0026#34;: severity, \u0026#34;title\u0026#34;: f\u0026#34;CloudWatch Alarm: {message_data.get(\u0026#39;AlarmName\u0026#39;)}\u0026#34;, \u0026#34;description\u0026#34;: message_data.get(\u0026#34;NewStateReason\u0026#34;, \u0026#34;State change detected\u0026#34;), \u0026#34;accountId\u0026#34;: message_data.get(\u0026#34;AWSAccountId\u0026#34;, \u0026#34;N/A\u0026#34;), \u0026#34;region\u0026#34;: message_data.get(\u0026#34;Region\u0026#34;, \u0026#34;N/A\u0026#34;), \u0026#34;type\u0026#34;: \u0026#34;CloudWatch Alarm\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;N/A\u0026#34; } else: finding = { \u0026#34;severity\u0026#34;: 0, \u0026#34;title\u0026#34;: \u0026#34;Unknown Alert\u0026#34;, \u0026#34;description\u0026#34;: f\u0026#34;Raw Payload: {json.dumps(message_data)}\u0026#34;, \u0026#34;accountId\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Unknown\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;N/A\u0026#34; } except Exception as e: logger.error(f\u0026#34;FATAL: Could not parse incoming SNS event: {e}\u0026#34;) return {\u0026#34;statusCode\u0026#34;: 500} # --- Send Telegram --- # if BOT_TOKEN and CHAT_ID: # send_to_telegram(finding, CHAT_ID, MESSAGE_THREAD_ID) # --- Send Slack --- if SLACK_WEBHOOK_URL: send_to_slack(finding) # --- Send SES Email --- if SENDER_EMAIL and RECIPIENT_EMAIL: send_to_ses(finding) return {\u0026#34;statusCode\u0026#34;: 200, \u0026#34;body\u0026#34;: \u0026#34;Dispatch complete\u0026#34;} "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.9-use-cdk/","title":"Use CDK","tags":[],"description":"","content":"Overview We have provided CDK stack to create all of the infrastructure required for this workshop.\nTo get the files please go to this Github Link and clone or download all the files to a folder\nSetup Guide Before deploying the CDK stack, you must configure your local environment to authenticate with your AWS account using the AWS Command Line Interface (CLI).\nInstall the AWS CLI.\nObtain Credentials: You need an Access Key ID and a Secret Access Key from an IAM user with deployment permissions.\nRun the Configuration Command: Open your terminal and execute aws configure.\n$ aws configure When prompted, enter your credentials and desired settings. The Default region name should match the region where you plan to deploy the stack (e.g., ap-southeast-1):\nPrompt Example Value AWS Access Key ID AKIA... AWS Secret Access Key wJalr... Default region name ap-southeast-1 Default output format json Verify Configuration: Test your setup by fetching your user identity. A successful output confirms you are authenticated.\n$ aws sts get-caller-identity Prerequisites Ensure the following tools and services are installed and configured on your system:\nPython 3.8+ and pip: Required for executing the CDK application and building Lambda function assets. Node.js and npm: Required for running the AWS CDK CLI and building the React dashboard. AWS CDK Toolkit: Install the CDK CLI globally: $ npm install -g aws-cdk Set Up Python Environment The infrastructure definition is written in Python. A dedicated virtual environment is used to manage project dependencies.\nCreate the Virtual Environment:\n$ python -m venv .venv Activate the Virtual Environment:\nOperating System Command macOS / Linux source .venv/bin/activate Windows (Command Prompt) .venv\\Scripts\\activate.bat Windows (PowerShell) .venv\\Scripts\\Activate.ps1 Install Python Dependencies:\n$ pip install -r requirements.txt Step to build the dashboard In the project folder location, check inside the react folder. If the dist folder already exists, you do not need to build. Otherwise, please follow the steps below. If you are on cmd use this command to move to react folder:\n$ cd react And use this command to list all content in react:\n$ ls Prerequisites Ensure you have Node.js and npm installed. You can check the current version by running:\n$ npm --version If the command is not recognized, please download and install Node.js from nodejs.org\nInstall dependencies Run the following command to install all necessary libraries:\n$ npm install Build the Project After the installation is complete, run the build command:\n$ npm run build Upon completion, a dist folder will be generated containing index.html and the assets folder.\nConfigure Deployment Context The stack utilizes context variables. These variables are read from cdk.context.json or provided via command-line flags.\nVariable Name Description Required if functionality is desired Default Value (in cdk.context.json) vpc_ids A list of VPC IDs for Flow Logs and DNS Query Logging. Yes [] alert_email A list of email addresses for alert notifications (requires SES). Yes [] sender_email The verified SES sender email address. Yes (if alert_email is set) \u0026quot;\u0026quot; slack_webhook_url The Slack webhook URL for sending alerts. No \u0026quot;\u0026quot; Example\n{ \u0026#34;vpc_ids\u0026#34;: [ \u0026#34;vpc-a1b2c3d4e5f6g7h8i\u0026#34; ], \u0026#34;alert_email\u0026#34;: [ \u0026#34;admin@example.com\u0026#34; ], \u0026#34;sender_email\u0026#34;: \u0026#34;alerts@your-domain.com\u0026#34;, \u0026#34;slack_webhook_url\u0026#34;: \u0026#34;\u0026#34; } Deploy the Stacks Before processing further, if inside the /react folder, enter this command to go back to the main folder:\n$ cd.. CDK Bootstrapping: If you have not used the AWS CDK in your target AWS account and region previously, run the bootstrap command once to provision necessary resources (e.g., S3 deployment bucket).\n$ cdk bootstrap (Optional) Synthesize and Diff: Review the proposed CloudFormation changes before deployment:\n$ cdk synth --all $ cdk diff --all Execute Deployment: Run the deployment command and approve any requested IAM security changes when prompted.\n$ cdk deploy --all The deployment is complete when the CDK CLI reports success for the stack: AwsIncidentResponseAutomationCdkStack and DashboardCdkStack\nIMPORTANT NOTE: After the deployment is complete, you should verify the email in SES. Create a user in Cognito to be able to log in to the Dashboard. Access the Security Group and remove the default outbound rule from the QuarantineSecurityGroup "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Fully research and test all components to prepare for integrating them into the final workshop build. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Tested EC2 isolation with mock events and real GuardDuty findings (debugged and fixed data parsing functions to accommodate real finding JSON structures).\n- Updated proposal:\n+ Included updated AWS Architecture and Services.\n+ Recalculated pricing estimates. 10/11/2025 10/11/2025 3 - Tested IAM quarantine and currently tackling similar data parsing problems.\n- Successfully built all policies and roles with least-privilege access. 11/11/2025 11/11/2025 4 - Successfully built Step Functions designs including states for: Check Finding Types, Isolate EC2, Quarantine Users, and No Action Needed. 12/11/2025 12/11/2025 5 - Successfully tested Lambda functions with both mock events and some real findings using the new scripts.\n- Currently aiming to build scripts capable of parsing every finding type.\n- Researching how to optimize the Lambda -\u0026gt; EventBridge -\u0026gt; Step Functions structure. 13/11/2025 13/11/2025 Week 10 Achievements: Incident Response Orchestration:\nSuccessfully designed and deployed the AWS Step Functions workflow, featuring distinct logical states for \u0026ldquo;Check Finding Types,\u0026rdquo; \u0026ldquo;Isolate EC2,\u0026rdquo; \u0026ldquo;Quarantine IAM User,\u0026rdquo; and \u0026ldquo;No Action Needed.\u0026rdquo; Real-world Testing \u0026amp; Debugging:\nTransitioned from testing with mock events to handling real Amazon GuardDuty findings. Successfully debugged and refined the Lambda data parsing logic to correctly interpret the complex JSON structure of actual GuardDuty alerts for both EC2 and IAM scenarios. Security \u0026amp; IAM Implementation:\nSuccessfully architected and deployed all necessary IAM roles and inline policies for the quarantine functions, strictly adhering to least-privilege access principles to ensure security best practices. Project Documentation \u0026amp; Optimization:\nUpdated the project proposal to reflect the shift to Step Functions architecture and provided recalculations for projected cloud costs. Initiated research into optimizing the integration pattern between AWS EventBridge and Step Functions to ensure faster and more reliable automated triggers. "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.10-cleanup/","title":"Cleanup","tags":[],"description":"","content":"Congratulations on completing this workshop! In this workshop, you have created an Automated Incident Response and Forensics System and familiarized with Lambda, Step Functions, EventBridge, Glue, Athena, CloudFront, Cognito, S3 Buckets\nCleanup Guide: Cleanup Guide for Manual Infrastructure Setup Clean Guide for CDK Setup "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.10-cleanup/5.10.1-manual-cleanup/","title":"Manual Cleanup","tags":[],"description":"","content":"Clean up (Manual Infrastructure Setup) Phase 1: Automation and Monitoring Cleanup The goal here is to stop all active processes and delete the monitoring and core automation resources (EventBridge, Step Functions, SNS, GuardDuty, Flow Logs, CloudTrail).\n1. Delete Incident Response Automation 1.1 Delete EventBridge Rule\nGo to EventBridge Console → Rules. Select the rule: IncidentResponseAlert. Click \u0026ldquo;Delete\u0026rdquo;. 1.2 Delete Step Functions State Machine\nGo to Step Functions Console → State Machines. Select the State Machine: IncidentResponseStepFunctions. Click \u0026ldquo;Delete\u0026rdquo;. 1.3 Delete SNS Topic and Subscription\nGo to SNS Console → Topics → IncidentResponseAlerts. First, delete the subscription associated with ir-alert-dispatch. Then, delete the topic itself by clicking \u0026ldquo;Delete topic\u0026rdquo;. 1.4 Delete GuardDuty Detector\nGo to GuardDuty Console → Settings → General. Click \u0026ldquo;Suspend\u0026rdquo; to stop processing, then click \u0026ldquo;Disable GuardDuty\u0026rdquo; (or \u0026ldquo;Delete detector\u0026rdquo;). 1.5 Disable VPC Flow Logs\nGo to VPC Console → VPC Flow Logs. Select the flow log created (associated with YOUR_VPC_ID). Click \u0026ldquo;Delete flow log\u0026rdquo;. 1.6 Delete CloudTrail Trail\nGo to CloudTrail Console → Trails. Select the trail: incident-responses-cloudtrail-ACCOUNT_ID-REGION. Click \u0026ldquo;Delete\u0026rdquo;. Phase 2: Lambda and Compute Cleanup 2. Delete All Lambda Functions (9 Functions) Go to the Lambda Console and delete the following functions:\nincident-response-cloudtrail-etl incident-response-guardduty-etl cloudwatch-etl-lambda cloudwatch-eni-etl-lambda cloudwatch-export-lambda ir-parse-findings-lambda ir-isolate-ec2-lambda ir-quarantine-iam-lambda ir-alert-dispatch 3. Delete Isolation Security Group Go to EC2 Console → Security Groups. Find and select the Security Group: IR-Isolation-SG (using ID sg-XXXXXXX). Click \u0026ldquo;Delete security group\u0026rdquo;. 4. Delete CloudWatch Log Groups Go to the CloudWatch Console → Log Groups and delete:\nThe centralized log group: /aws/incident-response/centralized-logs. Any associated Lambda log groups for the 9 deleted functions (e.g., /aws/lambda/ir-parse-findings-lambda). Phase 3: Processing and Data Lake Cleanup 5. Delete Kinesis Data Firehose Streams Go to the Kinesis Console → Delivery Streams and delete:\ncloudtrail-firehose-stream vpc-dns-firehose-stream vpc-flow-firehose-stream 6. Delete AWS Glue Tables and Database 6.1 Delete Glue Tables\nGo to Glue Console → Tables. Select and delete: security_logs.processed_cloudtrail, security_logs.processed_guardduty, security_logs.vpc_logs, and security_logs.eni_flow_logs. 6.2 Delete Glue Database\nGo to Glue Console → Databases. Select the database: security_logs and click \u0026ldquo;Delete\u0026rdquo;. 7. Delete IAM Roles and Policies 7.1 Delete IAM Policies\nGo to IAM Console → Policies. Delete the custom managed policy: IrQuarantineIAMPolicy. Note: Inline policies created in the setup will be deleted automatically when the corresponding role is deleted. 7.2 Delete IAM Roles\nGo to IAM Console → Roles. Delete the following 17 roles: Lambda Execution Roles: CloudTrailETLLambdaServiceRole, GuardDutyETLLambdaServiceRole, CloudWatchETLLambdaServiceRole, CloudWatchENIETLLambdaServiceRole, CloudWatchExportLambdaServiceRole, ParseFindingsLambdaServiceRole, IsolateEC2LambdaServiceRole, QuarantineIAMLambdaServiceRole, AlertDispatchLambdaServiceRole. Service Roles: CloudTrailFirehoseRole, CloudWatchFirehoseRole, StepFunctionsRole, IncidentResponseStepFunctionsEventRole, FlowLogsIAMRole, GlueCloudWatchRole. Phase 4: S3 Bucket Cleanup (Data Deletion) 8. Empty and Delete S3 Buckets This is the final step to ensure all storage charges are stopped.\nBucket Name Purpose incident-response-log-list-bucket-ACCOUNT_ID-REGION Primary Log Source (CloudTrail/GuardDuty/Exported CW) processed-cloudtrail-logs-ACCOUNT_ID-REGION Firehose Destination for CloudTrail logs processed-cloudwatch-logs-ACCOUNT_ID-REGION Firehose Destination for VPC DNS/Flow logs processed-guardduty-findings-ACCOUNT_ID-REGION ETL Destination for GuardDuty logs athena-query-results-ACCOUNT_ID-REGION Athena Query Results Storage Go to the S3 Console. For each of the 5 buckets: Click on the bucket name. Go to the \u0026ldquo;Objects\u0026rdquo; tab. Click \u0026ldquo;Empty\u0026rdquo; to clear all data. You must confirm the permanent delete by typing permanently delete. Go back to the S3 bucket list, select the bucket, and click \u0026ldquo;Delete\u0026rdquo;. "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.11-appendices/5.11.10-step-functions-state-machine-definition/","title":"Steps Functions Definition ASL Code","tags":[],"description":"","content":" { \u0026#34;Comment\u0026#34;: \u0026#34;Guardduty Incident Response Automation\u0026#34;, \u0026#34;StartAt\u0026#34;: \u0026#34;CheckFindingType\u0026#34;, \u0026#34;States\u0026#34;: { \u0026#34;CheckFindingType\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Choice\u0026#34;, \u0026#34;Choices\u0026#34;: [ { \u0026#34;Comment\u0026#34;: \u0026#34;Check if EC2\u0026#34;, \u0026#34;Variable\u0026#34;: \u0026#34;$.detail.resource.resourceType\u0026#34;, \u0026#34;StringEquals\u0026#34;: \u0026#34;Instance\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;ParseFindings\u0026#34; }, { \u0026#34;Comment\u0026#34;: \u0026#34;Check if IAM\u0026#34;, \u0026#34;Variable\u0026#34;: \u0026#34;$.detail.resource.resourceType\u0026#34;, \u0026#34;StringEquals\u0026#34;: \u0026#34;AccessKey\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;Quarantine_IAM_User\u0026#34; } ], \u0026#34;Default\u0026#34;: \u0026#34;NoActionNeeded\u0026#34; }, \u0026#34;ParseFindings\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::lambda:invoke\u0026#34;, \u0026#34;OutputPath\u0026#34;: \u0026#34;$.Payload\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;Payload.$\u0026#34;: \u0026#34;$\u0026#34;, \u0026#34;FunctionName\u0026#34;: \u0026#34;arn:aws:lambda:ap-southeast-1:831981618496:function:ir-parse-findings-lambda\u0026#34; }, \u0026#34;Retry\u0026#34;: [ { \u0026#34;ErrorEquals\u0026#34;: [ \u0026#34;Lambda.ServiceException\u0026#34;, \u0026#34;Lambda.AWSLambdaException\u0026#34;, \u0026#34;Lambda.SdkClientException\u0026#34;, \u0026#34;Lambda.TooManyRequestsException\u0026#34; ], \u0026#34;IntervalSeconds\u0026#34;: 1, \u0026#34;MaxAttempts\u0026#34;: 3, \u0026#34;BackoffRate\u0026#34;: 2, \u0026#34;JitterStrategy\u0026#34;: \u0026#34;FULL\u0026#34; } ], \u0026#34;Next\u0026#34;: \u0026#34;Isolate_EC2_Instance\u0026#34; }, \u0026#34;Isolate_EC2_Instance\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::lambda:invoke\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;FunctionName\u0026#34;: \u0026#34;arn:aws:lambda:ap-southeast-1:831981618496:function:ir-isolate-ec2-lambda\u0026#34;, \u0026#34;Payload\u0026#34;: { \u0026#34;InstanceId.$\u0026#34;: \u0026#34;$.InstanceIds[0]\u0026#34;, \u0026#34;Region.$\u0026#34;: \u0026#34;$.Region\u0026#34; } }, \u0026#34;Retry\u0026#34;: [ { \u0026#34;ErrorEquals\u0026#34;: [ \u0026#34;Lambda.TooManyRequestsException\u0026#34;, \u0026#34;Lambda.ServiceException\u0026#34;, \u0026#34;Lambda.AWSLambdaException\u0026#34;, \u0026#34;Lambda.SdkClientException\u0026#34; ], \u0026#34;IntervalSeconds\u0026#34;: 2, \u0026#34;MaxAttempts\u0026#34;: 3, \u0026#34;BackoffRate\u0026#34;: 2 } ], \u0026#34;Next\u0026#34;: \u0026#34;CheckIsolationStatus\u0026#34;, \u0026#34;OutputPath\u0026#34;: \u0026#34;$.Payload\u0026#34; }, \u0026#34;CheckIsolationStatus\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Choice\u0026#34;, \u0026#34;Choices\u0026#34;: [ { \u0026#34;Variable\u0026#34;: \u0026#34;$.IsolationSG\u0026#34;, \u0026#34;IsNull\u0026#34;: true, \u0026#34;Next\u0026#34;: \u0026#34;AlreadyIsolated\u0026#34; } ], \u0026#34;Default\u0026#34;: \u0026#34;EnableTerminationProtection\u0026#34; }, \u0026#34;AlreadyIsolated\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Succeed\u0026#34; }, \u0026#34;EnableTerminationProtection\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:ec2:modifyInstanceAttribute\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;InstanceId.$\u0026#34;: \u0026#34;$.InstanceId\u0026#34;, \u0026#34;DisableApiTermination\u0026#34;: { \u0026#34;Value\u0026#34;: true } }, \u0026#34;Next\u0026#34;: \u0026#34;CreateQuarantineTag\u0026#34;, \u0026#34;ResultPath\u0026#34;: null }, \u0026#34;CreateQuarantineTag\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:ec2:createTags\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;Resources.$\u0026#34;: \u0026#34;States.Array($.InstanceId)\u0026#34;, \u0026#34;Tags\u0026#34;: [ { \u0026#34;Key\u0026#34;: \u0026#34;Quarantine\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;True\u0026#34; }, { \u0026#34;Key\u0026#34;: \u0026#34;Security Group\u0026#34;, \u0026#34;Value.$\u0026#34;: \u0026#34;$.IsolationSG\u0026#34; } ] }, \u0026#34;Next\u0026#34;: \u0026#34;DescribeInstanceASG\u0026#34;, \u0026#34;ResultPath\u0026#34;: null }, \u0026#34;DescribeInstanceASG\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:autoscaling:describeAutoScalingInstances\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;InstanceIds.$\u0026#34;: \u0026#34;States.Array($.InstanceId)\u0026#34; }, \u0026#34;ResultPath\u0026#34;: \u0026#34;$.ASGInfo\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;CheckIfASGExists\u0026#34; }, \u0026#34;CheckIfASGExists\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Choice\u0026#34;, \u0026#34;Choices\u0026#34;: [ { \u0026#34;Variable\u0026#34;: \u0026#34;$.ASGInfo.AutoScalingInstances[0]\u0026#34;, \u0026#34;IsPresent\u0026#34;: true, \u0026#34;Next\u0026#34;: \u0026#34;UpdateASGConfiguration\u0026#34; } ], \u0026#34;Default\u0026#34;: \u0026#34;DescribeVolumes\u0026#34; }, \u0026#34;UpdateASGConfiguration\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:autoscaling:updateAutoScalingGroup\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;AutoScalingGroupName.$\u0026#34;: \u0026#34;$.ASGInfo.AutoScalingInstances[0].AutoScalingGroupName\u0026#34;, \u0026#34;MinSize\u0026#34;: 0 }, \u0026#34;ResultPath\u0026#34;: null, \u0026#34;Next\u0026#34;: \u0026#34;Wait for ASG\u0026#34; }, \u0026#34;Wait for ASG\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Wait\u0026#34;, \u0026#34;Seconds\u0026#34;: 10, \u0026#34;Next\u0026#34;: \u0026#34;DetachFromASG\u0026#34; }, \u0026#34;DetachFromASG\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:autoscaling:detachInstances\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;AutoScalingGroupName.$\u0026#34;: \u0026#34;$.ASGInfo.AutoScalingInstances[0].AutoScalingGroupName\u0026#34;, \u0026#34;InstanceIds.$\u0026#34;: \u0026#34;States.Array($.InstanceId)\u0026#34;, \u0026#34;ShouldDecrementDesiredCapacity\u0026#34;: false }, \u0026#34;Retry\u0026#34;: [ { \u0026#34;ErrorEquals\u0026#34;: [ \u0026#34;AutoScaling.ValidationException\u0026#34; ], \u0026#34;IntervalSeconds\u0026#34;: 15, \u0026#34;MaxAttempts\u0026#34;: 3, \u0026#34;BackoffRate\u0026#34;: 2 } ], \u0026#34;ResultPath\u0026#34;: null, \u0026#34;Next\u0026#34;: \u0026#34;DescribeVolumes\u0026#34; }, \u0026#34;DescribeVolumes\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:ec2:describeVolumes\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;Filters\u0026#34;: [ { \u0026#34;Name\u0026#34;: \u0026#34;attachment.instance-id\u0026#34;, \u0026#34;Values.$\u0026#34;: \u0026#34;States.Array($.InstanceId)\u0026#34; } ] }, \u0026#34;ResultPath\u0026#34;: \u0026#34;$.VolumeInfo\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;CreateSnapshots\u0026#34; }, \u0026#34;CreateSnapshots\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Map\u0026#34;, \u0026#34;ItemsPath\u0026#34;: \u0026#34;$.VolumeInfo.Volumes\u0026#34;, \u0026#34;MaxConcurrency\u0026#34;: 1, \u0026#34;Iterator\u0026#34;: { \u0026#34;StartAt\u0026#34;: \u0026#34;Wait before calling CreateSnapshot API\u0026#34;, \u0026#34;States\u0026#34;: { \u0026#34;Wait before calling CreateSnapshot API\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Wait\u0026#34;, \u0026#34;Seconds\u0026#34;: 15, \u0026#34;Next\u0026#34;: \u0026#34;CreateSnapshot\u0026#34; }, \u0026#34;CreateSnapshot\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:ec2:createSnapshot\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;VolumeId.$\u0026#34;: \u0026#34;$.VolumeId\u0026#34;, \u0026#34;Description.$\u0026#34;: \u0026#34;States.Format(\u0026#39;IR Snapshot for {} - {}\u0026#39;, $.Attachments[0].InstanceId, $.VolumeId)\u0026#34;, \u0026#34;TagSpecifications\u0026#34;: [ { \u0026#34;ResourceType\u0026#34;: \u0026#34;snapshot\u0026#34;, \u0026#34;Tags\u0026#34;: [ { \u0026#34;Key\u0026#34;: \u0026#34;Quarantine\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;True\u0026#34; } ] } ] }, \u0026#34;Retry\u0026#34;: [ { \u0026#34;ErrorEquals\u0026#34;: [ \u0026#34;Ec2.RequestLimitExceeded\u0026#34; ], \u0026#34;IntervalSeconds\u0026#34;: 60, \u0026#34;MaxAttempts\u0026#34;: 3, \u0026#34;BackoffRate\u0026#34;: 2 } ], \u0026#34;End\u0026#34;: true } } }, \u0026#34;End\u0026#34;: true }, \u0026#34;Quarantine_IAM_User\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Choice\u0026#34;, \u0026#34;Choices\u0026#34;: [ { \u0026#34;Variable\u0026#34;: \u0026#34;$.detail.resource.accessKeyDetails.userType\u0026#34;, \u0026#34;StringEquals\u0026#34;: \u0026#34;Root\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;RootUserDetected\u0026#34; } ], \u0026#34;Default\u0026#34;: \u0026#34;ExecuteIAMQuarantine\u0026#34; }, \u0026#34;RootUserDetected\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Succeed\u0026#34;, \u0026#34;Comment\u0026#34;: \u0026#34;Cannot quarantine root user\u0026#34; }, \u0026#34;ExecuteIAMQuarantine\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::lambda:invoke\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;FunctionName\u0026#34;: \u0026#34;arn:aws:lambda:ap-southeast-1:831981618496:function:ir-quarantine-iam-lambda\u0026#34;, \u0026#34;Payload.$\u0026#34;: \u0026#34;$\u0026#34; }, \u0026#34;Retry\u0026#34;: [ { \u0026#34;ErrorEquals\u0026#34;: [ \u0026#34;Lambda.TooManyRequestsException\u0026#34;, \u0026#34;Lambda.ServiceException\u0026#34;, \u0026#34;Lambda.AWSLambdaException\u0026#34;, \u0026#34;Lambda.SdkClientException\u0026#34; ], \u0026#34;IntervalSeconds\u0026#34;: 2, \u0026#34;MaxAttempts\u0026#34;: 3, \u0026#34;BackoffRate\u0026#34;: 2 } ], \u0026#34;End\u0026#34;: true }, \u0026#34;NoActionNeeded\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Succeed\u0026#34; } } } "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Optimize architecture for cost and reliability (SQS integration \u0026amp; S3 API analysis). Deepen knowledge of Edge Security (WAF/CloudFront) and Infrastructure as Code (CDK). Finalize EC2 isolation testing and refine the custom dashboard. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Joined theAWS Cloud Mastery Series #2: Gained insights on CDK and CloudFormation for the project and received mentor recommendations on demo strategy.\n- Wrote a recap of the event experience. 17/11/2025 17/11/2025 3 -Initiated a pricing update plan.\n- Implementation:\n+ Successfully isolated EC2 in the testing environment.\n+ Upgraded CloudWatch ETL Lambda to process logs via triggers without Crawlers.\n- Backed up all Lambda function codes. 18/11/2025 18/11/2025 4 - Joined theSecure Your Applications: AWS Perimeter Protection Workshop.\n+ Deep dived into CloudFront and WAF configuration.\n+ Reviewed the new CloudFront pricing tier.\n- Learnt how to set up API Gateway REST APIs to prepare for dashboard integration. 19/11/2025 19/11/2025 AWS Perimeter Protection 6 - Researched AWS CDK: Installation, usage, and stack configuration to prepare for next week\u0026rsquo;s migration to IaC. 21/11/2025 21/11/2025 AWS CDK Github AWS CDK Document Week 11 Achievements: Architecture Optimization \u0026amp; Cost Management:\nRefined the event-driven architecture by decoupling EventBridge and Step Functions using Amazon SQS to ensure message durability and reliability. Formulated a plan to optimize API costs. Security \u0026amp; Network Learning: Gained practical experience in perimeter protection by completing the AWS Perimeter Protection Workshop, focusing on AWS WAF (Web Application Firewall) and Amazon CloudFront configurations. Researched API Gateway REST APIs to facilitate the future integration of the custom security dashboard.\nInfrastructure as Code (IaC) Preparation:\nLeveraged insights from the AWS Cloud Mastery Series #2 to plan the project\u0026rsquo;s migration to Infrastructure as Code. Completed foundational research on AWS CDK, covering installation, stack configuration, and construct usage to prepare for the implementation phase. Project Implementation:\nSuccessfully executed and verified EC2 Isolation logic in the live testing environment. Updated the custom dashboard frontend to display more relevant security fields. "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/5-workshop/5.11-appendices/","title":"Appendices","tags":[],"description":"","content":"Appendices Lambda Codes: CloudTrail ETL GuardDuty ETL CloudWatch ETL CloudWatch ENI ETL CloudWatch Auto Export Parse Findings Isolate EC2 Instance Quarantine IAM Alert Dispatch Step Functions ASL Code: Step Functions ASL Code "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Enhance the Incident Response workflow with automated forensics collection (SSM). Debug and refine Lambda functions and IAM roles for reliable execution. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 3 - IR Step Functions update: Added a Map State to iterate over isolated Instances and trigger an SSM Lambda for those Instances to collect logs for forensics.\n- CDK: Moved the CDK testing environment to a new dedicated account. 26/11/2025 26/11/2025 6 - Team meetings:\n+ Assigned CDK tasks for members + Got started on updating proposal and architecture diagram - Fixed and improved IR Step Functions: + Fixed EC2Isolate Lambda: Corrected parsing method + Improved state: Added Parsing Lambda and reordered functions + SSM Failed due to missing IAM: Added necessary roles to the SSM Forensics Function. 28/11/2025 30/11/2025 Week 12 Achievements: Incident Response \u0026amp; Forensics Logic:\nEnhanced the Step Functions workflow by implementing a Map State, enabling the system to iterate through multiple isolated EC2 instances simultaneously.\nIntegrated an SSM (Systems Manager) Lambda to automatically trigger forensic log collection on compromised instances.\nDebugging \u0026amp; Optimization:\nRefined the data handling logic by fixing JSON parsing errors in the EC2Isolate Lambda. Resolved permission issues by attaching the correct IAM Roles to the SSM Forensics function, ensuring it has permission to execute commands on EC2 instances. Optimized the workflow structure by reordering states and adding a dedicated Parsing Lambda for better data flow. "},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://huyletran999.github.io/AWS-WorkLogS/tags/","title":"Tags","tags":[],"description":"","content":""}]